{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODvBkqPoeY1Y",
        "outputId": "0e1f0d77-1f3b-424b-997d-1bd93b9a538e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from typing import Dict, List, Tuple, Set\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "SWQ-xfhLemMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalLinkDataset:\n",
        "    \"\"\"Dataset with improved negative sampling using node corruption from actual edges.\"\"\"\n",
        "\n",
        "    def __init__(self, edges_df: pd.DataFrame, edge_features: torch.Tensor,\n",
        "                 sequence_length: int = 10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            edges_df: DataFrame with columns [src, dst, time, ext_roll] (ALL POSITIVE SAMPLES)\n",
        "            edge_features: Tensor of shape [num_edges, 2] with src and dst features\n",
        "            sequence_length: Length of temporal sequence to use for prediction\n",
        "        \"\"\"\n",
        "        self.edges_df = edges_df.copy()\n",
        "        self.edge_features = edge_features\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        # Verify alignment\n",
        "        assert len(self.edges_df) == len(self.edge_features), \\\n",
        "            f\"Mismatch: {len(self.edges_df)} edges vs {len(self.edge_features)} features\"\n",
        "\n",
        "        # Sort by time for temporal consistency\n",
        "        self.edges_df = self.edges_df.sort_values('time').reset_index(drop=True)\n",
        "        # Reorder features to match the sorted edges\n",
        "        original_indices = self.edges_df.index.tolist()  # This gives us the mapping\n",
        "        # Actually, after reset_index(drop=True), we need to track original order\n",
        "        # Let's be more careful:\n",
        "\n",
        "        # Store original index before sorting\n",
        "        self.edges_df['original_idx'] = self.edges_df.index\n",
        "        self.edges_df = self.edges_df.sort_values('time').reset_index(drop=True)\n",
        "\n",
        "        # Reorder features to match sorted edges\n",
        "        sorted_feature_indices = self.edges_df['original_idx'].tolist()\n",
        "        self.edge_features = self.edge_features[sorted_feature_indices]\n",
        "\n",
        "        # Now drop the helper column\n",
        "        self.edges_df = self.edges_df.drop('original_idx', axis=1)\n",
        "\n",
        "        # Get nodes that appear in each time split for realistic negative sampling\n",
        "        self.split_nodes = self._get_split_nodes()\n",
        "\n",
        "        # Create positive edge sets for each timestamp within each split\n",
        "        self.positive_edges_by_time = self._create_positive_edges_by_time()\n",
        "\n",
        "        # Generate negative samples using node corruption\n",
        "        self.samples = self._generate_samples_with_negatives()\n",
        "\n",
        "        print(f\"Created dataset with {len(self.samples)} samples\")\n",
        "        pos_count = sum(1 for s in self.samples if s['target'] == 1)\n",
        "        neg_count = sum(1 for s in self.samples if s['target'] == 0)\n",
        "        print(f\"Positive samples: {pos_count}\")\n",
        "        print(f\"Negative samples: {neg_count}\")\n",
        "\n",
        "    def _get_split_nodes(self) -> Dict[int, Set[int]]:\n",
        "        \"\"\"Get all nodes that appear in each split for realistic negative sampling.\"\"\"\n",
        "        split_nodes = {}\n",
        "        for ext_roll in [0, 1, 2]:\n",
        "            split_edges = self.edges_df[self.edges_df['ext_roll'] == ext_roll]\n",
        "            nodes = set(split_edges['src'].unique()) | set(split_edges['dst'].unique())\n",
        "            split_nodes[ext_roll] = nodes\n",
        "        return split_nodes\n",
        "\n",
        "    def _create_positive_edges_by_time(self) -> Dict[Tuple[int, int], Set[Tuple[int, int]]]:\n",
        "        \"\"\"Create positive edge sets for each (ext_roll, time) combination.\"\"\"\n",
        "        positive_edges = {}\n",
        "\n",
        "        for _, row in self.edges_df.iterrows():\n",
        "            key = (row['ext_roll'], row['time'])\n",
        "            if key not in positive_edges:\n",
        "                positive_edges[key] = set()\n",
        "            positive_edges[key].add((row['src'], row['dst']))\n",
        "\n",
        "        return positive_edges\n",
        "\n",
        "    def _generate_samples_with_negatives(self) -> List[Dict]:\n",
        "        \"\"\"Generate both positive samples and negative samples using node corruption.\"\"\"\n",
        "        samples = []\n",
        "\n",
        "        for idx, row in self.edges_df.iterrows():\n",
        "            src, dst, time, ext_roll = row['src'], row['dst'], row['time'], row['ext_roll']\n",
        "\n",
        "            # Get historical context (same logic as before but with corrected indexing)\n",
        "            if ext_roll == 0:  # Training\n",
        "                historical_mask = (self.edges_df['time'] < time) & (self.edges_df['ext_roll'] == 0)\n",
        "            else:  # Val/Test - can use training + previous split data\n",
        "                max_roll = 0 if ext_roll == 1 else 1\n",
        "                historical_mask = (self.edges_df['time'] < time) & (self.edges_df['ext_roll'] <= max_roll)\n",
        "\n",
        "            historical_edges = self.edges_df[historical_mask].tail(self.sequence_length)\n",
        "\n",
        "            # Create sequence features (now properly aligned)\n",
        "            if len(historical_edges) > 0:\n",
        "                hist_indices = historical_edges.index.tolist()\n",
        "                sequence_features = self.edge_features[hist_indices]\n",
        "\n",
        "                # Pad if necessary\n",
        "                if len(sequence_features) < self.sequence_length:\n",
        "                    padding = torch.zeros(self.sequence_length - len(sequence_features), 2)\n",
        "                    sequence_features = torch.cat([padding, sequence_features], dim=0)\n",
        "            else:\n",
        "                sequence_features = torch.zeros(self.sequence_length, 2)\n",
        "\n",
        "            # POSITIVE SAMPLE\n",
        "            current_features = self.edge_features[idx]\n",
        "            samples.append({\n",
        "                'sequence': sequence_features,\n",
        "                'current_features': current_features,\n",
        "                'target': 1,\n",
        "                'ext_roll': ext_roll,\n",
        "                'time': time,\n",
        "                'src': src,\n",
        "                'dst': dst,\n",
        "                'edge_idx': idx\n",
        "            })\n",
        "\n",
        "            # NEGATIVE SAMPLE using node corruption\n",
        "            negative_sample = self._create_negative_sample(src, dst, time, ext_roll, sequence_features)\n",
        "            if negative_sample is not None:\n",
        "                samples.append(negative_sample)\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _create_negative_sample(self, src: int, dst: int, time: int, ext_roll: int,\n",
        "                              sequence_features: torch.Tensor) -> Dict:\n",
        "        \"\"\"\n",
        "        Create negative sample by corrupting either src or dst node.\n",
        "        Uses nodes from the same split to ensure realistic corruption.\n",
        "        \"\"\"\n",
        "        # Get positive edges at this specific time and split\n",
        "        time_key = (ext_roll, time)\n",
        "        positive_edges_at_time = self.positive_edges_by_time.get(time_key, set())\n",
        "\n",
        "        # Get available nodes from this split\n",
        "        available_nodes = list(self.split_nodes[ext_roll])\n",
        "\n",
        "        # Try to corrupt dst first (keep src, change dst)\n",
        "        max_attempts = 50\n",
        "        attempts = 0\n",
        "\n",
        "        while attempts < max_attempts:\n",
        "            attempts += 1\n",
        "\n",
        "            # Randomly choose to corrupt src or dst\n",
        "            if random.random() < 0.5:\n",
        "                # Corrupt dst (keep src)\n",
        "                neg_src = src\n",
        "                neg_dst = random.choice(available_nodes)\n",
        "            else:\n",
        "                # Corrupt src (keep dst)\n",
        "                neg_src = random.choice(available_nodes)\n",
        "                neg_dst = dst\n",
        "\n",
        "            # Check if this would be a valid negative (not in positive edges at this time)\n",
        "            if ((neg_src, neg_dst) not in positive_edges_at_time and\n",
        "                neg_src != neg_dst):\n",
        "\n",
        "                # Create features for negative sample\n",
        "                # We'll use features from existing edges involving these nodes\n",
        "                neg_features = self._get_features_for_negative(neg_src, neg_dst, time, ext_roll)\n",
        "\n",
        "                if neg_features is not None:\n",
        "                    return {\n",
        "                        'sequence': sequence_features.clone(),\n",
        "                        'current_features': neg_features,\n",
        "                        'target': 0,\n",
        "                        'ext_roll': ext_roll,\n",
        "                        'time': time,\n",
        "                        'src': neg_src,\n",
        "                        'dst': neg_dst,\n",
        "                        'edge_idx': -1\n",
        "                    }\n",
        "\n",
        "        # If we couldn't create a valid negative sample, return None\n",
        "        return None\n",
        "\n",
        "    def _get_features_for_negative(self, neg_src: int, neg_dst: int, time: int, ext_roll: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Get features for negative sample by looking for edges involving these nodes\n",
        "        around the same time period, or use interpolation/averaging.\n",
        "        \"\"\"\n",
        "        # Strategy 1: Find edges involving neg_src or neg_dst around this time\n",
        "        time_window = 5  # Look within Â±5 time units\n",
        "\n",
        "        # Look for edges involving these nodes in a time window\n",
        "        mask = (\n",
        "            ((self.edges_df['src'] == neg_src) | (self.edges_df['dst'] == neg_src) |\n",
        "             (self.edges_df['src'] == neg_dst) | (self.edges_df['dst'] == neg_dst)) &\n",
        "            (abs(self.edges_df['time'] - time) <= time_window) &\n",
        "            (self.edges_df['ext_roll'] <= ext_roll)  # Only use past/current data\n",
        "        )\n",
        "\n",
        "        candidate_edges = self.edges_df[mask]\n",
        "\n",
        "        if len(candidate_edges) > 0:\n",
        "            # Use features from a random candidate edge\n",
        "            candidate_idx = random.choice(candidate_edges.index.tolist())\n",
        "            base_features = self.edge_features[candidate_idx].clone()\n",
        "\n",
        "            # Add small noise to make it slightly different\n",
        "            noise = torch.normal(0, 0.1, size=base_features.shape)\n",
        "            return base_features + noise\n",
        "\n",
        "        # Strategy 2: If no candidates found, use average features from the split with noise\n",
        "        split_mask = self.edges_df['ext_roll'] <= ext_roll\n",
        "        if split_mask.sum() > 0:\n",
        "            split_features = self.edge_features[split_mask]\n",
        "            mean_features = split_features.mean(dim=0)\n",
        "            std_features = split_features.std(dim=0)\n",
        "\n",
        "            # Generate features based on distribution\n",
        "            return torch.normal(mean_features, std_features * 0.2)\n",
        "\n",
        "        # Strategy 3: Fallback to zero features (should rarely happen)\n",
        "        return torch.zeros(2)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        return {\n",
        "            'sequence': sample['sequence'].float(),\n",
        "            'current_features': sample['current_features'].float(),\n",
        "            'target': torch.tensor(sample['target'], dtype=torch.float32),\n",
        "            'ext_roll': sample['ext_roll']\n",
        "        }"
      ],
      "metadata": {
        "id": "s54mgJtRe0nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalLSTMPredictor(nn.Module):\n",
        "    \"\"\"LSTM-based temporal link prediction model.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int = 2, hidden_dim: int = 64,\n",
        "                 num_layers: int = 2, dropout: float = 0.2, use_gru: bool = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: Dimension of edge features\n",
        "            hidden_dim: Hidden dimension of LSTM/GRU\n",
        "            num_layers: Number of LSTM/GRU layers\n",
        "            dropout: Dropout rate\n",
        "            use_gru: Whether to use GRU instead of LSTM\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.use_gru = use_gru\n",
        "\n",
        "        # Temporal sequence encoder\n",
        "        if use_gru:\n",
        "            self.temporal_encoder = nn.GRU(\n",
        "                input_dim, hidden_dim, num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
        "            )\n",
        "        else:\n",
        "            self.temporal_encoder = nn.LSTM(\n",
        "                input_dim, hidden_dim, num_layers,\n",
        "                batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
        "            )\n",
        "\n",
        "        # Current edge feature encoder\n",
        "        self.current_encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Fusion and prediction layers\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, sequence, current_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            sequence: [batch_size, seq_len, input_dim]\n",
        "            current_features: [batch_size, input_dim]\n",
        "        \"\"\"\n",
        "        batch_size = sequence.size(0)\n",
        "\n",
        "        # Encode temporal sequence\n",
        "        if self.use_gru:\n",
        "            _, hidden = self.temporal_encoder(sequence)\n",
        "            temporal_repr = hidden[-1]  # Use last layer's hidden state\n",
        "        else:\n",
        "            _, (hidden, _) = self.temporal_encoder(sequence)\n",
        "            temporal_repr = hidden[-1]  # Use last layer's hidden state\n",
        "\n",
        "        # Encode current features\n",
        "        current_repr = self.current_encoder(current_features)\n",
        "\n",
        "        # Fuse representations\n",
        "        fused = torch.cat([temporal_repr, current_repr], dim=1)\n",
        "\n",
        "        # Predict link probability\n",
        "        output = self.fusion(fused)\n",
        "\n",
        "        return output.squeeze()\n"
      ],
      "metadata": {
        "id": "0GFKPJGmfGxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalLinkPredictor:\n",
        "    \"\"\"Enhanced trainer class for temporal link prediction with ranking metrics.\"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "    def train_epoch(self, dataloader: DataLoader, optimizer: optim.Optimizer) -> float:\n",
        "        \"\"\"Train for one epoch.\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            sequence = batch['sequence'].to(self.device)\n",
        "            current_features = batch['current_features'].to(self.device)\n",
        "            targets = batch['target'].to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(sequence, current_features)\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def evaluate(self, dataloader: DataLoader) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate the model with standard binary classification metrics.\"\"\"\n",
        "        self.model.eval()\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                sequence = batch['sequence'].to(self.device)\n",
        "                current_features = batch['current_features'].to(self.device)\n",
        "                targets = batch['target'].to(self.device)\n",
        "\n",
        "                outputs = self.model(sequence, current_features)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                all_predictions.extend(outputs.cpu().numpy())\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "        all_predictions = np.array(all_predictions)\n",
        "        all_targets = np.array(all_targets)\n",
        "\n",
        "        auc = roc_auc_score(all_targets, all_predictions)\n",
        "        ap = average_precision_score(all_targets, all_predictions)\n",
        "\n",
        "        return {\n",
        "            'loss': total_loss / len(dataloader),\n",
        "            'auc': auc,\n",
        "            'ap': ap\n",
        "        }\n",
        "\n",
        "    def calculate_ranking_metrics(self, test_dataset, edges_df: pd.DataFrame) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Calculate R@1, R@5, R@10, and MRR metrics for temporal link prediction.\n",
        "\n",
        "        For each positive test edge (src, dst, time), we:\n",
        "        1. Keep the source node and temporal context fixed\n",
        "        2. Replace dst with all other possible nodes to create negative samples\n",
        "        3. Get model predictions for all candidates\n",
        "        4. Rank based on prediction scores\n",
        "        5. Calculate metrics based on true destination's rank\n",
        "\n",
        "        Args:\n",
        "            test_dataset: Test dataset containing positive samples\n",
        "            edges_df: Full edges dataframe to get all possible nodes\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with R@1, R@5, R@10, and MRR scores\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        # Get all unique nodes from the dataset\n",
        "        all_nodes = set(edges_df['src'].unique()) | set(edges_df['dst'].unique())\n",
        "        all_nodes = sorted(list(all_nodes))\n",
        "\n",
        "        # Group test samples by (src, time, ext_roll) to process efficiently\n",
        "        test_samples_by_context = defaultdict(list)\n",
        "\n",
        "        # Extract positive test samples from dataset\n",
        "        positive_samples = []\n",
        "        for sample in test_dataset.samples:\n",
        "            if sample['target'] == 1:  # Only positive samples\n",
        "                positive_samples.append(sample)\n",
        "\n",
        "                # Group by source context for efficient processing\n",
        "                context_key = (sample['src'], sample['time'], sample['ext_roll'])\n",
        "                test_samples_by_context[context_key].append(sample)\n",
        "\n",
        "        print(f\"Evaluating ranking metrics on {len(positive_samples)} positive test samples...\")\n",
        "\n",
        "        ranks = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (context_key, context_samples) in enumerate(test_samples_by_context.items()):\n",
        "                src, time, ext_roll = context_key\n",
        "\n",
        "                if i % 100 == 0:\n",
        "                    print(f\"Processing context {i+1}/{len(test_samples_by_context)}\")\n",
        "\n",
        "                # For each positive sample in this context\n",
        "                for sample in context_samples:\n",
        "                    true_dst = sample['dst']\n",
        "                    sequence_features = sample['sequence']\n",
        "\n",
        "                    # Create candidate pairs: (src, candidate_dst) for all possible destinations\n",
        "                    candidate_scores = []\n",
        "                    candidate_nodes = []\n",
        "\n",
        "                    for candidate_dst in all_nodes:\n",
        "                        if candidate_dst == src:  # Skip self-loops\n",
        "                            continue\n",
        "\n",
        "                        # Create features for this candidate pair\n",
        "                        candidate_features = self._get_candidate_features(\n",
        "                            src, candidate_dst, time, ext_roll,\n",
        "                            test_dataset, edges_df\n",
        "                        )\n",
        "\n",
        "                        if candidate_features is not None:\n",
        "                            # Get model prediction\n",
        "                            sequence_batch = sequence_features.unsqueeze(0).float().to(self.device)\n",
        "                            features_batch = candidate_features.unsqueeze(0).float().to(self.device)\n",
        "\n",
        "                            score = self.model(sequence_batch, features_batch)\n",
        "                            candidate_scores.append(score.item())\n",
        "                            candidate_nodes.append(candidate_dst)\n",
        "\n",
        "                    # Rank candidates by score (higher is better)\n",
        "                    if len(candidate_scores) > 0:\n",
        "                        scored_candidates = list(zip(candidate_nodes, candidate_scores))\n",
        "                        scored_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                        # Find rank of true destination (1-indexed)\n",
        "                        true_rank = None\n",
        "                        for rank, (node, score) in enumerate(scored_candidates, 1):\n",
        "                            if node == true_dst:\n",
        "                                true_rank = rank\n",
        "                                break\n",
        "\n",
        "                        if true_rank is not None:\n",
        "                            ranks.append(true_rank)\n",
        "                        else:\n",
        "                            # If true destination not found in candidates, assign worst rank\n",
        "                            ranks.append(len(scored_candidates) + 1)\n",
        "\n",
        "        # Calculate metrics\n",
        "        ranks = np.array(ranks)\n",
        "\n",
        "        # Recall at K\n",
        "        r_at_1 = np.mean(ranks <= 1)\n",
        "        r_at_5 = np.mean(ranks <= 5)\n",
        "        r_at_10 = np.mean(ranks <= 10)\n",
        "\n",
        "        # Mean Reciprocal Rank\n",
        "        mrr = np.mean(1.0 / ranks)\n",
        "\n",
        "        return {\n",
        "            'R@1': r_at_1,\n",
        "            'R@5': r_at_5,\n",
        "            'R@10': r_at_10,\n",
        "            'MRR': mrr,\n",
        "            'mean_rank': np.mean(ranks),\n",
        "            'median_rank': np.median(ranks),\n",
        "            'num_samples': len(ranks)\n",
        "        }\n",
        "\n",
        "    def _get_candidate_features(self, src: int, dst: int, time: int, ext_roll: int,\n",
        "                              test_dataset, edges_df: pd.DataFrame) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Get features for a candidate (src, dst) pair using the same strategy\n",
        "        as the dataset's negative sampling approach.\n",
        "        \"\"\"\n",
        "        # Use the same feature generation strategy as the dataset\n",
        "        return test_dataset._get_features_for_negative(src, dst, time, ext_roll)\n",
        "\n",
        "    def evaluate_comprehensive(self, test_dataset, edges_df: pd.DataFrame,\n",
        "                             test_dataloader: DataLoader = None) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Comprehensive evaluation including both standard binary classification metrics\n",
        "        and ranking metrics.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: Test dataset for ranking evaluation\n",
        "            edges_df: Full edges dataframe\n",
        "            test_dataloader: Optional dataloader for binary classification metrics\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing all metrics\n",
        "        \"\"\"\n",
        "        print(\"Calculating standard binary classification metrics...\")\n",
        "\n",
        "        # Standard evaluation using dataloader if provided\n",
        "        if test_dataloader is not None:\n",
        "            standard_metrics = self.evaluate(test_dataloader)\n",
        "        else:\n",
        "            # Create temporary dataloader if not provided\n",
        "            temp_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "            standard_metrics = self.evaluate(temp_loader)\n",
        "\n",
        "        print(\"Calculating ranking metrics...\")\n",
        "\n",
        "        # Ranking metrics\n",
        "        ranking_metrics = self.calculate_ranking_metrics(test_dataset, edges_df)\n",
        "\n",
        "        # Combine all metrics\n",
        "        all_metrics = {\n",
        "            'loss': standard_metrics['loss'],\n",
        "            'auc': standard_metrics['auc'],\n",
        "            'ap': standard_metrics['ap'],\n",
        "            **ranking_metrics\n",
        "        }\n",
        "\n",
        "        return all_metrics\n",
        "\n",
        "    def print_evaluation_results(self, metrics: Dict[str, float]):\n",
        "        \"\"\"Pretty print evaluation results.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"EVALUATION RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(\"\\nBinary Classification Metrics:\")\n",
        "        if 'loss' in metrics:\n",
        "            print(f\"  Loss:                   {metrics['loss']:.4f}\")\n",
        "        print(f\"  AUC:                    {metrics['auc']:.4f}\")\n",
        "        print(f\"  Average Precision (AP): {metrics['ap']:.4f}\")\n",
        "\n",
        "        if 'R@1' in metrics:\n",
        "            print(\"\\nRanking Metrics:\")\n",
        "            print(f\"  R@1:                   {metrics['R@1']:.4f}\")\n",
        "            print(f\"  R@5:                   {metrics['R@5']:.4f}\")\n",
        "            print(f\"  R@10:                  {metrics['R@10']:.4f}\")\n",
        "            print(f\"  MRR:                   {metrics['MRR']:.4f}\")\n",
        "\n",
        "            print(f\"\\nRank Statistics:\")\n",
        "            print(f\"  Mean Rank:             {metrics['mean_rank']:.2f}\")\n",
        "            print(f\"  Median Rank:           {metrics['median_rank']:.2f}\")\n",
        "            print(f\"  Number of Samples:     {metrics['num_samples']}\")\n",
        "\n",
        "        print(\"=\"*60)\n",
        "\n",
        "\n",
        "        # \"\"\"\n",
        "        # Standalone function for backward compatibility.\n",
        "\n",
        "        # Args:\n",
        "        #     model: Trained temporal link prediction model\n",
        "        #     test_dataset: Test dataset containing positive samples\n",
        "        #     edges_df: Full edges dataframe to get all possible nodes\n",
        "        #     device: Device to run inference on\n",
        "\n",
        "        # Returns:\n",
        "        #     Dictionary with R@1, R@5, R@10, and MRR scores\n",
        "        # \"\"\"\n",
        "        # # Create a temporary trainer instance to use the class method\n",
        "        # trainer = TemporalLinkPredictor(model, device)\n",
        "        # return trainer.calculate_ranking_metrics(test_dataset, edges_df)\n",
        "\n",
        "\n",
        "def evaluate_model_with_ranking(model: torch.nn.Module, test_dataset, edges_df: pd.DataFrame,\n",
        "                               device: str = 'cuda') -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Standalone function for comprehensive evaluation (backward compatibility).\n",
        "    \"\"\"\n",
        "    trainer = TemporalLinkPredictor(model, device)\n",
        "    return trainer.evaluate_comprehensive(test_dataset, edges_df)\n",
        "\n",
        "\n",
        "def print_evaluation_results(metrics: Dict[str, float]):\n",
        "    \"\"\"Standalone function for printing results (backward compatibility).\"\"\"\n",
        "    # Create a dummy trainer instance to use the class method\n",
        "    dummy_model = torch.nn.Linear(1, 1)  # Dummy model just for the printer\n",
        "    trainer = TemporalLinkPredictor(dummy_model)\n",
        "    trainer.print_evaluation_results(metrics)"
      ],
      "metadata": {
        "id": "mL1NuS5kfQxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(edges_df: pd.DataFrame, edge_features: torch.Tensor,\n",
        "                              sequence_length: int = 10, batch_size: int = 32):\n",
        "    \"\"\"Create dataloaders with improved negative sampling.\"\"\"\n",
        "\n",
        "    # Split data based on ext_roll\n",
        "    train_df = edges_df[edges_df['ext_roll'] == 0].copy()\n",
        "    val_df = edges_df[edges_df['ext_roll'] == 1].copy()\n",
        "    test_df = edges_df[edges_df['ext_roll'] == 2].copy()\n",
        "\n",
        "    # Get corresponding features\n",
        "    train_features = edge_features[edges_df['ext_roll'] == 0]\n",
        "    val_features = edge_features[edges_df['ext_roll'] == 1]\n",
        "    test_features = edge_features[edges_df['ext_roll'] == 2]\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TemporalLinkDataset(train_df, train_features, sequence_length)\n",
        "    val_dataset = TemporalLinkDataset(val_df, val_features, sequence_length)\n",
        "    test_dataset = TemporalLinkDataset(test_df, test_features, sequence_length)\n",
        "\n",
        "    # Create dataloaders\n",
        "    from torch.utils.data import DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def main(edges_df_path, edge_features_path):\n",
        "    # Load data (same as your existing code)\n",
        "    edges_df = pd.read_csv(edges_df_path)\n",
        "    edge_features = torch.load(edge_features_path)\n",
        "\n",
        "    print(f\"Loaded {len(edges_df)} edges and {edge_features.shape[0]} edge features\")\n",
        "\n",
        "    # Hyperparameters\n",
        "    sequence_length = 20\n",
        "    hidden_dim = 128\n",
        "    num_layers = 2\n",
        "    dropout = 0.2\n",
        "    batch_size = 128\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 100\n",
        "    use_gru = False\n",
        "\n",
        "    # Create dataloaders (using your existing function)\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(\n",
        "        edges_df, edge_features, sequence_length, batch_size\n",
        "    )\n",
        "\n",
        "    # Create model (using your existing class)\n",
        "    model = TemporalLSTMPredictor(\n",
        "        input_dim=2,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        use_gru=use_gru\n",
        "    )\n",
        "\n",
        "    # Create enhanced trainer\n",
        "    trainer = TemporalLinkPredictor(model)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
        "\n",
        "    print(f\"\\nTraining {'GRU' if use_gru else 'LSTM'} model...\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "\n",
        "    # Training loop\n",
        "    best_val_auc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train\n",
        "        train_loss = trainer.train_epoch(train_loader, optimizer)\n",
        "\n",
        "        # Validate\n",
        "        val_metrics = trainer.evaluate(val_loader)\n",
        "        scheduler.step(val_metrics['loss'])\n",
        "\n",
        "        # Save best model\n",
        "        if val_metrics['auc'] > best_val_auc:\n",
        "            best_val_auc = val_metrics['auc']\n",
        "            torch.save(model.state_dict(), f'best_{\"gru\" if use_gru else \"lstm\"}_model.pt')\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | \"\n",
        "                  f\"Val Loss: {val_metrics['loss']:.4f} | Val AUC: {val_metrics['auc']:.4f} | \"\n",
        "                  f\"Val AP: {val_metrics['ap']:.4f}\")\n",
        "\n",
        "    # Load best model\n",
        "    trainer.model.load_state_dict(torch.load(f'best_{\"gru\" if use_gru else \"lstm\"}_model.pt'))\n",
        "\n",
        "    # Prepare test dataset for ranking evaluation\n",
        "    test_df = edges_df[edges_df['ext_roll'] == 2].copy()\n",
        "    test_features = edge_features[edges_df['ext_roll'] == 2]\n",
        "    test_dataset = TemporalLinkDataset(test_df, test_features, sequence_length)\n",
        "\n",
        "    # Comprehensive evaluation with both standard and ranking metrics\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL EVALUATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    all_metrics = trainer.evaluate_comprehensive(\n",
        "        test_dataset=test_dataset,\n",
        "        edges_df=edges_df,\n",
        "        test_dataloader=test_loader\n",
        "    )\n",
        "\n",
        "    # Print results using the class method\n",
        "    trainer.print_evaluation_results(all_metrics)\n",
        "\n",
        "    return all_metrics, trainer"
      ],
      "metadata": {
        "id": "APhbQlHOfWX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Positive + Negative**"
      ],
      "metadata": {
        "id": "SnNDqlXMAd8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edges_df_path, edge_features_path = 'leadlag/edges.csv', 'leadlag/edge_features.pt'\n",
        "main(edges_df_path, edge_features_path)"
      ],
      "metadata": {
        "id": "p_gSWYA3VhKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0fe6681-3c07-4da4-d002-49c22523420c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 9440 edges and 9440 edge features\n",
            "Created dataset with 17372 samples\n",
            "Positive samples: 8686\n",
            "Negative samples: 8686\n",
            "Created dataset with 746 samples\n",
            "Positive samples: 373\n",
            "Negative samples: 373\n",
            "Created dataset with 762 samples\n",
            "Positive samples: 381\n",
            "Negative samples: 381\n",
            "\n",
            "Training LSTM model...\n",
            "Model parameters: 65409\n",
            "Epoch   0 | Train Loss: 0.6971 | Val Loss: 0.6955 | Val AUC: 0.4828 | Val AP: 0.4930\n",
            "Epoch  10 | Train Loss: 0.6842 | Val Loss: 0.6961 | Val AUC: 0.5007 | Val AP: 0.4922\n",
            "Epoch  20 | Train Loss: 0.6587 | Val Loss: 0.6941 | Val AUC: 0.5006 | Val AP: 0.5004\n",
            "Epoch  30 | Train Loss: 0.6478 | Val Loss: 0.6937 | Val AUC: 0.5214 | Val AP: 0.5208\n",
            "Epoch  40 | Train Loss: 0.6424 | Val Loss: 0.6943 | Val AUC: 0.5251 | Val AP: 0.5229\n",
            "Epoch  50 | Train Loss: 0.6407 | Val Loss: 0.6945 | Val AUC: 0.5261 | Val AP: 0.5231\n",
            "Epoch  60 | Train Loss: 0.6397 | Val Loss: 0.6946 | Val AUC: 0.5266 | Val AP: 0.5234\n",
            "Epoch  70 | Train Loss: 0.6409 | Val Loss: 0.6946 | Val AUC: 0.5267 | Val AP: 0.5235\n",
            "Epoch  80 | Train Loss: 0.6406 | Val Loss: 0.6946 | Val AUC: 0.5268 | Val AP: 0.5235\n",
            "Epoch  90 | Train Loss: 0.6400 | Val Loss: 0.6946 | Val AUC: 0.5268 | Val AP: 0.5235\n",
            "Created dataset with 762 samples\n",
            "Positive samples: 381\n",
            "Negative samples: 381\n",
            "\n",
            "==================================================\n",
            "FINAL EVALUATION\n",
            "==================================================\n",
            "Calculating standard binary classification metrics...\n",
            "Calculating ranking metrics...\n",
            "Evaluating ranking metrics on 381 positive test samples...\n",
            "Processing context 1/160\n",
            "Processing context 101/160\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Binary Classification Metrics:\n",
            "  Loss:                   0.6949\n",
            "  AUC:                    0.5193\n",
            "  Average Precision (AP): 0.5333\n",
            "\n",
            "Ranking Metrics:\n",
            "  R@1:                   0.0656\n",
            "  R@5:                   0.2205\n",
            "  R@10:                  0.3832\n",
            "  MRR:                   0.1735\n",
            "\n",
            "Rank Statistics:\n",
            "  Mean Rank:             14.73\n",
            "  Median Rank:           15.00\n",
            "  Number of Samples:     381\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'loss': 0.6948979496955872,\n",
              "  'auc': np.float64(0.5192923719180772),\n",
              "  'ap': np.float64(0.5332981736199693),\n",
              "  'R@1': np.float64(0.06561679790026247),\n",
              "  'R@5': np.float64(0.2204724409448819),\n",
              "  'R@10': np.float64(0.38320209973753283),\n",
              "  'MRR': np.float64(0.1734564224512255),\n",
              "  'mean_rank': np.float64(14.729658792650918),\n",
              "  'median_rank': np.float64(15.0),\n",
              "  'num_samples': 381},\n",
              " <__main__.TemporalLinkPredictor at 0x7aa45d163bd0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positive only"
      ],
      "metadata": {
        "id": "McBlH6-rJKqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edges_df_path, edge_features_path = 'positive/edges.csv', 'positive/edge_features.pt'\n",
        "main(edges_df_path, edge_features_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4Qz2QSdHxH1",
        "outputId": "10115676-00fb-4334-a962-d2492615abe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2604 edges and 2604 edge features\n",
            "Created dataset with 4660 samples\n",
            "Positive samples: 2330\n",
            "Negative samples: 2330\n",
            "Created dataset with 338 samples\n",
            "Positive samples: 169\n",
            "Negative samples: 169\n",
            "Created dataset with 210 samples\n",
            "Positive samples: 105\n",
            "Negative samples: 105\n",
            "\n",
            "Training LSTM model...\n",
            "Model parameters: 257793\n",
            "Epoch   0 | Train Loss: 0.7038 | Val Loss: 0.6932 | Val AUC: 0.5040 | Val AP: 0.4990\n",
            "Epoch  10 | Train Loss: 0.6908 | Val Loss: 0.6983 | Val AUC: 0.4811 | Val AP: 0.4988\n",
            "Epoch  20 | Train Loss: 0.6897 | Val Loss: 0.6958 | Val AUC: 0.4815 | Val AP: 0.4917\n",
            "Epoch  30 | Train Loss: 0.6827 | Val Loss: 0.6995 | Val AUC: 0.4813 | Val AP: 0.4932\n",
            "Epoch  40 | Train Loss: 0.6789 | Val Loss: 0.7003 | Val AUC: 0.4808 | Val AP: 0.4921\n",
            "Epoch  50 | Train Loss: 0.6798 | Val Loss: 0.7004 | Val AUC: 0.4810 | Val AP: 0.4923\n",
            "Epoch  60 | Train Loss: 0.6793 | Val Loss: 0.7004 | Val AUC: 0.4809 | Val AP: 0.4923\n",
            "Epoch  70 | Train Loss: 0.6789 | Val Loss: 0.7004 | Val AUC: 0.4809 | Val AP: 0.4923\n",
            "Epoch  80 | Train Loss: 0.6789 | Val Loss: 0.7004 | Val AUC: 0.4809 | Val AP: 0.4923\n",
            "Epoch  90 | Train Loss: 0.6793 | Val Loss: 0.7004 | Val AUC: 0.4809 | Val AP: 0.4923\n",
            "Created dataset with 210 samples\n",
            "Positive samples: 105\n",
            "Negative samples: 105\n",
            "\n",
            "==================================================\n",
            "FINAL EVALUATION\n",
            "==================================================\n",
            "Calculating standard binary classification metrics...\n",
            "Calculating ranking metrics...\n",
            "Evaluating ranking metrics on 105 positive test samples...\n",
            "Processing context 1/56\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Binary Classification Metrics:\n",
            "  Loss:                   0.6919\n",
            "  AUC:                    0.5240\n",
            "  Average Precision (AP): 0.5236\n",
            "\n",
            "Ranking Metrics:\n",
            "  R@1:                   0.0095\n",
            "  R@5:                   0.1619\n",
            "  R@10:                  0.3333\n",
            "  MRR:                   0.1215\n",
            "\n",
            "Rank Statistics:\n",
            "  Mean Rank:             15.17\n",
            "  Median Rank:           15.00\n",
            "  Number of Samples:     105\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'loss': 0.6919388175010681,\n",
              "  'auc': np.float64(0.5239909297052154),\n",
              "  'ap': np.float64(0.5236112611717691),\n",
              "  'R@1': np.float64(0.009523809523809525),\n",
              "  'R@5': np.float64(0.1619047619047619),\n",
              "  'R@10': np.float64(0.3333333333333333),\n",
              "  'MRR': np.float64(0.1214554935202401),\n",
              "  'mean_rank': np.float64(15.17142857142857),\n",
              "  'median_rank': np.float64(15.0),\n",
              "  'num_samples': 105},\n",
              " <__main__.TemporalLinkPredictor at 0x7b4018918ad0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnpsUbgya4NG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}