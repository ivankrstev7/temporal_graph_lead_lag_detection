{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl-B4-UJoE0G",
        "outputId": "3742062c-16ae-4c92-8cb8-085e38ae3171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ],
      "metadata": {
        "id": "jMrdRcwsoObG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/amazon-science/tgl.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUUlY_UNpCBP",
        "outputId": "69bbeee0-10a3-409c-9232-286cec83d0ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tgl'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 97 (delta 37), reused 10 (delta 10), pack-reused 42 (from 1)\u001b[K\n",
            "Receiving objects: 100% (97/97), 22.34 MiB | 14.79 MiB/s, done.\n",
            "Resolving deltas: 100% (38/38), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-asYK5_gFqQB",
        "outputId": "e0996cfa-6c4f-44cb-ecae-9ad14497db0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.0%2Bcu121-cp311-cp311-linux_x86_64.whl (799.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.1/799.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.19.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.4.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.0) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0+cu121 torchaudio-2.4.0+cu121 torchvision-0.19.0+cu121 triton-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjXvHF5-Fqf2",
        "outputId": "55425f62-eed0-43c7-d293-15ac13f7d450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch\n",
        "\n",
        "# pytorch-geometric\n",
        "!pip install --verbose torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "\n",
        "# pybind11 (used for c++ sampler)\n",
        "!pip install pybind11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn6-VMCJFqtA",
        "outputId": "febeb9bd-2114-4668-9497-6befa0df7283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_scatter-2.1.2%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_sparse-0.6.18%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_cluster-1.6.3%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt24cu121-cp311-cp311-linux_x86_64.whl (989 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m989.8/989.8 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Obtaining dependency information for torch-geometric from https://files.pythonhosted.org/packages/03/9f/157e913626c1acfb3b19ce000b1a6e4e4fb177c0bc0ea0c67ca5bd714b5a/torch_geometric-2.6.1-py3-none-any.whl.metadata\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.16.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.12.14)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.7.14)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.14.1)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster, torch-geometric\n",
            "Successfully installed torch-cluster-1.6.3+pt24cu121 torch-geometric-2.6.1 torch-scatter-2.1.2+pt24cu121 torch-sparse-0.6.18+pt24cu121 torch-spline-conv-1.2.2+pt24cu121\n",
            "Collecting pybind11\n",
            "  Downloading pybind11-3.0.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Downloading pybind11-3.0.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybind11\n",
            "Successfully installed pybind11-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybind11"
      ],
      "metadata": {
        "id": "wHjDjN4NwuQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ca3fd5-a7f2-4cb6-aaa8-77610bd19ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.11/dist-packages (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jedi"
      ],
      "metadata": {
        "id": "2sh_vtBV0S0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1b19aa-0f82-460d-f6b4-306d6ffed36f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jedi\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi) (0.8.4)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Wheel needed: cp{sys.version_info.major}{sys.version_info.minor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0zAaLUYxGer",
        "outputId": "88faeae5-6482-49d0-f060-29414e764d80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            "Wheel needed: cp311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install /content/dgl-2.4.0+cu121-cp311-cp311-manylinux1_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BonugUSqyx9c",
        "outputId": "d57b3230-9aef-4df9-b757-8c8533371c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./dgl-2.4.0+cu121-cp311-cp311-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl==2.4.0+cu121) (3.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl==2.4.0+cu121) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from dgl==2.4.0+cu121) (25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dgl==2.4.0+cu121) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl==2.4.0+cu121) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from dgl==2.4.0+cu121) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from dgl==2.4.0+cu121) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl==2.4.0+cu121) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl==2.4.0+cu121) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl==2.4.0+cu121) (4.67.1)\n",
            "Requirement already satisfied: torch<=2.4.0 in /usr/local/lib/python3.11/dist-packages (from dgl==2.4.0+cu121) (2.4.0+cu121)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl==2.4.0+cu121) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl==2.4.0+cu121) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl==2.4.0+cu121) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->dgl==2.4.0+cu121) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==2.4.0+cu121) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==2.4.0+cu121) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==2.4.0+cu121) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==2.4.0+cu121) (2025.7.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (3.18.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0->dgl==2.4.0+cu121) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.4.0->dgl==2.4.0+cu121) (12.5.82)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl==2.4.0+cu121) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl==2.4.0+cu121) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dgl==2.4.0+cu121) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->dgl==2.4.0+cu121) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<=2.4.0->dgl==2.4.0+cu121) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<=2.4.0->dgl==2.4.0+cu121) (1.3.0)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-2.4.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tgl"
      ],
      "metadata": {
        "id": "vecFc5fCJDxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir DATA"
      ],
      "metadata": {
        "id": "57_Qq1ycFABt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd tgl/DATA && mkdir leadlag\n",
        "# !mv ./edges.csv tgl/DATA/leadlag\n",
        "# !mv ./node_features.pt tgl/DATA/leadlag\n",
        "# !mv ./edge_features.pt tgl/DATA/leadlag"
      ],
      "metadata": {
        "id": "msrkeulOIf-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py build_ext --inplace"
      ],
      "metadata": {
        "id": "yiJ22A2fKyyv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d88e8d8-b12c-4bbb-ff45-175c5acb5d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running build_ext\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gen_graph.py --data leadlag"
      ],
      "metadata": {
        "id": "Kqjd_P8FLd9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TGN**"
      ],
      "metadata": {
        "id": "ansQJoJ7FDwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data leadlag --config ./config/TGN.yml --eval_neg_samples 30"
      ],
      "metadata": {
        "id": "NZf9t0PeUwy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88d256c6-f97b-4fcb-8f2e-ec305449e5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/utils.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  node_feats = torch.load('DATA/{}/node_features.pt'.format(d))\n",
            "Epoch 0:\n",
            "\ttrain loss:11691.7658 train ap:0.585925  train auc:0.598255  val loss:4.917857 val ap:0.690725  val auc:0.732509\n",
            "\ttotal time:2.73s sample time:0.00s prep time:0.30s\n",
            "Epoch 1:\n",
            "\ttrain loss:11108.8877 train ap:0.631844  train auc:0.663204  val loss:4.665054 val ap:0.755381  val auc:0.786741\n",
            "\ttotal time:1.77s sample time:0.00s prep time:0.25s\n",
            "Epoch 2:\n",
            "\ttrain loss:10951.3795 train ap:0.641628  train auc:0.670678  val loss:5.006509 val ap:0.686248  val auc:0.720256\n",
            "\ttotal time:0.97s sample time:0.00s prep time:0.14s\n",
            "Epoch 3:\n",
            "\ttrain loss:10862.4329 train ap:0.640750  train auc:0.679145  val loss:4.678687 val ap:0.742256  val auc:0.770028\n",
            "\ttotal time:0.95s sample time:0.00s prep time:0.14s\n",
            "Epoch 4:\n",
            "\ttrain loss:10760.2017 train ap:0.648142  train auc:0.685675  val loss:4.621093 val ap:0.704671  val auc:0.753682\n",
            "\ttotal time:0.98s sample time:0.00s prep time:0.14s\n",
            "Epoch 5:\n",
            "\ttrain loss:10611.7963 train ap:0.657037  train auc:0.694674  val loss:4.625788 val ap:0.696047  val auc:0.744439\n",
            "\ttotal time:1.08s sample time:0.00s prep time:0.15s\n",
            "Epoch 6:\n",
            "\ttrain loss:10663.1833 train ap:0.652549  train auc:0.692238  val loss:4.699815 val ap:0.707929  val auc:0.734067\n",
            "\ttotal time:1.40s sample time:0.00s prep time:0.19s\n",
            "Epoch 7:\n",
            "\ttrain loss:10551.5651 train ap:0.661389  train auc:0.703059  val loss:4.628548 val ap:0.728005  val auc:0.745741\n",
            "\ttotal time:1.31s sample time:0.00s prep time:0.18s\n",
            "Epoch 8:\n",
            "\ttrain loss:10445.5360 train ap:0.671707  train auc:0.710441  val loss:4.411283 val ap:0.718427  val auc:0.778180\n",
            "\ttotal time:1.45s sample time:0.00s prep time:0.20s\n",
            "Epoch 9:\n",
            "\ttrain loss:10432.0760 train ap:0.665555  train auc:0.706398  val loss:4.473301 val ap:0.741289  val auc:0.769407\n",
            "\ttotal time:0.97s sample time:0.00s prep time:0.14s\n",
            "Epoch 10:\n",
            "\ttrain loss:10462.9753 train ap:0.667506  train auc:0.706969  val loss:4.739144 val ap:0.677914  val auc:0.729151\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.14s\n",
            "Epoch 11:\n",
            "\ttrain loss:10342.8121 train ap:0.672059  train auc:0.713946  val loss:4.473466 val ap:0.732901  val auc:0.764170\n",
            "\ttotal time:0.98s sample time:0.00s prep time:0.14s\n",
            "Epoch 12:\n",
            "\ttrain loss:10240.0348 train ap:0.685591  train auc:0.722995  val loss:4.525089 val ap:0.725958  val auc:0.764636\n",
            "\ttotal time:0.96s sample time:0.00s prep time:0.14s\n",
            "Epoch 13:\n",
            "\ttrain loss:10269.6055 train ap:0.676217  train auc:0.720360  val loss:4.598114 val ap:0.711979  val auc:0.752264\n",
            "\ttotal time:1.02s sample time:0.00s prep time:0.14s\n",
            "Epoch 14:\n",
            "\ttrain loss:10221.6862 train ap:0.680805  train auc:0.723815  val loss:4.633749 val ap:0.711092  val auc:0.754746\n",
            "\ttotal time:1.07s sample time:0.00s prep time:0.14s\n",
            "Epoch 15:\n",
            "\ttrain loss:10208.6788 train ap:0.677566  train auc:0.723356  val loss:4.381089 val ap:0.753669  val auc:0.792491\n",
            "\ttotal time:1.09s sample time:0.00s prep time:0.14s\n",
            "Epoch 16:\n",
            "\ttrain loss:10037.8723 train ap:0.695229  train auc:0.738201  val loss:4.577801 val ap:0.728090  val auc:0.770832\n",
            "\ttotal time:1.08s sample time:0.00s prep time:0.14s\n",
            "Epoch 17:\n",
            "\ttrain loss:10095.1951 train ap:0.683120  train auc:0.728123  val loss:5.051995 val ap:0.658155  val auc:0.682730\n",
            "\ttotal time:1.07s sample time:0.00s prep time:0.14s\n",
            "Epoch 18:\n",
            "\ttrain loss:10057.5094 train ap:0.696205  train auc:0.737358  val loss:4.637496 val ap:0.767993  val auc:0.786684\n",
            "\ttotal time:1.30s sample time:0.00s prep time:0.17s\n",
            "Epoch 19:\n",
            "\ttrain loss:9992.4091 train ap:0.692865  train auc:0.739374  val loss:4.580496 val ap:0.722953  val auc:0.763193\n",
            "\ttotal time:1.45s sample time:0.00s prep time:0.18s\n",
            "Epoch 20:\n",
            "\ttrain loss:10151.1863 train ap:0.691941  train auc:0.733661  val loss:4.557104 val ap:0.754309  val auc:0.793708\n",
            "\ttotal time:1.51s sample time:0.00s prep time:0.20s\n",
            "Epoch 21:\n",
            "\ttrain loss:9928.3579 train ap:0.693421  train auc:0.739727  val loss:4.653775 val ap:0.719110  val auc:0.766596\n",
            "\ttotal time:1.05s sample time:0.00s prep time:0.14s\n",
            "Epoch 22:\n",
            "\ttrain loss:9956.1516 train ap:0.691625  train auc:0.737886  val loss:4.703575 val ap:0.742487  val auc:0.769539\n",
            "\ttotal time:1.06s sample time:0.00s prep time:0.14s\n",
            "Epoch 23:\n",
            "\ttrain loss:9969.0989 train ap:0.689623  train auc:0.736286  val loss:4.412093 val ap:0.748172  val auc:0.787863\n",
            "\ttotal time:1.09s sample time:0.00s prep time:0.14s\n",
            "Epoch 24:\n",
            "\ttrain loss:9858.5464 train ap:0.702920  train auc:0.746206  val loss:4.843650 val ap:0.705413  val auc:0.742922\n",
            "\ttotal time:1.10s sample time:0.00s prep time:0.14s\n",
            "Epoch 25:\n",
            "\ttrain loss:9819.0146 train ap:0.705005  train auc:0.746799  val loss:4.924473 val ap:0.716365  val auc:0.758717\n",
            "\ttotal time:1.06s sample time:0.00s prep time:0.14s\n",
            "Epoch 26:\n",
            "\ttrain loss:9764.8213 train ap:0.706723  train auc:0.752111  val loss:5.261603 val ap:0.679771  val auc:0.719133\n",
            "\ttotal time:1.37s sample time:0.00s prep time:0.18s\n",
            "Epoch 27:\n",
            "\ttrain loss:9651.4212 train ap:0.712973  train auc:0.759049  val loss:5.321316 val ap:0.687416  val auc:0.712529\n",
            "\ttotal time:1.04s sample time:0.00s prep time:0.14s\n",
            "Epoch 28:\n",
            "\ttrain loss:9837.9787 train ap:0.701320  train auc:0.746286  val loss:4.901676 val ap:0.742476  val auc:0.772633\n",
            "\ttotal time:1.05s sample time:0.00s prep time:0.14s\n",
            "Epoch 29:\n",
            "\ttrain loss:9753.9712 train ap:0.698088  train auc:0.748629  val loss:5.337284 val ap:0.759353  val auc:0.771897\n",
            "\ttotal time:1.22s sample time:0.00s prep time:0.16s\n",
            "Epoch 30:\n",
            "\ttrain loss:9696.7030 train ap:0.701746  train auc:0.749318  val loss:5.034893 val ap:0.721628  val auc:0.745915\n",
            "\ttotal time:1.39s sample time:0.00s prep time:0.18s\n",
            "Epoch 31:\n",
            "\ttrain loss:9552.0686 train ap:0.715419  train auc:0.758671  val loss:4.771281 val ap:0.753282  val auc:0.789974\n",
            "\ttotal time:1.52s sample time:0.00s prep time:0.20s\n",
            "Epoch 32:\n",
            "\ttrain loss:9591.2141 train ap:0.701971  train auc:0.754741  val loss:4.732537 val ap:0.696655  val auc:0.746405\n",
            "\ttotal time:1.23s sample time:0.00s prep time:0.16s\n",
            "Epoch 33:\n",
            "\ttrain loss:9615.5516 train ap:0.707333  train auc:0.754189  val loss:5.163924 val ap:0.705258  val auc:0.741168\n",
            "\ttotal time:1.08s sample time:0.00s prep time:0.14s\n",
            "Epoch 34:\n",
            "\ttrain loss:9635.7516 train ap:0.698506  train auc:0.749252  val loss:5.127355 val ap:0.717991  val auc:0.750666\n",
            "\ttotal time:1.10s sample time:0.00s prep time:0.15s\n",
            "Epoch 35:\n",
            "\ttrain loss:9471.9320 train ap:0.714033  train auc:0.761914  val loss:5.911036 val ap:0.652426  val auc:0.675653\n",
            "\ttotal time:1.10s sample time:0.00s prep time:0.14s\n",
            "Epoch 36:\n",
            "\ttrain loss:9590.5843 train ap:0.717467  train auc:0.763783  val loss:5.029556 val ap:0.716829  val auc:0.739707\n",
            "\ttotal time:1.05s sample time:0.00s prep time:0.14s\n",
            "Epoch 37:\n",
            "\ttrain loss:9532.7963 train ap:0.710440  train auc:0.759123  val loss:4.936153 val ap:0.726960  val auc:0.757100\n",
            "\ttotal time:1.04s sample time:0.00s prep time:0.14s\n",
            "Epoch 38:\n",
            "\ttrain loss:9445.9854 train ap:0.722323  train auc:0.771186  val loss:5.115417 val ap:0.721246  val auc:0.739350\n",
            "\ttotal time:1.06s sample time:0.00s prep time:0.14s\n",
            "Epoch 39:\n",
            "\ttrain loss:9474.1034 train ap:0.717067  train auc:0.765943  val loss:5.993980 val ap:0.683766  val auc:0.715789\n",
            "\ttotal time:1.07s sample time:0.00s prep time:0.14s\n",
            "Epoch 40:\n",
            "\ttrain loss:9318.3628 train ap:0.721192  train auc:0.771343  val loss:5.438389 val ap:0.721752  val auc:0.756394\n",
            "\ttotal time:1.08s sample time:0.00s prep time:0.14s\n",
            "Epoch 41:\n",
            "\ttrain loss:9371.3829 train ap:0.720187  train auc:0.769440  val loss:5.749806 val ap:0.682799  val auc:0.727668\n",
            "\ttotal time:1.36s sample time:0.00s prep time:0.18s\n",
            "Epoch 42:\n",
            "\ttrain loss:9230.0620 train ap:0.724250  train auc:0.774539  val loss:5.616028 val ap:0.723102  val auc:0.755134\n",
            "\ttotal time:1.46s sample time:0.00s prep time:0.19s\n",
            "Epoch 43:\n",
            "\ttrain loss:9356.4221 train ap:0.717077  train auc:0.767681  val loss:5.606001 val ap:0.736995  val auc:0.750419\n",
            "\ttotal time:1.53s sample time:0.00s prep time:0.20s\n",
            "Epoch 44:\n",
            "\ttrain loss:9271.9975 train ap:0.719990  train auc:0.769349  val loss:5.707060 val ap:0.703711  val auc:0.725245\n",
            "\ttotal time:1.10s sample time:0.00s prep time:0.14s\n",
            "Epoch 45:\n",
            "\ttrain loss:9241.1263 train ap:0.724954  train auc:0.775026  val loss:5.563608 val ap:0.755054  val auc:0.765945\n",
            "\ttotal time:1.07s sample time:0.00s prep time:0.14s\n",
            "Epoch 46:\n",
            "\ttrain loss:9269.1127 train ap:0.720485  train auc:0.774875  val loss:5.460505 val ap:0.739495  val auc:0.763314\n",
            "\ttotal time:1.06s sample time:0.00s prep time:0.14s\n",
            "Epoch 47:\n",
            "\ttrain loss:9200.1856 train ap:0.731108  train auc:0.776663  val loss:6.022217 val ap:0.708429  val auc:0.737353\n",
            "\ttotal time:1.05s sample time:0.00s prep time:0.14s\n",
            "Epoch 48:\n",
            "\ttrain loss:9373.7642 train ap:0.715373  train auc:0.766291  val loss:5.418029 val ap:0.717981  val auc:0.755928\n",
            "\ttotal time:1.09s sample time:0.00s prep time:0.14s\n",
            "Epoch 49:\n",
            "\ttrain loss:9207.3507 train ap:0.723057  train auc:0.774477  val loss:5.283617 val ap:0.730762  val auc:0.755384\n",
            "\ttotal time:1.05s sample time:0.00s prep time:0.14s\n",
            "Epoch 50:\n",
            "\ttrain loss:9165.7301 train ap:0.730398  train auc:0.777709  val loss:6.378031 val ap:0.662394  val auc:0.712160\n",
            "\ttotal time:1.12s sample time:0.00s prep time:0.14s\n",
            "Epoch 51:\n",
            "\ttrain loss:9118.6712 train ap:0.733619  train auc:0.779899  val loss:6.318182 val ap:0.676969  val auc:0.712725\n",
            "\ttotal time:1.10s sample time:0.00s prep time:0.15s\n",
            "Epoch 52:\n",
            "\ttrain loss:9105.6972 train ap:0.730652  train auc:0.781206  val loss:5.938698 val ap:0.719829  val auc:0.737851\n",
            "\ttotal time:1.19s sample time:0.00s prep time:0.15s\n",
            "Epoch 53:\n",
            "\ttrain loss:9236.5579 train ap:0.724787  train auc:0.772265  val loss:6.598066 val ap:0.711069  val auc:0.734993\n",
            "\ttotal time:1.43s sample time:0.00s prep time:0.19s\n",
            "Epoch 54:\n",
            "\ttrain loss:9063.1239 train ap:0.733497  train auc:0.781847  val loss:5.608859 val ap:0.703445  val auc:0.740240\n",
            "\ttotal time:1.50s sample time:0.00s prep time:0.19s\n",
            "Epoch 55:\n",
            "\ttrain loss:8970.1501 train ap:0.742602  train auc:0.789845  val loss:5.625185 val ap:0.723794  val auc:0.767468\n",
            "\ttotal time:1.75s sample time:0.00s prep time:0.19s\n",
            "Epoch 56:\n",
            "\ttrain loss:9112.3368 train ap:0.730716  train auc:0.781685  val loss:6.544184 val ap:0.705451  val auc:0.724547\n",
            "\ttotal time:1.51s sample time:0.00s prep time:0.18s\n",
            "Epoch 57:\n",
            "\ttrain loss:9070.8295 train ap:0.735335  train auc:0.783879  val loss:6.670206 val ap:0.666489  val auc:0.712970\n",
            "\ttotal time:1.09s sample time:0.00s prep time:0.14s\n",
            "Epoch 58:\n",
            "\ttrain loss:9048.7331 train ap:0.732994  train auc:0.782244  val loss:6.844787 val ap:0.702204  val auc:0.723367\n",
            "\ttotal time:1.11s sample time:0.00s prep time:0.14s\n",
            "Epoch 59:\n",
            "\ttrain loss:8984.2904 train ap:0.734121  train auc:0.785201  val loss:6.121339 val ap:0.726069  val auc:0.742236\n",
            "\ttotal time:1.09s sample time:0.00s prep time:0.15s\n",
            "Epoch 60:\n",
            "\ttrain loss:9017.3544 train ap:0.732414  train auc:0.784318  val loss:6.271159 val ap:0.680979  val auc:0.708775\n",
            "\ttotal time:1.13s sample time:0.00s prep time:0.15s\n",
            "Epoch 61:\n",
            "\ttrain loss:9050.3703 train ap:0.727274  train auc:0.779095  val loss:7.536609 val ap:0.666531  val auc:0.687643\n",
            "\ttotal time:1.11s sample time:0.00s prep time:0.15s\n",
            "Epoch 62:\n",
            "\ttrain loss:8812.6439 train ap:0.742903  train auc:0.792045  val loss:7.265263 val ap:0.651220  val auc:0.685204\n",
            "\ttotal time:1.09s sample time:0.00s prep time:0.14s\n",
            "Epoch 63:\n",
            "\ttrain loss:8753.9433 train ap:0.742031  train auc:0.795233  val loss:6.141618 val ap:0.722428  val auc:0.759212\n",
            "\ttotal time:1.32s sample time:0.00s prep time:0.17s\n",
            "Epoch 64:\n",
            "\ttrain loss:8888.5184 train ap:0.737907  train auc:0.790213  val loss:7.634749 val ap:0.717682  val auc:0.751438\n",
            "\ttotal time:1.48s sample time:0.00s prep time:0.19s\n",
            "Epoch 65:\n",
            "\ttrain loss:9009.7257 train ap:0.728725  train auc:0.781854  val loss:6.759024 val ap:0.698024  val auc:0.729500\n",
            "\ttotal time:1.55s sample time:0.00s prep time:0.20s\n",
            "Epoch 66:\n",
            "\ttrain loss:9045.9226 train ap:0.732670  train auc:0.783421  val loss:6.189707 val ap:0.711179  val auc:0.725670\n",
            "\ttotal time:1.08s sample time:0.00s prep time:0.14s\n",
            "Epoch 67:\n",
            "\ttrain loss:9000.3069 train ap:0.733083  train auc:0.780218  val loss:5.727112 val ap:0.727241  val auc:0.761167\n",
            "\ttotal time:1.13s sample time:0.00s prep time:0.15s\n",
            "Epoch 68:\n",
            "\ttrain loss:8903.3851 train ap:0.740991  train auc:0.789234  val loss:7.006485 val ap:0.684739  val auc:0.720063\n",
            "\ttotal time:1.07s sample time:0.00s prep time:0.14s\n",
            "Epoch 69:\n",
            "\ttrain loss:8898.4284 train ap:0.737611  train auc:0.788957  val loss:6.501189 val ap:0.701434  val auc:0.738658\n",
            "\ttotal time:1.12s sample time:0.00s prep time:0.14s\n",
            "Epoch 70:\n",
            "\ttrain loss:8880.3008 train ap:0.733632  train auc:0.787882  val loss:8.241469 val ap:0.660029  val auc:0.677532\n",
            "\ttotal time:1.06s sample time:0.00s prep time:0.14s\n",
            "Epoch 71:\n",
            "\ttrain loss:8851.7278 train ap:0.742947  train auc:0.792332  val loss:6.953987 val ap:0.729793  val auc:0.726316\n",
            "\ttotal time:1.07s sample time:0.00s prep time:0.15s\n",
            "Epoch 72:\n",
            "\ttrain loss:8828.6974 train ap:0.736245  train auc:0.788245  val loss:6.271276 val ap:0.702392  val auc:0.737699\n",
            "\ttotal time:1.07s sample time:0.00s prep time:0.14s\n",
            "Epoch 73:\n",
            "\ttrain loss:8785.3297 train ap:0.737292  train auc:0.789854  val loss:6.702712 val ap:0.740520  val auc:0.763726\n",
            "\ttotal time:1.09s sample time:0.00s prep time:0.14s\n",
            "Epoch 74:\n",
            "\ttrain loss:8795.5818 train ap:0.743647  train auc:0.793401  val loss:5.635450 val ap:0.764563  val auc:0.793427\n",
            "\ttotal time:1.12s sample time:0.00s prep time:0.15s\n",
            "Epoch 75:\n",
            "\ttrain loss:8751.4505 train ap:0.739124  train auc:0.794166  val loss:6.417356 val ap:0.747951  val auc:0.756422\n",
            "\ttotal time:1.40s sample time:0.00s prep time:0.18s\n",
            "Epoch 76:\n",
            "\ttrain loss:8621.3266 train ap:0.746825  train auc:0.798476  val loss:6.927598 val ap:0.705813  val auc:0.749472\n",
            "\ttotal time:1.52s sample time:0.00s prep time:0.19s\n",
            "Epoch 77:\n",
            "\ttrain loss:8597.0144 train ap:0.752322  train auc:0.801963  val loss:6.553203 val ap:0.729168  val auc:0.759570\n",
            "\ttotal time:1.38s sample time:0.00s prep time:0.18s\n",
            "Epoch 78:\n",
            "\ttrain loss:8711.5601 train ap:0.742205  train auc:0.792591  val loss:6.213580 val ap:0.709133  val auc:0.737712\n",
            "\ttotal time:1.10s sample time:0.00s prep time:0.15s\n",
            "Epoch 79:\n",
            "\ttrain loss:8931.8103 train ap:0.735814  train auc:0.788752  val loss:6.055130 val ap:0.747308  val auc:0.772512\n",
            "\ttotal time:1.13s sample time:0.00s prep time:0.15s\n",
            "Epoch 80:\n",
            "\ttrain loss:8655.9000 train ap:0.741039  train auc:0.795283  val loss:7.213632 val ap:0.716979  val auc:0.740241\n",
            "\ttotal time:1.08s sample time:0.00s prep time:0.14s\n",
            "Epoch 81:\n",
            "\ttrain loss:8677.6250 train ap:0.751877  train auc:0.799973  val loss:5.662082 val ap:0.752814  val auc:0.772304\n",
            "\ttotal time:1.10s sample time:0.00s prep time:0.14s\n",
            "Epoch 82:\n",
            "\ttrain loss:8607.0489 train ap:0.745098  train auc:0.797712  val loss:6.279082 val ap:0.647196  val auc:0.682992\n",
            "\ttotal time:1.11s sample time:0.00s prep time:0.14s\n",
            "Epoch 83:\n",
            "\ttrain loss:8586.3343 train ap:0.745456  train auc:0.797888  val loss:5.942638 val ap:0.659851  val auc:0.709692\n",
            "\ttotal time:1.08s sample time:0.00s prep time:0.14s\n",
            "Epoch 84:\n",
            "\ttrain loss:8675.1617 train ap:0.744228  train auc:0.794808  val loss:5.924755 val ap:0.675537  val auc:0.736431\n",
            "\ttotal time:1.06s sample time:0.00s prep time:0.14s\n",
            "Epoch 85:\n",
            "\ttrain loss:8719.7696 train ap:0.738467  train auc:0.790491  val loss:6.686870 val ap:0.704153  val auc:0.735337\n",
            "\ttotal time:1.09s sample time:0.00s prep time:0.14s\n",
            "Epoch 86:\n",
            "\ttrain loss:8803.4494 train ap:0.738185  train auc:0.790075  val loss:6.391244 val ap:0.688660  val auc:0.730029\n",
            "\ttotal time:1.42s sample time:0.00s prep time:0.18s\n",
            "Epoch 87:\n",
            "\ttrain loss:8745.3985 train ap:0.735891  train auc:0.792399  val loss:7.307376 val ap:0.668147  val auc:0.699082\n",
            "\ttotal time:1.49s sample time:0.00s prep time:0.20s\n",
            "Epoch 88:\n",
            "\ttrain loss:8555.3855 train ap:0.749513  train auc:0.800614  val loss:6.430810 val ap:0.694960  val auc:0.730051\n",
            "\ttotal time:1.50s sample time:0.00s prep time:0.20s\n",
            "Epoch 89:\n",
            "\ttrain loss:8510.4437 train ap:0.749918  train auc:0.801567  val loss:6.372603 val ap:0.710368  val auc:0.722515\n",
            "\ttotal time:1.10s sample time:0.00s prep time:0.14s\n",
            "Loading model at epoch 18...\n",
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/train.py:290: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path_saver))\n",
            "{'test_R@1': 0.20472442, 'test_R@5': 0.55643046, 'test_R@10': 0.8425197, 'test_mrr': 0.2960566394281537}\n",
            "\taverage test precision:0.710368  test AUC:0.722515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positive lead/lag"
      ],
      "metadata": {
        "id": "bje9A1dQkn7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python gen_graph.py --data leadlag"
      ],
      "metadata": {
        "id": "u9rQ9NRvC8wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data positive --config ./config/TGN.yml --eval_neg_samples 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yqcZJvHkm94",
        "outputId": "8cb77f68-e6c0-4492-8559-9c9ea6f570d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/utils.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  node_feats = torch.load('DATA/{}/node_features.pt'.format(d))\n",
            "Epoch 0:\n",
            "\ttrain loss:3357.5886 train ap:0.536508  train auc:0.548157  val loss:2.715363 val ap:0.586947  val auc:0.596577\n",
            "\ttotal time:1.17s sample time:0.00s prep time:0.09s\n",
            "Epoch 1:\n",
            "\ttrain loss:3274.1942 train ap:0.594250  train auc:0.619207  val loss:2.709489 val ap:0.575666  val auc:0.613566\n",
            "\ttotal time:0.26s sample time:0.00s prep time:0.04s\n",
            "Epoch 2:\n",
            "\ttrain loss:3181.7146 train ap:0.620341  train auc:0.658264  val loss:2.795695 val ap:0.570969  val auc:0.601722\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 3:\n",
            "\ttrain loss:3150.3835 train ap:0.623382  train auc:0.650096  val loss:2.729210 val ap:0.639443  val auc:0.664460\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 4:\n",
            "\ttrain loss:3085.7793 train ap:0.649020  train auc:0.676384  val loss:2.861247 val ap:0.590633  val auc:0.607070\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 5:\n",
            "\ttrain loss:3054.2855 train ap:0.648005  train auc:0.684630  val loss:2.914694 val ap:0.587396  val auc:0.607679\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 6:\n",
            "\ttrain loss:3018.6893 train ap:0.654407  train auc:0.688868  val loss:2.775972 val ap:0.642703  val auc:0.658695\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 7:\n",
            "\ttrain loss:2965.3250 train ap:0.655403  train auc:0.701227  val loss:2.868768 val ap:0.613164  val auc:0.642873\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 8:\n",
            "\ttrain loss:2950.0542 train ap:0.653888  train auc:0.701213  val loss:2.776581 val ap:0.616144  val auc:0.649136\n",
            "\ttotal time:0.26s sample time:0.00s prep time:0.04s\n",
            "Epoch 9:\n",
            "\ttrain loss:2927.6476 train ap:0.656553  train auc:0.702580  val loss:2.706665 val ap:0.651170  val auc:0.654806\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 10:\n",
            "\ttrain loss:2925.7308 train ap:0.659313  train auc:0.706387  val loss:2.718038 val ap:0.608223  val auc:0.630553\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 11:\n",
            "\ttrain loss:2889.4908 train ap:0.670301  train auc:0.719636  val loss:2.889792 val ap:0.554811  val auc:0.579592\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.04s\n",
            "Epoch 12:\n",
            "\ttrain loss:2859.6408 train ap:0.675750  train auc:0.719386  val loss:2.734278 val ap:0.609504  val auc:0.650534\n",
            "\ttotal time:0.38s sample time:0.00s prep time:0.06s\n",
            "Epoch 13:\n",
            "\ttrain loss:2888.9918 train ap:0.676035  train auc:0.712671  val loss:2.753189 val ap:0.602941  val auc:0.649811\n",
            "\ttotal time:0.33s sample time:0.00s prep time:0.05s\n",
            "Epoch 14:\n",
            "\ttrain loss:2846.0339 train ap:0.678243  train auc:0.722229  val loss:2.680176 val ap:0.646268  val auc:0.687522\n",
            "\ttotal time:0.34s sample time:0.00s prep time:0.05s\n",
            "Epoch 15:\n",
            "\ttrain loss:2853.0454 train ap:0.673573  train auc:0.721107  val loss:2.802862 val ap:0.584873  val auc:0.602468\n",
            "\ttotal time:0.37s sample time:0.00s prep time:0.05s\n",
            "Epoch 16:\n",
            "\ttrain loss:2850.8850 train ap:0.682213  train auc:0.729093  val loss:2.523726 val ap:0.648985  val auc:0.687743\n",
            "\ttotal time:0.35s sample time:0.00s prep time:0.05s\n",
            "Epoch 17:\n",
            "\ttrain loss:2879.7385 train ap:0.677958  train auc:0.730968  val loss:2.556502 val ap:0.648855  val auc:0.683626\n",
            "\ttotal time:0.33s sample time:0.00s prep time:0.05s\n",
            "Epoch 18:\n",
            "\ttrain loss:2838.6173 train ap:0.683438  train auc:0.720842  val loss:2.754754 val ap:0.591868  val auc:0.639091\n",
            "\ttotal time:0.39s sample time:0.00s prep time:0.06s\n",
            "Epoch 19:\n",
            "\ttrain loss:2811.5715 train ap:0.672398  train auc:0.727380  val loss:2.725077 val ap:0.644979  val auc:0.656155\n",
            "\ttotal time:0.38s sample time:0.00s prep time:0.06s\n",
            "Epoch 20:\n",
            "\ttrain loss:2807.3275 train ap:0.694683  train auc:0.733647  val loss:2.663962 val ap:0.565109  val auc:0.626003\n",
            "\ttotal time:0.36s sample time:0.00s prep time:0.05s\n",
            "Epoch 21:\n",
            "\ttrain loss:2791.8887 train ap:0.689829  train auc:0.735715  val loss:2.740449 val ap:0.608225  val auc:0.634963\n",
            "\ttotal time:0.39s sample time:0.00s prep time:0.06s\n",
            "Epoch 22:\n",
            "\ttrain loss:2735.3327 train ap:0.692290  train auc:0.738097  val loss:2.636212 val ap:0.613647  val auc:0.672787\n",
            "\ttotal time:0.37s sample time:0.00s prep time:0.05s\n",
            "Epoch 23:\n",
            "\ttrain loss:2731.7912 train ap:0.705632  train auc:0.745976  val loss:2.735920 val ap:0.643005  val auc:0.657699\n",
            "\ttotal time:0.35s sample time:0.00s prep time:0.05s\n",
            "Epoch 24:\n",
            "\ttrain loss:2774.2214 train ap:0.702675  train auc:0.739813  val loss:2.603875 val ap:0.609060  val auc:0.656261\n",
            "\ttotal time:0.26s sample time:0.00s prep time:0.04s\n",
            "Epoch 25:\n",
            "\ttrain loss:2720.4348 train ap:0.694610  train auc:0.742979  val loss:2.643270 val ap:0.644465  val auc:0.678205\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 26:\n",
            "\ttrain loss:2720.7471 train ap:0.692533  train auc:0.737207  val loss:2.713872 val ap:0.633388  val auc:0.677200\n",
            "\ttotal time:0.47s sample time:0.00s prep time:0.06s\n",
            "Epoch 27:\n",
            "\ttrain loss:2692.0138 train ap:0.704576  train auc:0.750325  val loss:2.674303 val ap:0.604051  val auc:0.644381\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 28:\n",
            "\ttrain loss:2730.9138 train ap:0.698076  train auc:0.738997  val loss:2.731443 val ap:0.609697  val auc:0.631473\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 29:\n",
            "\ttrain loss:2657.8580 train ap:0.707557  train auc:0.754324  val loss:2.753557 val ap:0.631388  val auc:0.661084\n",
            "\ttotal time:0.33s sample time:0.00s prep time:0.05s\n",
            "Epoch 30:\n",
            "\ttrain loss:2646.7407 train ap:0.712480  train auc:0.754554  val loss:2.824715 val ap:0.613347  val auc:0.645270\n",
            "\ttotal time:0.34s sample time:0.00s prep time:0.05s\n",
            "Epoch 31:\n",
            "\ttrain loss:2686.1676 train ap:0.709556  train auc:0.747556  val loss:2.537210 val ap:0.653170  val auc:0.704674\n",
            "\ttotal time:0.32s sample time:0.00s prep time:0.05s\n",
            "Epoch 32:\n",
            "\ttrain loss:2642.5570 train ap:0.723242  train auc:0.760190  val loss:2.742313 val ap:0.603958  val auc:0.630586\n",
            "\ttotal time:0.37s sample time:0.00s prep time:0.05s\n",
            "Epoch 33:\n",
            "\ttrain loss:2633.9103 train ap:0.705012  train auc:0.752397  val loss:2.717226 val ap:0.616185  val auc:0.660256\n",
            "\ttotal time:0.35s sample time:0.00s prep time:0.05s\n",
            "Epoch 34:\n",
            "\ttrain loss:2606.3912 train ap:0.720554  train auc:0.764465  val loss:2.585840 val ap:0.654079  val auc:0.679983\n",
            "\ttotal time:0.33s sample time:0.00s prep time:0.05s\n",
            "Epoch 35:\n",
            "\ttrain loss:2628.2462 train ap:0.719179  train auc:0.761378  val loss:3.040302 val ap:0.594139  val auc:0.627529\n",
            "\ttotal time:0.36s sample time:0.00s prep time:0.05s\n",
            "Epoch 36:\n",
            "\ttrain loss:2604.1384 train ap:0.723241  train auc:0.765624  val loss:2.734965 val ap:0.633917  val auc:0.676091\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.07s\n",
            "Epoch 37:\n",
            "\ttrain loss:2674.3665 train ap:0.708286  train auc:0.748187  val loss:2.794123 val ap:0.603239  val auc:0.652319\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.08s\n",
            "Epoch 38:\n",
            "\ttrain loss:2606.0912 train ap:0.739830  train auc:0.779072  val loss:2.695818 val ap:0.639856  val auc:0.659132\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.04s\n",
            "Epoch 39:\n",
            "\ttrain loss:2616.2215 train ap:0.734114  train auc:0.772280  val loss:2.855147 val ap:0.574878  val auc:0.627240\n",
            "\ttotal time:0.26s sample time:0.00s prep time:0.04s\n",
            "Epoch 40:\n",
            "\ttrain loss:2615.6470 train ap:0.726872  train auc:0.765085  val loss:2.799185 val ap:0.613347  val auc:0.668252\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 41:\n",
            "\ttrain loss:2657.0643 train ap:0.723736  train auc:0.761454  val loss:2.919935 val ap:0.690231  val auc:0.726656\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 42:\n",
            "\ttrain loss:2625.2878 train ap:0.721249  train auc:0.760236  val loss:2.807046 val ap:0.593658  val auc:0.616211\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 43:\n",
            "\ttrain loss:2596.3629 train ap:0.733155  train auc:0.770874  val loss:2.770994 val ap:0.635120  val auc:0.655881\n",
            "\ttotal time:0.26s sample time:0.00s prep time:0.04s\n",
            "Epoch 44:\n",
            "\ttrain loss:2514.4412 train ap:0.752874  train auc:0.783759  val loss:3.026960 val ap:0.630281  val auc:0.654932\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 45:\n",
            "\ttrain loss:2514.3086 train ap:0.740565  train auc:0.779319  val loss:3.027940 val ap:0.605245  val auc:0.641938\n",
            "\ttotal time:0.24s sample time:0.00s prep time:0.04s\n",
            "Epoch 46:\n",
            "\ttrain loss:2481.5261 train ap:0.751426  train auc:0.787315  val loss:3.352561 val ap:0.609815  val auc:0.619385\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 47:\n",
            "\ttrain loss:2518.5151 train ap:0.745913  train auc:0.784145  val loss:3.255819 val ap:0.606769  val auc:0.619114\n",
            "\ttotal time:0.26s sample time:0.00s prep time:0.04s\n",
            "Epoch 48:\n",
            "\ttrain loss:2500.1787 train ap:0.745068  train auc:0.786256  val loss:3.193535 val ap:0.582180  val auc:0.606334\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 49:\n",
            "\ttrain loss:2464.7123 train ap:0.756364  train auc:0.789627  val loss:3.273730 val ap:0.568225  val auc:0.620152\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 50:\n",
            "\ttrain loss:2399.5762 train ap:0.772085  train auc:0.809476  val loss:3.128316 val ap:0.613532  val auc:0.649858\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.04s\n",
            "Epoch 51:\n",
            "\ttrain loss:2493.6912 train ap:0.745620  train auc:0.784433  val loss:3.117712 val ap:0.619259  val auc:0.636027\n",
            "\ttotal time:0.34s sample time:0.00s prep time:0.05s\n",
            "Epoch 52:\n",
            "\ttrain loss:2404.7613 train ap:0.762687  train auc:0.804986  val loss:3.279139 val ap:0.618495  val auc:0.657669\n",
            "\ttotal time:0.39s sample time:0.00s prep time:0.05s\n",
            "Epoch 53:\n",
            "\ttrain loss:2494.4954 train ap:0.747072  train auc:0.788479  val loss:3.243659 val ap:0.633616  val auc:0.672933\n",
            "\ttotal time:0.35s sample time:0.00s prep time:0.05s\n",
            "Epoch 54:\n",
            "\ttrain loss:2447.0163 train ap:0.749257  train auc:0.788113  val loss:3.360920 val ap:0.646767  val auc:0.701259\n",
            "\ttotal time:0.39s sample time:0.00s prep time:0.06s\n",
            "Epoch 55:\n",
            "\ttrain loss:2464.5985 train ap:0.759254  train auc:0.793947  val loss:3.082928 val ap:0.569668  val auc:0.620935\n",
            "\ttotal time:0.36s sample time:0.00s prep time:0.05s\n",
            "Epoch 56:\n",
            "\ttrain loss:2582.5343 train ap:0.729654  train auc:0.776245  val loss:2.734217 val ap:0.694428  val auc:0.718099\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.05s\n",
            "Epoch 57:\n",
            "\ttrain loss:2502.1264 train ap:0.751241  train auc:0.783368  val loss:3.341666 val ap:0.633859  val auc:0.654570\n",
            "\ttotal time:0.62s sample time:0.00s prep time:0.09s\n",
            "Epoch 58:\n",
            "\ttrain loss:2427.0985 train ap:0.758214  train auc:0.792109  val loss:3.371355 val ap:0.610444  val auc:0.635215\n",
            "\ttotal time:0.48s sample time:0.00s prep time:0.06s\n",
            "Epoch 59:\n",
            "\ttrain loss:2450.3185 train ap:0.750807  train auc:0.788548  val loss:3.131085 val ap:0.580750  val auc:0.637535\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.06s\n",
            "Epoch 60:\n",
            "\ttrain loss:2407.2196 train ap:0.762388  train auc:0.799795  val loss:3.447031 val ap:0.622963  val auc:0.662485\n",
            "\ttotal time:0.44s sample time:0.00s prep time:0.06s\n",
            "Epoch 61:\n",
            "\ttrain loss:2499.4532 train ap:0.740068  train auc:0.780166  val loss:3.955879 val ap:0.621935  val auc:0.663176\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.07s\n",
            "Epoch 62:\n",
            "\ttrain loss:2355.6247 train ap:0.777267  train auc:0.808597  val loss:3.371967 val ap:0.659626  val auc:0.664232\n",
            "\ttotal time:0.35s sample time:0.00s prep time:0.05s\n",
            "Epoch 63:\n",
            "\ttrain loss:2332.4191 train ap:0.769850  train auc:0.806952  val loss:3.502480 val ap:0.657695  val auc:0.668526\n",
            "\ttotal time:0.37s sample time:0.00s prep time:0.05s\n",
            "Epoch 64:\n",
            "\ttrain loss:2398.1945 train ap:0.765236  train auc:0.802281  val loss:3.237480 val ap:0.644660  val auc:0.656098\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.05s\n",
            "Epoch 65:\n",
            "\ttrain loss:2359.6446 train ap:0.770384  train auc:0.809566  val loss:3.171386 val ap:0.647932  val auc:0.685147\n",
            "\ttotal time:0.38s sample time:0.00s prep time:0.05s\n",
            "Epoch 66:\n",
            "\ttrain loss:2402.1039 train ap:0.766334  train auc:0.801851  val loss:3.186704 val ap:0.601919  val auc:0.626459\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.06s\n",
            "Epoch 67:\n",
            "\ttrain loss:2443.1300 train ap:0.751730  train auc:0.793647  val loss:3.688975 val ap:0.587853  val auc:0.607014\n",
            "\ttotal time:0.30s sample time:0.00s prep time:0.04s\n",
            "Epoch 68:\n",
            "\ttrain loss:2418.8707 train ap:0.757734  train auc:0.797561  val loss:3.447129 val ap:0.596510  val auc:0.676802\n",
            "\ttotal time:0.31s sample time:0.00s prep time:0.04s\n",
            "Epoch 69:\n",
            "\ttrain loss:2367.4860 train ap:0.759323  train auc:0.804495  val loss:2.685472 val ap:0.634707  val auc:0.707221\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.04s\n",
            "Epoch 70:\n",
            "\ttrain loss:2375.9166 train ap:0.761609  train auc:0.800650  val loss:3.472612 val ap:0.597985  val auc:0.655531\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.04s\n",
            "Epoch 71:\n",
            "\ttrain loss:2404.0211 train ap:0.771366  train auc:0.805818  val loss:3.551959 val ap:0.618314  val auc:0.611120\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 72:\n",
            "\ttrain loss:2290.4409 train ap:0.777120  train auc:0.813798  val loss:3.877416 val ap:0.660713  val auc:0.676536\n",
            "\ttotal time:0.31s sample time:0.00s prep time:0.05s\n",
            "Epoch 73:\n",
            "\ttrain loss:2288.7223 train ap:0.786338  train auc:0.822957  val loss:3.727262 val ap:0.565463  val auc:0.621419\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.04s\n",
            "Epoch 74:\n",
            "\ttrain loss:2248.1698 train ap:0.784491  train auc:0.816399  val loss:4.070151 val ap:0.638647  val auc:0.653020\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.04s\n",
            "Epoch 75:\n",
            "\ttrain loss:2333.5655 train ap:0.761618  train auc:0.803918  val loss:3.791133 val ap:0.604680  val auc:0.659428\n",
            "\ttotal time:0.25s sample time:0.00s prep time:0.04s\n",
            "Epoch 76:\n",
            "\ttrain loss:2269.9831 train ap:0.786122  train auc:0.822964  val loss:4.170451 val ap:0.634547  val auc:0.673114\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.04s\n",
            "Epoch 77:\n",
            "\ttrain loss:2272.9826 train ap:0.765065  train auc:0.809920  val loss:4.013785 val ap:0.576588  val auc:0.632748\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.04s\n",
            "Epoch 78:\n",
            "\ttrain loss:2303.2033 train ap:0.787455  train auc:0.825630  val loss:4.002952 val ap:0.613255  val auc:0.657698\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.04s\n",
            "Epoch 79:\n",
            "\ttrain loss:2338.1451 train ap:0.773391  train auc:0.808808  val loss:3.451812 val ap:0.631612  val auc:0.648597\n",
            "\ttotal time:0.26s sample time:0.00s prep time:0.04s\n",
            "Epoch 80:\n",
            "\ttrain loss:2352.4309 train ap:0.772918  train auc:0.809540  val loss:4.395944 val ap:0.608161  val auc:0.641488\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.04s\n",
            "Epoch 81:\n",
            "\ttrain loss:2351.6615 train ap:0.772976  train auc:0.808604  val loss:3.781137 val ap:0.649679  val auc:0.714335\n",
            "\ttotal time:0.26s sample time:0.00s prep time:0.04s\n",
            "Epoch 82:\n",
            "\ttrain loss:2312.8787 train ap:0.776067  train auc:0.811420  val loss:3.863092 val ap:0.584148  val auc:0.630749\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.04s\n",
            "Epoch 83:\n",
            "\ttrain loss:2360.7747 train ap:0.771270  train auc:0.804899  val loss:3.151238 val ap:0.657817  val auc:0.686054\n",
            "\ttotal time:0.30s sample time:0.00s prep time:0.04s\n",
            "Epoch 84:\n",
            "\ttrain loss:2318.5547 train ap:0.766436  train auc:0.810199  val loss:4.063146 val ap:0.636235  val auc:0.701014\n",
            "\ttotal time:0.30s sample time:0.00s prep time:0.04s\n",
            "Epoch 85:\n",
            "\ttrain loss:2308.5607 train ap:0.769334  train auc:0.807292  val loss:3.656563 val ap:0.580641  val auc:0.603746\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.04s\n",
            "Epoch 86:\n",
            "\ttrain loss:2283.1077 train ap:0.773745  train auc:0.812489  val loss:4.021054 val ap:0.574706  val auc:0.641843\n",
            "\ttotal time:0.26s sample time:0.00s prep time:0.04s\n",
            "Epoch 87:\n",
            "\ttrain loss:2224.2907 train ap:0.781400  train auc:0.820138  val loss:4.055116 val ap:0.622680  val auc:0.668170\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.04s\n",
            "Epoch 88:\n",
            "\ttrain loss:2218.3959 train ap:0.783177  train auc:0.823394  val loss:3.860658 val ap:0.599680  val auc:0.644968\n",
            "\ttotal time:0.26s sample time:0.00s prep time:0.04s\n",
            "Epoch 89:\n",
            "\ttrain loss:2201.1670 train ap:0.795191  train auc:0.828325  val loss:3.795803 val ap:0.634382  val auc:0.671246\n",
            "\ttotal time:0.26s sample time:0.00s prep time:0.04s\n",
            "Loading model at epoch 56...\n",
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/train.py:290: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path_saver))\n",
            "{'test_R@1': 0.13333334, 'test_R@5': 0.46666667, 'test_R@10': 0.7714286, 'test_mrr': 0.23116113523547685}\n",
            "\taverage test precision:0.634382  test AUC:0.671246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **JODIE**"
      ],
      "metadata": {
        "id": "fWeG-i2vIJ9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data leadlag --config ./config/JODIE.yml --eval_neg_samples 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4DVPP0-VGP8",
        "outputId": "e48708bc-94fa-4dcc-fbb1-a9d8ad142aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/utils.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  node_feats = torch.load('DATA/{}/node_features.pt'.format(d))\n",
            "Epoch 0:\n",
            "\ttrain loss:11818.2332 train ap:0.563217  train auc:0.587474  val loss:4.964284 val ap:0.675029  val auc:0.723428\n",
            "\ttotal time:0.85s sample time:0.00s prep time:0.21s\n",
            "Epoch 1:\n",
            "\ttrain loss:11186.2479 train ap:0.623492  train auc:0.661114  val loss:4.833115 val ap:0.633845  val auc:0.702109\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 2:\n",
            "\ttrain loss:11067.3650 train ap:0.631247  train auc:0.666817  val loss:4.650436 val ap:0.711111  val auc:0.753000\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.12s\n",
            "Epoch 3:\n",
            "\ttrain loss:10958.2404 train ap:0.638085  train auc:0.674596  val loss:4.572246 val ap:0.721806  val auc:0.762085\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 4:\n",
            "\ttrain loss:10815.6823 train ap:0.645618  train auc:0.685637  val loss:4.170234 val ap:0.785144  val auc:0.811350\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.12s\n",
            "Epoch 5:\n",
            "\ttrain loss:10853.1802 train ap:0.642184  train auc:0.679836  val loss:4.642971 val ap:0.703000  val auc:0.746565\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 6:\n",
            "\ttrain loss:10740.7859 train ap:0.649492  train auc:0.691370  val loss:4.420308 val ap:0.777851  val auc:0.799044\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 7:\n",
            "\ttrain loss:10690.4998 train ap:0.648228  train auc:0.690096  val loss:4.631679 val ap:0.716574  val auc:0.752213\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 8:\n",
            "\ttrain loss:10624.1242 train ap:0.654769  train auc:0.698941  val loss:4.429612 val ap:0.745170  val auc:0.786898\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 9:\n",
            "\ttrain loss:10627.2259 train ap:0.655465  train auc:0.699986  val loss:4.499314 val ap:0.714035  val auc:0.764410\n",
            "\ttotal time:0.63s sample time:0.00s prep time:0.13s\n",
            "Epoch 10:\n",
            "\ttrain loss:10563.7223 train ap:0.659361  train auc:0.704098  val loss:4.290244 val ap:0.787202  val auc:0.818187\n",
            "\ttotal time:0.72s sample time:0.00s prep time:0.15s\n",
            "Epoch 11:\n",
            "\ttrain loss:10589.3610 train ap:0.662883  train auc:0.703320  val loss:4.245716 val ap:0.801379  val auc:0.826327\n",
            "\ttotal time:0.75s sample time:0.00s prep time:0.17s\n",
            "Epoch 12:\n",
            "\ttrain loss:10576.2728 train ap:0.660205  train auc:0.702056  val loss:4.501604 val ap:0.728917  val auc:0.773577\n",
            "\ttotal time:0.73s sample time:0.00s prep time:0.15s\n",
            "Epoch 13:\n",
            "\ttrain loss:10484.7317 train ap:0.666098  train auc:0.708903  val loss:4.464519 val ap:0.754348  val auc:0.783132\n",
            "\ttotal time:0.80s sample time:0.00s prep time:0.17s\n",
            "Epoch 14:\n",
            "\ttrain loss:10405.8173 train ap:0.671008  train auc:0.717569  val loss:4.409746 val ap:0.731606  val auc:0.782309\n",
            "\ttotal time:0.70s sample time:0.00s prep time:0.15s\n",
            "Epoch 15:\n",
            "\ttrain loss:10428.1709 train ap:0.671310  train auc:0.714482  val loss:4.223831 val ap:0.774955  val auc:0.812691\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 16:\n",
            "\ttrain loss:10394.8961 train ap:0.675614  train auc:0.718560  val loss:4.684734 val ap:0.715098  val auc:0.752361\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 17:\n",
            "\ttrain loss:10385.7874 train ap:0.670409  train auc:0.716731  val loss:4.379196 val ap:0.750013  val auc:0.785624\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 18:\n",
            "\ttrain loss:10251.4440 train ap:0.679154  train auc:0.726893  val loss:4.577703 val ap:0.732242  val auc:0.754664\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.12s\n",
            "Epoch 19:\n",
            "\ttrain loss:10425.2499 train ap:0.664372  train auc:0.710064  val loss:4.621175 val ap:0.709554  val auc:0.753162\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 20:\n",
            "\ttrain loss:10267.6125 train ap:0.677469  train auc:0.722669  val loss:4.410483 val ap:0.761861  val auc:0.792495\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.12s\n",
            "Epoch 21:\n",
            "\ttrain loss:10278.2164 train ap:0.683387  train auc:0.724682  val loss:4.454571 val ap:0.732734  val auc:0.773645\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 22:\n",
            "\ttrain loss:10221.6707 train ap:0.679616  train auc:0.728903  val loss:4.642220 val ap:0.713891  val auc:0.756287\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 23:\n",
            "\ttrain loss:10185.4770 train ap:0.681705  train auc:0.729895  val loss:4.676872 val ap:0.691624  val auc:0.746404\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 24:\n",
            "\ttrain loss:10119.4478 train ap:0.685890  train auc:0.733217  val loss:4.600981 val ap:0.733688  val auc:0.768757\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 25:\n",
            "\ttrain loss:10199.0546 train ap:0.682702  train auc:0.730760  val loss:4.324913 val ap:0.721099  val auc:0.784777\n",
            "\ttotal time:0.63s sample time:0.00s prep time:0.13s\n",
            "Epoch 26:\n",
            "\ttrain loss:10234.7999 train ap:0.679778  train auc:0.726774  val loss:4.789636 val ap:0.688722  val auc:0.729786\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 27:\n",
            "\ttrain loss:10143.9146 train ap:0.685243  train auc:0.733481  val loss:4.329949 val ap:0.786702  val auc:0.814288\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 28:\n",
            "\ttrain loss:10112.0266 train ap:0.678951  train auc:0.730800  val loss:4.720079 val ap:0.699213  val auc:0.738854\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 29:\n",
            "\ttrain loss:9973.3241 train ap:0.694890  train auc:0.742089  val loss:4.629893 val ap:0.705643  val auc:0.748252\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 30:\n",
            "\ttrain loss:10002.5059 train ap:0.691938  train auc:0.739556  val loss:4.474054 val ap:0.755084  val auc:0.797464\n",
            "\ttotal time:0.59s sample time:0.00s prep time:0.13s\n",
            "Epoch 31:\n",
            "\ttrain loss:9965.5720 train ap:0.693219  train auc:0.741303  val loss:4.934858 val ap:0.684653  val auc:0.735121\n",
            "\ttotal time:0.69s sample time:0.00s prep time:0.15s\n",
            "Epoch 32:\n",
            "\ttrain loss:9889.9151 train ap:0.697901  train auc:0.745108  val loss:4.889864 val ap:0.672126  val auc:0.719257\n",
            "\ttotal time:0.74s sample time:0.00s prep time:0.15s\n",
            "Epoch 33:\n",
            "\ttrain loss:9991.4305 train ap:0.692873  train auc:0.742572  val loss:4.505348 val ap:0.747818  val auc:0.783137\n",
            "\ttotal time:0.76s sample time:0.00s prep time:0.16s\n",
            "Epoch 34:\n",
            "\ttrain loss:9872.4694 train ap:0.699287  train auc:0.746859  val loss:4.837430 val ap:0.724852  val auc:0.764117\n",
            "\ttotal time:0.76s sample time:0.00s prep time:0.16s\n",
            "Epoch 35:\n",
            "\ttrain loss:9928.0471 train ap:0.690604  train auc:0.741173  val loss:4.804518 val ap:0.672573  val auc:0.739845\n",
            "\ttotal time:0.84s sample time:0.00s prep time:0.18s\n",
            "Epoch 36:\n",
            "\ttrain loss:9965.8888 train ap:0.694348  train auc:0.743822  val loss:4.897618 val ap:0.736777  val auc:0.757243\n",
            "\ttotal time:0.70s sample time:0.00s prep time:0.15s\n",
            "Epoch 37:\n",
            "\ttrain loss:9892.3199 train ap:0.693889  train auc:0.746992  val loss:4.798227 val ap:0.698260  val auc:0.748263\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 38:\n",
            "\ttrain loss:9844.4962 train ap:0.701384  train auc:0.749074  val loss:4.866608 val ap:0.692071  val auc:0.751300\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 39:\n",
            "\ttrain loss:9777.9280 train ap:0.695445  train auc:0.748686  val loss:4.759923 val ap:0.699110  val auc:0.750597\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 40:\n",
            "\ttrain loss:9894.9747 train ap:0.693109  train auc:0.742343  val loss:4.746346 val ap:0.719400  val auc:0.756549\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 41:\n",
            "\ttrain loss:9801.9936 train ap:0.697195  train auc:0.750817  val loss:4.798561 val ap:0.694056  val auc:0.739602\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.12s\n",
            "Epoch 42:\n",
            "\ttrain loss:9792.3362 train ap:0.705194  train auc:0.755291  val loss:4.920339 val ap:0.718586  val auc:0.745180\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.12s\n",
            "Epoch 43:\n",
            "\ttrain loss:9766.4967 train ap:0.708481  train auc:0.755391  val loss:4.574432 val ap:0.754355  val auc:0.788697\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 44:\n",
            "\ttrain loss:9732.7627 train ap:0.700719  train auc:0.754268  val loss:5.167335 val ap:0.709497  val auc:0.727855\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 45:\n",
            "\ttrain loss:9641.7957 train ap:0.707019  train auc:0.759083  val loss:5.248660 val ap:0.633710  val auc:0.686167\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 46:\n",
            "\ttrain loss:9593.5423 train ap:0.713466  train auc:0.761214  val loss:4.705543 val ap:0.766583  val auc:0.777202\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 47:\n",
            "\ttrain loss:9647.3892 train ap:0.708999  train auc:0.757768  val loss:4.863912 val ap:0.713925  val auc:0.748284\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 48:\n",
            "\ttrain loss:9608.2430 train ap:0.706046  train auc:0.757782  val loss:4.802961 val ap:0.732669  val auc:0.762344\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 49:\n",
            "\ttrain loss:9564.5380 train ap:0.710148  train auc:0.761524  val loss:5.015791 val ap:0.714029  val auc:0.728809\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 50:\n",
            "\ttrain loss:9601.6057 train ap:0.710033  train auc:0.761255  val loss:4.793047 val ap:0.697423  val auc:0.764213\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 51:\n",
            "\ttrain loss:9662.1912 train ap:0.703023  train auc:0.757159  val loss:4.881037 val ap:0.694533  val auc:0.735862\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.12s\n",
            "Epoch 52:\n",
            "\ttrain loss:9505.6984 train ap:0.711934  train auc:0.767004  val loss:4.780310 val ap:0.702806  val auc:0.750843\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 53:\n",
            "\ttrain loss:9577.5131 train ap:0.707924  train auc:0.761915  val loss:4.827689 val ap:0.745999  val auc:0.779444\n",
            "\ttotal time:0.72s sample time:0.00s prep time:0.15s\n",
            "Epoch 54:\n",
            "\ttrain loss:9696.3662 train ap:0.705144  train auc:0.757648  val loss:5.152003 val ap:0.675885  val auc:0.721581\n",
            "\ttotal time:0.77s sample time:0.00s prep time:0.16s\n",
            "Epoch 55:\n",
            "\ttrain loss:9601.2023 train ap:0.713195  train auc:0.764233  val loss:5.268291 val ap:0.685411  val auc:0.722891\n",
            "\ttotal time:0.73s sample time:0.00s prep time:0.15s\n",
            "Epoch 56:\n",
            "\ttrain loss:9638.3328 train ap:0.710758  train auc:0.761953  val loss:5.145483 val ap:0.667321  val auc:0.708900\n",
            "\ttotal time:0.79s sample time:0.00s prep time:0.17s\n",
            "Epoch 57:\n",
            "\ttrain loss:9647.6031 train ap:0.705836  train auc:0.761450  val loss:4.596118 val ap:0.743920  val auc:0.800792\n",
            "\ttotal time:0.78s sample time:0.00s prep time:0.17s\n",
            "Epoch 58:\n",
            "\ttrain loss:9497.2520 train ap:0.711698  train auc:0.766088  val loss:5.011065 val ap:0.684028  val auc:0.728599\n",
            "\ttotal time:0.63s sample time:0.00s prep time:0.13s\n",
            "Epoch 59:\n",
            "\ttrain loss:9608.8965 train ap:0.710211  train auc:0.760375  val loss:4.773119 val ap:0.716299  val auc:0.768740\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 60:\n",
            "\ttrain loss:9539.0501 train ap:0.712300  train auc:0.763649  val loss:5.188407 val ap:0.675001  val auc:0.719672\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 61:\n",
            "\ttrain loss:9355.3883 train ap:0.722512  train auc:0.772735  val loss:4.728658 val ap:0.737475  val auc:0.778515\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 62:\n",
            "\ttrain loss:9476.3820 train ap:0.708318  train auc:0.764771  val loss:4.939686 val ap:0.707315  val auc:0.741492\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 63:\n",
            "\ttrain loss:9507.5355 train ap:0.714181  train auc:0.764991  val loss:4.695149 val ap:0.742995  val auc:0.773670\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 64:\n",
            "\ttrain loss:9430.5527 train ap:0.714205  train auc:0.766620  val loss:5.341127 val ap:0.690161  val auc:0.716615\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 65:\n",
            "\ttrain loss:9354.4842 train ap:0.715467  train auc:0.770650  val loss:4.851610 val ap:0.748860  val auc:0.769122\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.12s\n",
            "Epoch 66:\n",
            "\ttrain loss:9353.5757 train ap:0.715315  train auc:0.769887  val loss:4.755125 val ap:0.737251  val auc:0.769330\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 67:\n",
            "\ttrain loss:9340.1416 train ap:0.714220  train auc:0.770092  val loss:5.321336 val ap:0.672400  val auc:0.712649\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 68:\n",
            "\ttrain loss:9290.3353 train ap:0.720076  train auc:0.775633  val loss:5.448592 val ap:0.718275  val auc:0.726670\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 69:\n",
            "\ttrain loss:9255.8356 train ap:0.726928  train auc:0.778476  val loss:4.937394 val ap:0.712810  val auc:0.748465\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 70:\n",
            "\ttrain loss:9373.1001 train ap:0.716138  train auc:0.770300  val loss:4.998216 val ap:0.714967  val auc:0.750102\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 71:\n",
            "\ttrain loss:9329.9009 train ap:0.716510  train auc:0.774393  val loss:4.714686 val ap:0.754180  val auc:0.785289\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 72:\n",
            "\ttrain loss:9265.8564 train ap:0.722527  train auc:0.774756  val loss:4.591971 val ap:0.734223  val auc:0.772791\n",
            "\ttotal time:0.62s sample time:0.00s prep time:0.13s\n",
            "Epoch 73:\n",
            "\ttrain loss:9361.5464 train ap:0.712653  train auc:0.770200  val loss:4.876072 val ap:0.725520  val auc:0.760224\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.12s\n",
            "Epoch 74:\n",
            "\ttrain loss:9138.4621 train ap:0.727859  train auc:0.784208  val loss:4.994507 val ap:0.727210  val auc:0.755499\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 75:\n",
            "\ttrain loss:9256.1901 train ap:0.723413  train auc:0.778428  val loss:5.054543 val ap:0.679502  val auc:0.727106\n",
            "\ttotal time:0.79s sample time:0.00s prep time:0.17s\n",
            "Epoch 76:\n",
            "\ttrain loss:9336.0994 train ap:0.719252  train auc:0.774146  val loss:4.978061 val ap:0.700202  val auc:0.736354\n",
            "\ttotal time:0.76s sample time:0.00s prep time:0.16s\n",
            "Epoch 77:\n",
            "\ttrain loss:9233.7672 train ap:0.721555  train auc:0.775359  val loss:4.628810 val ap:0.754292  val auc:0.781449\n",
            "\ttotal time:0.71s sample time:0.00s prep time:0.15s\n",
            "Epoch 78:\n",
            "\ttrain loss:9360.6613 train ap:0.719167  train auc:0.773626  val loss:4.865778 val ap:0.695640  val auc:0.744708\n",
            "\ttotal time:0.82s sample time:0.00s prep time:0.17s\n",
            "Epoch 79:\n",
            "\ttrain loss:9210.6263 train ap:0.724911  train auc:0.780166  val loss:4.893832 val ap:0.709754  val auc:0.746586\n",
            "\ttotal time:0.82s sample time:0.00s prep time:0.17s\n",
            "Epoch 80:\n",
            "\ttrain loss:9144.4241 train ap:0.721366  train auc:0.779542  val loss:4.829835 val ap:0.745623  val auc:0.773374\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 81:\n",
            "\ttrain loss:9154.8456 train ap:0.722446  train auc:0.779948  val loss:5.054915 val ap:0.644232  val auc:0.707674\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.12s\n",
            "Epoch 82:\n",
            "\ttrain loss:9187.8612 train ap:0.723903  train auc:0.779487  val loss:5.202787 val ap:0.673608  val auc:0.711601\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 83:\n",
            "\ttrain loss:9086.0149 train ap:0.728150  train auc:0.785752  val loss:5.434628 val ap:0.689994  val auc:0.714423\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 84:\n",
            "\ttrain loss:9084.4140 train ap:0.731986  train auc:0.784522  val loss:5.234384 val ap:0.662189  val auc:0.717696\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 85:\n",
            "\ttrain loss:9060.7441 train ap:0.732324  train auc:0.788591  val loss:4.863866 val ap:0.725773  val auc:0.752683\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 86:\n",
            "\ttrain loss:9163.2384 train ap:0.722544  train auc:0.780068  val loss:4.782813 val ap:0.719530  val auc:0.752165\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Epoch 87:\n",
            "\ttrain loss:9117.1668 train ap:0.730138  train auc:0.784260  val loss:5.157146 val ap:0.686432  val auc:0.713157\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.12s\n",
            "Epoch 88:\n",
            "\ttrain loss:9201.9846 train ap:0.722619  train auc:0.777080  val loss:5.520042 val ap:0.692758  val auc:0.718123\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.12s\n",
            "Epoch 89:\n",
            "\ttrain loss:9017.1367 train ap:0.738211  train auc:0.790368  val loss:5.112883 val ap:0.725672  val auc:0.755725\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.12s\n",
            "Loading model at epoch 11...\n",
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/train.py:290: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path_saver))\n",
            "{'test_R@1': 0.20997375, 'test_R@5': 0.62204725, 'test_R@10': 0.86614174, 'test_mrr': 0.31094620397671835}\n",
            "\taverage test precision:0.725672  test AUC:0.755725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positive lead/lag"
      ],
      "metadata": {
        "id": "UoOAh6I9kxGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python gen_graph.py --data leadlag"
      ],
      "metadata": {
        "id": "u0wNv4SxC-Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data leadlag --config ./config/JODIE.yml --eval_neg_samples 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4W7WGJ1kyWw",
        "outputId": "959b641e-ba0b-4dbe-f2f6-bd6d8c1649b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/utils.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  node_feats = torch.load('DATA/{}/node_features.pt'.format(d))\n",
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/utils.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  edge_feats = torch.load('DATA/{}/edge_features.pt'.format(d))\n",
            "Epoch 0:\n",
            "\ttrain loss:12038.7775 train ap:0.542100  train auc:0.565415  val loss:5.264453 val ap:0.626873  val auc:0.667228\n",
            "\ttotal time:0.80s sample time:0.00s prep time:0.20s\n",
            "Epoch 1:\n",
            "\ttrain loss:11409.3660 train ap:0.608923  train auc:0.642354  val loss:5.036734 val ap:0.642498  val auc:0.691707\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 2:\n",
            "\ttrain loss:11172.4902 train ap:0.632738  train auc:0.666642  val loss:4.909589 val ap:0.737715  val auc:0.754267\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 3:\n",
            "\ttrain loss:11012.9646 train ap:0.638058  train auc:0.675598  val loss:4.867030 val ap:0.684839  val auc:0.728855\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 4:\n",
            "\ttrain loss:10876.0308 train ap:0.643137  train auc:0.682256  val loss:4.749349 val ap:0.724968  val auc:0.738792\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 5:\n",
            "\ttrain loss:10863.6666 train ap:0.642776  train auc:0.682250  val loss:4.658057 val ap:0.744077  val auc:0.767542\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 6:\n",
            "\ttrain loss:10757.0067 train ap:0.649437  train auc:0.693076  val loss:4.749524 val ap:0.703140  val auc:0.731741\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 7:\n",
            "\ttrain loss:10678.2017 train ap:0.656261  train auc:0.698016  val loss:4.495284 val ap:0.748807  val auc:0.778483\n",
            "\ttotal time:0.71s sample time:0.00s prep time:0.16s\n",
            "Epoch 8:\n",
            "\ttrain loss:10494.9063 train ap:0.670759  train auc:0.712784  val loss:4.552850 val ap:0.751913  val auc:0.782414\n",
            "\ttotal time:0.73s sample time:0.00s prep time:0.16s\n",
            "Epoch 9:\n",
            "\ttrain loss:10490.7337 train ap:0.670141  train auc:0.713022  val loss:4.700109 val ap:0.696947  val auc:0.747547\n",
            "\ttotal time:0.69s sample time:0.00s prep time:0.16s\n",
            "Epoch 10:\n",
            "\ttrain loss:10389.4987 train ap:0.682302  train auc:0.721008  val loss:4.628825 val ap:0.727272  val auc:0.760504\n",
            "\ttotal time:0.77s sample time:0.00s prep time:0.18s\n",
            "Epoch 11:\n",
            "\ttrain loss:10466.6962 train ap:0.674172  train auc:0.714765  val loss:4.602680 val ap:0.718303  val auc:0.760467\n",
            "\ttotal time:0.75s sample time:0.00s prep time:0.17s\n",
            "Epoch 12:\n",
            "\ttrain loss:10311.6080 train ap:0.681605  train auc:0.722483  val loss:4.600248 val ap:0.755783  val auc:0.767605\n",
            "\ttotal time:0.59s sample time:0.00s prep time:0.13s\n",
            "Epoch 13:\n",
            "\ttrain loss:10413.3474 train ap:0.671474  train auc:0.715973  val loss:4.467103 val ap:0.744633  val auc:0.785426\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.13s\n",
            "Epoch 14:\n",
            "\ttrain loss:10268.3695 train ap:0.683728  train auc:0.728192  val loss:4.786963 val ap:0.708669  val auc:0.734431\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 15:\n",
            "\ttrain loss:10305.4255 train ap:0.677088  train auc:0.724731  val loss:4.585345 val ap:0.748275  val auc:0.772177\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 16:\n",
            "\ttrain loss:10267.1843 train ap:0.687086  train auc:0.731981  val loss:4.650685 val ap:0.750070  val auc:0.764440\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 17:\n",
            "\ttrain loss:10227.8123 train ap:0.683050  train auc:0.731706  val loss:4.669419 val ap:0.698683  val auc:0.730530\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 18:\n",
            "\ttrain loss:10100.3589 train ap:0.693428  train auc:0.741077  val loss:4.426745 val ap:0.776243  val auc:0.789753\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 19:\n",
            "\ttrain loss:10144.9472 train ap:0.690109  train auc:0.737557  val loss:4.752565 val ap:0.693441  val auc:0.732130\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 20:\n",
            "\ttrain loss:10015.7037 train ap:0.694572  train auc:0.744628  val loss:4.562891 val ap:0.748814  val auc:0.777261\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 21:\n",
            "\ttrain loss:10076.6675 train ap:0.690682  train auc:0.740717  val loss:4.358759 val ap:0.808610  val auc:0.809568\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 22:\n",
            "\ttrain loss:10042.1941 train ap:0.693750  train auc:0.744520  val loss:4.639080 val ap:0.742386  val auc:0.757087\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 23:\n",
            "\ttrain loss:10062.5560 train ap:0.694012  train auc:0.741543  val loss:4.505586 val ap:0.761255  val auc:0.783058\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 24:\n",
            "\ttrain loss:10100.8418 train ap:0.689429  train auc:0.740613  val loss:4.656047 val ap:0.720719  val auc:0.754329\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 25:\n",
            "\ttrain loss:10103.0769 train ap:0.694366  train auc:0.740786  val loss:4.619253 val ap:0.734943  val auc:0.756590\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 26:\n",
            "\ttrain loss:10012.8407 train ap:0.690823  train auc:0.741776  val loss:4.553050 val ap:0.735456  val auc:0.768469\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 27:\n",
            "\ttrain loss:9934.9498 train ap:0.697816  train auc:0.747192  val loss:4.557555 val ap:0.755128  val auc:0.771856\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 28:\n",
            "\ttrain loss:9970.1896 train ap:0.701782  train auc:0.747554  val loss:4.434132 val ap:0.802252  val auc:0.795820\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 29:\n",
            "\ttrain loss:9875.2814 train ap:0.705180  train auc:0.750995  val loss:4.389386 val ap:0.743568  val auc:0.795937\n",
            "\ttotal time:0.67s sample time:0.00s prep time:0.15s\n",
            "Epoch 30:\n",
            "\ttrain loss:9902.6940 train ap:0.695890  train auc:0.747552  val loss:4.461350 val ap:0.776412  val auc:0.784836\n",
            "\ttotal time:0.71s sample time:0.00s prep time:0.16s\n",
            "Epoch 31:\n",
            "\ttrain loss:9866.4096 train ap:0.697577  train auc:0.749471  val loss:4.586730 val ap:0.769298  val auc:0.771453\n",
            "\ttotal time:0.72s sample time:0.00s prep time:0.15s\n",
            "Epoch 32:\n",
            "\ttrain loss:9837.8107 train ap:0.698142  train auc:0.753399  val loss:4.477111 val ap:0.767480  val auc:0.783551\n",
            "\ttotal time:0.74s sample time:0.00s prep time:0.16s\n",
            "Epoch 33:\n",
            "\ttrain loss:9822.0134 train ap:0.704282  train auc:0.754441  val loss:4.177554 val ap:0.813676  val auc:0.823974\n",
            "\ttotal time:0.76s sample time:0.00s prep time:0.17s\n",
            "Epoch 34:\n",
            "\ttrain loss:9899.7473 train ap:0.698169  train auc:0.749425  val loss:4.368422 val ap:0.775728  val auc:0.795148\n",
            "\ttotal time:0.61s sample time:0.00s prep time:0.14s\n",
            "Epoch 35:\n",
            "\ttrain loss:9695.9476 train ap:0.711863  train auc:0.761145  val loss:4.423009 val ap:0.783663  val auc:0.792672\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 36:\n",
            "\ttrain loss:9753.6073 train ap:0.705882  train auc:0.756429  val loss:4.381064 val ap:0.733910  val auc:0.782111\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 37:\n",
            "\ttrain loss:9707.0122 train ap:0.710384  train auc:0.760535  val loss:4.613854 val ap:0.754443  val auc:0.766520\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 38:\n",
            "\ttrain loss:9790.6701 train ap:0.699803  train auc:0.751129  val loss:4.541115 val ap:0.738378  val auc:0.776447\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 39:\n",
            "\ttrain loss:9652.5808 train ap:0.714707  train auc:0.763972  val loss:4.798765 val ap:0.677254  val auc:0.722732\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 40:\n",
            "\ttrain loss:9707.0039 train ap:0.708781  train auc:0.757483  val loss:4.318034 val ap:0.779257  val auc:0.798422\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 41:\n",
            "\ttrain loss:9627.3243 train ap:0.711173  train auc:0.762901  val loss:4.579486 val ap:0.722082  val auc:0.761598\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.13s\n",
            "Epoch 42:\n",
            "\ttrain loss:9564.2718 train ap:0.720114  train auc:0.767124  val loss:4.469384 val ap:0.767372  val auc:0.784093\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.13s\n",
            "Epoch 43:\n",
            "\ttrain loss:9576.1400 train ap:0.712173  train auc:0.762485  val loss:4.519495 val ap:0.759860  val auc:0.774032\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 44:\n",
            "\ttrain loss:9710.6050 train ap:0.705518  train auc:0.754731  val loss:4.529924 val ap:0.762239  val auc:0.778718\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 45:\n",
            "\ttrain loss:9626.3918 train ap:0.703315  train auc:0.758817  val loss:4.494431 val ap:0.751614  val auc:0.777729\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 46:\n",
            "\ttrain loss:9653.9839 train ap:0.703056  train auc:0.757201  val loss:4.547921 val ap:0.751887  val auc:0.779210\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 47:\n",
            "\ttrain loss:9648.8118 train ap:0.708676  train auc:0.760473  val loss:4.714928 val ap:0.739264  val auc:0.754030\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 48:\n",
            "\ttrain loss:9556.1331 train ap:0.719461  train auc:0.766071  val loss:4.547232 val ap:0.734990  val auc:0.765360\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 49:\n",
            "\ttrain loss:9572.9926 train ap:0.709574  train auc:0.762513  val loss:4.525657 val ap:0.726590  val auc:0.769950\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.12s\n",
            "Epoch 50:\n",
            "\ttrain loss:9541.3530 train ap:0.717100  train auc:0.767128  val loss:4.353761 val ap:0.781788  val auc:0.794489\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 51:\n",
            "\ttrain loss:9534.9505 train ap:0.708922  train auc:0.763863  val loss:4.361208 val ap:0.756406  val auc:0.792709\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.13s\n",
            "Epoch 52:\n",
            "\ttrain loss:9564.5595 train ap:0.712051  train auc:0.763774  val loss:4.388711 val ap:0.755440  val auc:0.792350\n",
            "\ttotal time:0.70s sample time:0.00s prep time:0.16s\n",
            "Epoch 53:\n",
            "\ttrain loss:9522.9322 train ap:0.715057  train auc:0.766019  val loss:4.554699 val ap:0.743535  val auc:0.769660\n",
            "\ttotal time:0.73s sample time:0.00s prep time:0.17s\n",
            "Epoch 54:\n",
            "\ttrain loss:9401.1410 train ap:0.722110  train auc:0.772760  val loss:4.405424 val ap:0.738374  val auc:0.777278\n",
            "\ttotal time:0.68s sample time:0.00s prep time:0.15s\n",
            "Epoch 55:\n",
            "\ttrain loss:9383.9410 train ap:0.721470  train auc:0.772407  val loss:4.408389 val ap:0.771788  val auc:0.785924\n",
            "\ttotal time:0.77s sample time:0.00s prep time:0.17s\n",
            "Epoch 56:\n",
            "\ttrain loss:9425.1711 train ap:0.724480  train auc:0.774374  val loss:4.655659 val ap:0.733573  val auc:0.761664\n",
            "\ttotal time:0.72s sample time:0.00s prep time:0.16s\n",
            "Epoch 57:\n",
            "\ttrain loss:9368.5912 train ap:0.718873  train auc:0.772777  val loss:4.481692 val ap:0.725391  val auc:0.776945\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 58:\n",
            "\ttrain loss:9435.7046 train ap:0.715643  train auc:0.768482  val loss:4.473051 val ap:0.762755  val auc:0.786285\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 59:\n",
            "\ttrain loss:9448.8061 train ap:0.717816  train auc:0.768919  val loss:4.565501 val ap:0.742063  val auc:0.766710\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 60:\n",
            "\ttrain loss:9547.9162 train ap:0.713930  train auc:0.765246  val loss:4.613576 val ap:0.747087  val auc:0.765552\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 61:\n",
            "\ttrain loss:9387.1141 train ap:0.718436  train auc:0.771971  val loss:4.618405 val ap:0.750062  val auc:0.766172\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 62:\n",
            "\ttrain loss:9458.3560 train ap:0.710185  train auc:0.767800  val loss:5.026968 val ap:0.668818  val auc:0.706482\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.13s\n",
            "Epoch 63:\n",
            "\ttrain loss:9318.5597 train ap:0.723456  train auc:0.776532  val loss:4.893883 val ap:0.688031  val auc:0.727547\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 64:\n",
            "\ttrain loss:9298.6505 train ap:0.725468  train auc:0.774885  val loss:4.887372 val ap:0.712413  val auc:0.735378\n",
            "\ttotal time:0.56s sample time:0.00s prep time:0.13s\n",
            "Epoch 65:\n",
            "\ttrain loss:9414.2747 train ap:0.720270  train auc:0.772160  val loss:4.514289 val ap:0.752172  val auc:0.779976\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 66:\n",
            "\ttrain loss:9313.2495 train ap:0.722922  train auc:0.776078  val loss:4.591525 val ap:0.713498  val auc:0.751037\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.13s\n",
            "Epoch 67:\n",
            "\ttrain loss:9253.8093 train ap:0.724258  train auc:0.777512  val loss:4.698109 val ap:0.706377  val auc:0.744704\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 68:\n",
            "\ttrain loss:9149.5570 train ap:0.731506  train auc:0.783191  val loss:4.758703 val ap:0.711431  val auc:0.741441\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 69:\n",
            "\ttrain loss:9174.3444 train ap:0.729409  train auc:0.782049  val loss:4.782943 val ap:0.716949  val auc:0.736489\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 70:\n",
            "\ttrain loss:9159.1835 train ap:0.727426  train auc:0.783231  val loss:4.521422 val ap:0.731535  val auc:0.776162\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 71:\n",
            "\ttrain loss:9222.6211 train ap:0.724164  train auc:0.778032  val loss:4.829016 val ap:0.687962  val auc:0.729070\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 72:\n",
            "\ttrain loss:9191.5312 train ap:0.733528  train auc:0.783892  val loss:4.691223 val ap:0.742308  val auc:0.755443\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 73:\n",
            "\ttrain loss:9149.8933 train ap:0.733436  train auc:0.783669  val loss:4.638133 val ap:0.741993  val auc:0.765781\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.12s\n",
            "Epoch 74:\n",
            "\ttrain loss:9224.9953 train ap:0.723443  train auc:0.776788  val loss:4.888453 val ap:0.676240  val auc:0.719411\n",
            "\ttotal time:0.66s sample time:0.00s prep time:0.15s\n",
            "Epoch 75:\n",
            "\ttrain loss:9255.7015 train ap:0.724105  train auc:0.777744  val loss:4.905726 val ap:0.683594  val auc:0.720171\n",
            "\ttotal time:0.71s sample time:0.00s prep time:0.16s\n",
            "Epoch 76:\n",
            "\ttrain loss:9141.3950 train ap:0.729458  train auc:0.782941  val loss:4.560998 val ap:0.745754  val auc:0.774204\n",
            "\ttotal time:0.70s sample time:0.00s prep time:0.15s\n",
            "Epoch 77:\n",
            "\ttrain loss:9142.0330 train ap:0.727422  train auc:0.780528  val loss:4.837079 val ap:0.709046  val auc:0.726570\n",
            "\ttotal time:0.71s sample time:0.00s prep time:0.16s\n",
            "Epoch 78:\n",
            "\ttrain loss:9026.7124 train ap:0.731108  train auc:0.785998  val loss:4.428362 val ap:0.783333  val auc:0.798337\n",
            "\ttotal time:0.75s sample time:0.00s prep time:0.17s\n",
            "Epoch 79:\n",
            "\ttrain loss:9165.6456 train ap:0.722828  train auc:0.779499  val loss:5.010496 val ap:0.700240  val auc:0.722443\n",
            "\ttotal time:0.62s sample time:0.00s prep time:0.14s\n",
            "Epoch 80:\n",
            "\ttrain loss:9093.9650 train ap:0.735775  train auc:0.786443  val loss:5.002787 val ap:0.707532  val auc:0.729793\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 81:\n",
            "\ttrain loss:9112.0848 train ap:0.728000  train auc:0.783837  val loss:4.921940 val ap:0.709062  val auc:0.727917\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.13s\n",
            "Epoch 82:\n",
            "\ttrain loss:9172.1359 train ap:0.722810  train auc:0.778954  val loss:5.076352 val ap:0.658173  val auc:0.687017\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 83:\n",
            "\ttrain loss:9082.2765 train ap:0.731772  train auc:0.785735  val loss:4.802016 val ap:0.726992  val auc:0.750648\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 84:\n",
            "\ttrain loss:9058.9031 train ap:0.734223  train auc:0.787363  val loss:4.826828 val ap:0.721537  val auc:0.739389\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 85:\n",
            "\ttrain loss:8992.2902 train ap:0.737751  train auc:0.790824  val loss:4.502384 val ap:0.759241  val auc:0.785206\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 86:\n",
            "\ttrain loss:9020.3473 train ap:0.734046  train auc:0.787077  val loss:4.709617 val ap:0.722117  val auc:0.756314\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.13s\n",
            "Epoch 87:\n",
            "\ttrain loss:9506.0020 train ap:0.725384  train auc:0.777369  val loss:4.902707 val ap:0.701996  val auc:0.735713\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Epoch 88:\n",
            "\ttrain loss:9124.1903 train ap:0.730324  train auc:0.783246  val loss:4.760887 val ap:0.725246  val auc:0.745711\n",
            "\ttotal time:0.55s sample time:0.00s prep time:0.12s\n",
            "Epoch 89:\n",
            "\ttrain loss:9030.8965 train ap:0.738174  train auc:0.789131  val loss:4.788785 val ap:0.752669  val auc:0.765598\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.12s\n",
            "Loading model at epoch 33...\n",
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/train.py:290: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path_saver))\n",
            "{'test_R@1': 0.12860893, 'test_R@5': 0.46981627, 'test_R@10': 0.7401575, 'test_mrr': 0.23455838803633558}\n",
            "\taverage test precision:0.752669  test AUC:0.765598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DySAT**"
      ],
      "metadata": {
        "id": "U1IHCfLIKoke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data leadlag --config ./config/DySAT.yml --eval_neg_samples 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODKuCLTmVRb6",
        "outputId": "0d7ecc21-1e67-4c8a-988e-0b565806689c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/utils.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  node_feats = torch.load('DATA/{}/node_features.pt'.format(d))\n",
            "Epoch 0:\n",
            "\ttrain loss:11801.7616 train ap:0.577091  train auc:0.595941  val loss:5.054037 val ap:0.671051  val auc:0.707855\n",
            "\ttotal time:2.63s sample time:0.01s prep time:0.40s\n",
            "Epoch 1:\n",
            "\ttrain loss:11388.2855 train ap:0.619157  train auc:0.649122  val loss:4.891953 val ap:0.691347  val auc:0.705766\n",
            "\ttotal time:1.83s sample time:0.01s prep time:0.35s\n",
            "Epoch 2:\n",
            "\ttrain loss:11413.5040 train ap:0.617509  train auc:0.651301  val loss:4.999502 val ap:0.676202  val auc:0.696110\n",
            "\ttotal time:1.81s sample time:0.00s prep time:0.35s\n",
            "Epoch 3:\n",
            "\ttrain loss:11330.7892 train ap:0.622681  train auc:0.658410  val loss:4.679294 val ap:0.739771  val auc:0.757546\n",
            "\ttotal time:1.81s sample time:0.01s prep time:0.35s\n",
            "Epoch 4:\n",
            "\ttrain loss:11281.0748 train ap:0.626306  train auc:0.660275  val loss:4.814072 val ap:0.755303  val auc:0.750629\n",
            "\ttotal time:1.88s sample time:0.01s prep time:0.36s\n",
            "Epoch 5:\n",
            "\ttrain loss:11234.0666 train ap:0.628697  train auc:0.664320  val loss:4.933718 val ap:0.744557  val auc:0.742951\n",
            "\ttotal time:2.69s sample time:0.00s prep time:0.49s\n",
            "Epoch 6:\n",
            "\ttrain loss:11194.1460 train ap:0.631248  train auc:0.663929  val loss:4.756518 val ap:0.714936  val auc:0.740908\n",
            "\ttotal time:2.23s sample time:0.01s prep time:0.40s\n",
            "Epoch 7:\n",
            "\ttrain loss:11167.1152 train ap:0.632986  train auc:0.664880  val loss:4.617378 val ap:0.748274  val auc:0.768638\n",
            "\ttotal time:1.83s sample time:0.01s prep time:0.35s\n",
            "Epoch 8:\n",
            "\ttrain loss:11078.3411 train ap:0.634408  train auc:0.670655  val loss:4.578452 val ap:0.733687  val auc:0.758924\n",
            "\ttotal time:1.79s sample time:0.01s prep time:0.35s\n",
            "Epoch 9:\n",
            "\ttrain loss:11162.8888 train ap:0.630903  train auc:0.663760  val loss:4.760356 val ap:0.695866  val auc:0.735207\n",
            "\ttotal time:1.77s sample time:0.01s prep time:0.33s\n",
            "Epoch 10:\n",
            "\ttrain loss:11119.6721 train ap:0.637039  train auc:0.666711  val loss:4.839465 val ap:0.746655  val auc:0.741951\n",
            "\ttotal time:1.82s sample time:0.01s prep time:0.35s\n",
            "Epoch 11:\n",
            "\ttrain loss:11070.2002 train ap:0.635054  train auc:0.668635  val loss:4.769562 val ap:0.735193  val auc:0.757612\n",
            "\ttotal time:2.27s sample time:0.01s prep time:0.42s\n",
            "Epoch 12:\n",
            "\ttrain loss:11003.7689 train ap:0.642516  train auc:0.673698  val loss:4.578309 val ap:0.760111  val auc:0.778622\n",
            "\ttotal time:2.68s sample time:0.00s prep time:0.48s\n",
            "Epoch 13:\n",
            "\ttrain loss:11072.5121 train ap:0.635615  train auc:0.667752  val loss:4.553231 val ap:0.775253  val auc:0.781998\n",
            "\ttotal time:1.83s sample time:0.01s prep time:0.34s\n",
            "Epoch 14:\n",
            "\ttrain loss:10984.5362 train ap:0.640668  train auc:0.675599  val loss:4.672532 val ap:0.736308  val auc:0.752207\n",
            "\ttotal time:1.79s sample time:0.01s prep time:0.34s\n",
            "Epoch 15:\n",
            "\ttrain loss:11035.6846 train ap:0.643154  train auc:0.674206  val loss:4.805106 val ap:0.709078  val auc:0.739848\n",
            "\ttotal time:1.83s sample time:0.01s prep time:0.35s\n",
            "Epoch 16:\n",
            "\ttrain loss:11033.1906 train ap:0.640972  train auc:0.671777  val loss:4.737420 val ap:0.725506  val auc:0.754681\n",
            "\ttotal time:1.85s sample time:0.01s prep time:0.36s\n",
            "Epoch 17:\n",
            "\ttrain loss:11027.8133 train ap:0.639611  train auc:0.672674  val loss:4.767632 val ap:0.712421  val auc:0.742413\n",
            "\ttotal time:1.81s sample time:0.01s prep time:0.34s\n",
            "Epoch 18:\n",
            "\ttrain loss:10973.1699 train ap:0.641367  train auc:0.677088  val loss:4.859386 val ap:0.708422  val auc:0.725803\n",
            "\ttotal time:2.69s sample time:0.00s prep time:0.48s\n",
            "Epoch 19:\n",
            "\ttrain loss:10954.9017 train ap:0.648563  train auc:0.676280  val loss:4.741758 val ap:0.709711  val auc:0.752227\n",
            "\ttotal time:2.24s sample time:0.01s prep time:0.41s\n",
            "Epoch 20:\n",
            "\ttrain loss:10933.1247 train ap:0.652464  train auc:0.680641  val loss:4.792795 val ap:0.727717  val auc:0.747440\n",
            "\ttotal time:1.86s sample time:0.01s prep time:0.35s\n",
            "Epoch 21:\n",
            "\ttrain loss:10988.1832 train ap:0.644389  train auc:0.677132  val loss:4.606892 val ap:0.775207  val auc:0.778581\n",
            "\ttotal time:1.91s sample time:0.01s prep time:0.36s\n",
            "Epoch 22:\n",
            "\ttrain loss:10923.3707 train ap:0.649820  train auc:0.681514  val loss:4.726128 val ap:0.734266  val auc:0.762962\n",
            "\ttotal time:1.90s sample time:0.01s prep time:0.36s\n",
            "Epoch 23:\n",
            "\ttrain loss:10913.6843 train ap:0.650578  train auc:0.683025  val loss:4.829519 val ap:0.740744  val auc:0.747835\n",
            "\ttotal time:1.90s sample time:0.01s prep time:0.35s\n",
            "Epoch 24:\n",
            "\ttrain loss:10930.0595 train ap:0.652245  train auc:0.681911  val loss:4.814076 val ap:0.740978  val auc:0.750964\n",
            "\ttotal time:2.35s sample time:0.00s prep time:0.43s\n",
            "Epoch 25:\n",
            "\ttrain loss:10941.6348 train ap:0.650785  train auc:0.680144  val loss:4.849291 val ap:0.715976  val auc:0.750271\n",
            "\ttotal time:2.60s sample time:0.01s prep time:0.46s\n",
            "Epoch 26:\n",
            "\ttrain loss:10953.0224 train ap:0.639887  train auc:0.676157  val loss:4.748758 val ap:0.697754  val auc:0.737817\n",
            "\ttotal time:1.93s sample time:0.01s prep time:0.36s\n",
            "Epoch 27:\n",
            "\ttrain loss:10918.7617 train ap:0.648432  train auc:0.680113  val loss:4.643740 val ap:0.753282  val auc:0.770592\n",
            "\ttotal time:1.93s sample time:0.01s prep time:0.36s\n",
            "Epoch 28:\n",
            "\ttrain loss:10857.8082 train ap:0.652713  train auc:0.685692  val loss:4.760998 val ap:0.711660  val auc:0.751459\n",
            "\ttotal time:1.87s sample time:0.00s prep time:0.34s\n",
            "Epoch 29:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positive lead/lag"
      ],
      "metadata": {
        "id": "TSUf17pSk91_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python gen_graph.py --data leadlag"
      ],
      "metadata": {
        "id": "L1q6UnMjC_-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data positive --config ./config/DySAT.yml --eval_neg_samples 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grW-w0NNk_AY",
        "outputId": "b9945c83-44bf-4b72-99d8-3f2d288733ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/utils.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  node_feats = torch.load('DATA/{}/node_features.pt'.format(d))\n",
            "Epoch 0:\n",
            "\ttrain loss:3342.6556 train ap:0.567568  train auc:0.567961  val loss:2.699990 val ap:0.616647  val auc:0.657899\n",
            "\ttotal time:1.09s sample time:0.00s prep time:0.13s\n",
            "Epoch 1:\n",
            "\ttrain loss:3245.0935 train ap:0.646640  train auc:0.665176  val loss:2.674381 val ap:0.610849  val auc:0.635184\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.09s\n",
            "Epoch 2:\n",
            "\ttrain loss:3187.2879 train ap:0.638409  train auc:0.667108  val loss:2.493318 val ap:0.657882  val auc:0.712228\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.09s\n",
            "Epoch 3:\n",
            "\ttrain loss:3116.7337 train ap:0.649066  train auc:0.681872  val loss:2.550846 val ap:0.604500  val auc:0.656749\n",
            "\ttotal time:0.49s sample time:0.00s prep time:0.09s\n",
            "Epoch 4:\n",
            "\ttrain loss:3079.7015 train ap:0.658852  train auc:0.688580  val loss:2.514173 val ap:0.644550  val auc:0.669008\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 5:\n",
            "\ttrain loss:3063.3499 train ap:0.661878  train auc:0.692558  val loss:2.433079 val ap:0.648560  val auc:0.708217\n",
            "\ttotal time:0.49s sample time:0.00s prep time:0.10s\n",
            "Epoch 6:\n",
            "\ttrain loss:3059.2161 train ap:0.650647  train auc:0.689801  val loss:2.497127 val ap:0.635248  val auc:0.674349\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.09s\n",
            "Epoch 7:\n",
            "\ttrain loss:3028.4027 train ap:0.668598  train auc:0.704334  val loss:2.445511 val ap:0.689619  val auc:0.715910\n",
            "\ttotal time:0.61s sample time:0.00s prep time:0.12s\n",
            "Epoch 8:\n",
            "\ttrain loss:3061.5690 train ap:0.648412  train auc:0.687065  val loss:2.521434 val ap:0.654022  val auc:0.683492\n",
            "\ttotal time:0.71s sample time:0.00s prep time:0.12s\n",
            "Epoch 9:\n",
            "\ttrain loss:3010.8706 train ap:0.665359  train auc:0.703295  val loss:2.451695 val ap:0.662945  val auc:0.709200\n",
            "\ttotal time:0.73s sample time:0.00s prep time:0.13s\n",
            "Epoch 10:\n",
            "\ttrain loss:2998.9271 train ap:0.658253  train auc:0.705596  val loss:2.654462 val ap:0.612652  val auc:0.641482\n",
            "\ttotal time:0.77s sample time:0.00s prep time:0.14s\n",
            "Epoch 11:\n",
            "\ttrain loss:2947.7720 train ap:0.683540  train auc:0.722049  val loss:2.443811 val ap:0.656742  val auc:0.708205\n",
            "\ttotal time:0.78s sample time:0.00s prep time:0.14s\n",
            "Epoch 12:\n",
            "\ttrain loss:2997.5861 train ap:0.664248  train auc:0.708046  val loss:2.428068 val ap:0.670102  val auc:0.718426\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.09s\n",
            "Epoch 13:\n",
            "\ttrain loss:2961.2406 train ap:0.683139  train auc:0.723240  val loss:2.492182 val ap:0.654131  val auc:0.698735\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.10s\n",
            "Epoch 14:\n",
            "\ttrain loss:2950.0521 train ap:0.684079  train auc:0.725018  val loss:2.434000 val ap:0.681165  val auc:0.711334\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.10s\n",
            "Epoch 15:\n",
            "\ttrain loss:2951.7252 train ap:0.676939  train auc:0.718315  val loss:2.500859 val ap:0.658412  val auc:0.697645\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.09s\n",
            "Epoch 16:\n",
            "\ttrain loss:2937.1026 train ap:0.676138  train auc:0.722637  val loss:2.519179 val ap:0.665719  val auc:0.688253\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.10s\n",
            "Epoch 17:\n",
            "\ttrain loss:2923.8911 train ap:0.684044  train auc:0.725409  val loss:2.483956 val ap:0.670669  val auc:0.701256\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.09s\n",
            "Epoch 18:\n",
            "\ttrain loss:2949.6355 train ap:0.669822  train auc:0.716808  val loss:2.419733 val ap:0.685126  val auc:0.727642\n",
            "\ttotal time:0.48s sample time:0.00s prep time:0.09s\n",
            "Epoch 19:\n",
            "\ttrain loss:2913.3935 train ap:0.690901  train auc:0.728874  val loss:2.521217 val ap:0.658303  val auc:0.699654\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 20:\n",
            "\ttrain loss:2895.6385 train ap:0.682293  train auc:0.729502  val loss:2.536349 val ap:0.679681  val auc:0.688977\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.10s\n",
            "Epoch 21:\n",
            "\ttrain loss:2944.1572 train ap:0.670520  train auc:0.714387  val loss:2.569362 val ap:0.633952  val auc:0.667562\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.10s\n",
            "Epoch 22:\n",
            "\ttrain loss:2909.9950 train ap:0.679446  train auc:0.722919  val loss:2.568012 val ap:0.628357  val auc:0.678313\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 23:\n",
            "\ttrain loss:2866.5921 train ap:0.693133  train auc:0.738926  val loss:2.348927 val ap:0.739520  val auc:0.760358\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.09s\n",
            "Epoch 24:\n",
            "\ttrain loss:2874.0454 train ap:0.698738  train auc:0.735522  val loss:2.559595 val ap:0.640442  val auc:0.689758\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.10s\n",
            "Epoch 25:\n",
            "\ttrain loss:2885.6191 train ap:0.696536  train auc:0.730483  val loss:2.466608 val ap:0.665985  val auc:0.715694\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.09s\n",
            "Epoch 26:\n",
            "\ttrain loss:2859.1372 train ap:0.682262  train auc:0.732640  val loss:2.656320 val ap:0.653281  val auc:0.670315\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.10s\n",
            "Epoch 27:\n",
            "\ttrain loss:2876.6226 train ap:0.700290  train auc:0.735773  val loss:2.531817 val ap:0.666713  val auc:0.710615\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.09s\n",
            "Epoch 28:\n",
            "\ttrain loss:2861.2985 train ap:0.691195  train auc:0.733314  val loss:2.399437 val ap:0.707027  val auc:0.753117\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.10s\n",
            "Epoch 29:\n",
            "\ttrain loss:2852.0737 train ap:0.696220  train auc:0.741767  val loss:2.703118 val ap:0.624326  val auc:0.655188\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 30:\n",
            "\ttrain loss:2852.2362 train ap:0.695795  train auc:0.739728  val loss:2.546768 val ap:0.670744  val auc:0.716534\n",
            "\ttotal time:0.74s sample time:0.00s prep time:0.14s\n",
            "Epoch 31:\n",
            "\ttrain loss:2828.9489 train ap:0.701824  train auc:0.744911  val loss:2.705182 val ap:0.617048  val auc:0.659430\n",
            "\ttotal time:0.73s sample time:0.00s prep time:0.13s\n",
            "Epoch 32:\n",
            "\ttrain loss:2797.2787 train ap:0.704124  train auc:0.752521  val loss:2.623407 val ap:0.607858  val auc:0.669215\n",
            "\ttotal time:0.70s sample time:0.00s prep time:0.13s\n",
            "Epoch 33:\n",
            "\ttrain loss:2846.2775 train ap:0.690468  train auc:0.737324  val loss:2.576354 val ap:0.653517  val auc:0.699960\n",
            "\ttotal time:0.76s sample time:0.00s prep time:0.14s\n",
            "Epoch 34:\n",
            "\ttrain loss:2803.8019 train ap:0.693470  train auc:0.748108  val loss:2.620984 val ap:0.643632  val auc:0.680448\n",
            "\ttotal time:0.71s sample time:0.00s prep time:0.13s\n",
            "Epoch 35:\n",
            "\ttrain loss:2801.2603 train ap:0.710993  train auc:0.748431  val loss:2.709258 val ap:0.658701  val auc:0.674353\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.10s\n",
            "Epoch 36:\n",
            "\ttrain loss:2809.1667 train ap:0.697259  train auc:0.743057  val loss:2.827642 val ap:0.606191  val auc:0.635422\n",
            "\ttotal time:0.49s sample time:0.00s prep time:0.09s\n",
            "Epoch 37:\n",
            "\ttrain loss:2789.1365 train ap:0.698400  train auc:0.748673  val loss:2.528650 val ap:0.694393  val auc:0.729321\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.09s\n",
            "Epoch 38:\n",
            "\ttrain loss:2781.8093 train ap:0.716757  train auc:0.753219  val loss:2.486818 val ap:0.721411  val auc:0.753277\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 39:\n",
            "\ttrain loss:2820.2977 train ap:0.688591  train auc:0.740376  val loss:2.738838 val ap:0.627421  val auc:0.657839\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 40:\n",
            "\ttrain loss:2788.1737 train ap:0.693918  train auc:0.748029  val loss:2.636757 val ap:0.641067  val auc:0.686773\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 41:\n",
            "\ttrain loss:2768.6994 train ap:0.713751  train auc:0.760062  val loss:2.668499 val ap:0.645249  val auc:0.693170\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.10s\n",
            "Epoch 42:\n",
            "\ttrain loss:2783.5805 train ap:0.698327  train auc:0.749564  val loss:2.553685 val ap:0.682637  val auc:0.722241\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.09s\n",
            "Epoch 43:\n",
            "\ttrain loss:2824.9769 train ap:0.686140  train auc:0.734471  val loss:2.459163 val ap:0.714504  val auc:0.746473\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.09s\n",
            "Epoch 44:\n",
            "\ttrain loss:2803.2468 train ap:0.698393  train auc:0.744956  val loss:2.705935 val ap:0.638103  val auc:0.677080\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 45:\n",
            "\ttrain loss:2799.7541 train ap:0.702546  train auc:0.743607  val loss:2.707355 val ap:0.586786  val auc:0.652634\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.10s\n",
            "Epoch 46:\n",
            "\ttrain loss:2761.0657 train ap:0.710574  train auc:0.754731  val loss:2.694976 val ap:0.652469  val auc:0.684925\n",
            "\ttotal time:0.49s sample time:0.00s prep time:0.09s\n",
            "Epoch 47:\n",
            "\ttrain loss:2722.5848 train ap:0.715714  train auc:0.761638  val loss:2.681256 val ap:0.688595  val auc:0.716396\n",
            "\ttotal time:0.48s sample time:0.00s prep time:0.09s\n",
            "Epoch 48:\n",
            "\ttrain loss:2759.6020 train ap:0.705487  train auc:0.753601  val loss:2.745646 val ap:0.632409  val auc:0.683376\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.09s\n",
            "Epoch 49:\n",
            "\ttrain loss:2768.4100 train ap:0.707650  train auc:0.748059  val loss:2.696689 val ap:0.642059  val auc:0.692470\n",
            "\ttotal time:0.49s sample time:0.00s prep time:0.09s\n",
            "Epoch 50:\n",
            "\ttrain loss:2730.5924 train ap:0.720076  train auc:0.763677  val loss:2.732058 val ap:0.630856  val auc:0.689500\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.09s\n",
            "Epoch 51:\n",
            "\ttrain loss:2793.6330 train ap:0.696579  train auc:0.747710  val loss:2.631083 val ap:0.652460  val auc:0.712201\n",
            "\ttotal time:0.49s sample time:0.00s prep time:0.09s\n",
            "Epoch 52:\n",
            "\ttrain loss:2728.1372 train ap:0.713051  train auc:0.758150  val loss:2.843882 val ap:0.627024  val auc:0.649059\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 53:\n",
            "\ttrain loss:2745.8559 train ap:0.709799  train auc:0.755731  val loss:2.614584 val ap:0.654460  val auc:0.708619\n",
            "\ttotal time:0.70s sample time:0.00s prep time:0.12s\n",
            "Epoch 54:\n",
            "\ttrain loss:2760.2851 train ap:0.710425  train auc:0.753562  val loss:2.733605 val ap:0.640595  val auc:0.684989\n",
            "\ttotal time:0.74s sample time:0.00s prep time:0.15s\n",
            "Epoch 55:\n",
            "\ttrain loss:2778.5045 train ap:0.695524  train auc:0.745460  val loss:2.751614 val ap:0.639992  val auc:0.678690\n",
            "\ttotal time:0.72s sample time:0.00s prep time:0.13s\n",
            "Epoch 56:\n",
            "\ttrain loss:2778.6553 train ap:0.707187  train auc:0.747762  val loss:2.878645 val ap:0.623720  val auc:0.649781\n",
            "\ttotal time:0.76s sample time:0.00s prep time:0.14s\n",
            "Epoch 57:\n",
            "\ttrain loss:2712.9100 train ap:0.724528  train auc:0.764892  val loss:2.791471 val ap:0.615903  val auc:0.670667\n",
            "\ttotal time:0.67s sample time:0.00s prep time:0.12s\n",
            "Epoch 58:\n",
            "\ttrain loss:2729.7022 train ap:0.714699  train auc:0.758259  val loss:2.949180 val ap:0.609137  val auc:0.643488\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.10s\n",
            "Epoch 59:\n",
            "\ttrain loss:2697.0890 train ap:0.713365  train auc:0.763863  val loss:2.824357 val ap:0.631720  val auc:0.676596\n",
            "\ttotal time:0.49s sample time:0.00s prep time:0.09s\n",
            "Epoch 60:\n",
            "\ttrain loss:2681.7265 train ap:0.727944  train auc:0.769973  val loss:2.910000 val ap:0.591385  val auc:0.646334\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.10s\n",
            "Epoch 61:\n",
            "\ttrain loss:2718.7068 train ap:0.722052  train auc:0.765733  val loss:2.700541 val ap:0.645221  val auc:0.702202\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.09s\n",
            "Epoch 62:\n",
            "\ttrain loss:2679.7079 train ap:0.720506  train auc:0.766389  val loss:2.821266 val ap:0.601205  val auc:0.672526\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 63:\n",
            "\ttrain loss:2685.1021 train ap:0.730183  train auc:0.769068  val loss:2.762157 val ap:0.654808  val auc:0.695116\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 64:\n",
            "\ttrain loss:2715.0791 train ap:0.714537  train auc:0.762964  val loss:2.722520 val ap:0.656616  val auc:0.696374\n",
            "\ttotal time:0.49s sample time:0.00s prep time:0.10s\n",
            "Epoch 65:\n",
            "\ttrain loss:2709.7704 train ap:0.726873  train auc:0.764268  val loss:2.766521 val ap:0.640480  val auc:0.679798\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.10s\n",
            "Epoch 66:\n",
            "\ttrain loss:2697.1564 train ap:0.717075  train auc:0.764792  val loss:2.759046 val ap:0.649279  val auc:0.693734\n",
            "\ttotal time:0.48s sample time:0.00s prep time:0.09s\n",
            "Epoch 67:\n",
            "\ttrain loss:2707.9624 train ap:0.721747  train auc:0.761070  val loss:3.019064 val ap:0.602966  val auc:0.631072\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 68:\n",
            "\ttrain loss:2669.1333 train ap:0.731951  train auc:0.778514  val loss:2.776350 val ap:0.688636  val auc:0.716233\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.10s\n",
            "Epoch 69:\n",
            "\ttrain loss:2697.8335 train ap:0.713901  train auc:0.763195  val loss:2.905726 val ap:0.618799  val auc:0.656345\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.09s\n",
            "Epoch 70:\n",
            "\ttrain loss:2657.6859 train ap:0.738161  train auc:0.776219  val loss:2.746056 val ap:0.646419  val auc:0.700100\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.10s\n",
            "Epoch 71:\n",
            "\ttrain loss:2718.5681 train ap:0.721388  train auc:0.762532  val loss:2.704948 val ap:0.668924  val auc:0.711269\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.10s\n",
            "Epoch 72:\n",
            "\ttrain loss:2705.6530 train ap:0.720991  train auc:0.761347  val loss:2.646745 val ap:0.674397  val auc:0.718228\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.09s\n",
            "Epoch 73:\n",
            "\ttrain loss:2667.7953 train ap:0.715850  train auc:0.764306  val loss:2.801062 val ap:0.692185  val auc:0.702045\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.10s\n",
            "Epoch 74:\n",
            "\ttrain loss:2663.2483 train ap:0.724670  train auc:0.773170  val loss:2.920317 val ap:0.611253  val auc:0.660341\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.10s\n",
            "Epoch 75:\n",
            "\ttrain loss:2674.8876 train ap:0.721510  train auc:0.767997  val loss:2.892267 val ap:0.637492  val auc:0.682202\n",
            "\ttotal time:0.61s sample time:0.00s prep time:0.11s\n",
            "Epoch 76:\n",
            "\ttrain loss:2713.3627 train ap:0.718761  train auc:0.762304  val loss:2.767035 val ap:0.660734  val auc:0.707518\n",
            "\ttotal time:0.76s sample time:0.00s prep time:0.13s\n",
            "Epoch 77:\n",
            "\ttrain loss:2673.7471 train ap:0.724963  train auc:0.766434  val loss:2.916807 val ap:0.627882  val auc:0.668207\n",
            "\ttotal time:0.78s sample time:0.00s prep time:0.14s\n",
            "Epoch 78:\n",
            "\ttrain loss:2673.8767 train ap:0.728220  train auc:0.770245  val loss:2.875027 val ap:0.642857  val auc:0.681470\n",
            "\ttotal time:0.77s sample time:0.00s prep time:0.13s\n",
            "Epoch 79:\n",
            "\ttrain loss:2704.7155 train ap:0.714037  train auc:0.757830  val loss:2.720019 val ap:0.696798  val auc:0.718075\n",
            "\ttotal time:0.80s sample time:0.00s prep time:0.14s\n",
            "Epoch 80:\n",
            "\ttrain loss:2662.2350 train ap:0.721401  train auc:0.769363  val loss:2.737810 val ap:0.666612  val auc:0.711051\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.10s\n",
            "Epoch 81:\n",
            "\ttrain loss:2653.2261 train ap:0.721082  train auc:0.771031  val loss:2.859723 val ap:0.644436  val auc:0.673371\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.10s\n",
            "Epoch 82:\n",
            "\ttrain loss:2688.8191 train ap:0.716172  train auc:0.763084  val loss:2.841902 val ap:0.632655  val auc:0.678821\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.09s\n",
            "Epoch 83:\n",
            "\ttrain loss:2627.0133 train ap:0.738640  train auc:0.780341  val loss:2.935848 val ap:0.635889  val auc:0.670912\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.09s\n",
            "Epoch 84:\n",
            "\ttrain loss:2664.7086 train ap:0.730204  train auc:0.769903  val loss:2.902053 val ap:0.616500  val auc:0.666603\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.09s\n",
            "Epoch 85:\n",
            "\ttrain loss:2644.4089 train ap:0.731471  train auc:0.774607  val loss:2.939949 val ap:0.610464  val auc:0.646701\n",
            "\ttotal time:0.49s sample time:0.00s prep time:0.09s\n",
            "Epoch 86:\n",
            "\ttrain loss:2675.4694 train ap:0.725763  train auc:0.766908  val loss:2.832149 val ap:0.665630  val auc:0.689567\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.10s\n",
            "Epoch 87:\n",
            "\ttrain loss:2653.4693 train ap:0.733667  train auc:0.777162  val loss:2.667245 val ap:0.699278  val auc:0.738483\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.10s\n",
            "Epoch 88:\n",
            "\ttrain loss:2644.8407 train ap:0.724913  train auc:0.768756  val loss:2.852089 val ap:0.645800  val auc:0.683206\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.09s\n",
            "Epoch 89:\n",
            "\ttrain loss:2662.1534 train ap:0.717147  train auc:0.768002  val loss:2.908111 val ap:0.645658  val auc:0.677292\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.10s\n",
            "Loading model at epoch 23...\n",
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/train.py:290: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path_saver))\n",
            "{'test_R@1': 0.1904762, 'test_R@5': 0.44761905, 'test_R@10': 0.6761905, 'test_mrr': 0.264461441889902}\n",
            "\taverage test precision:0.645658  test AUC:0.677292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TGAT"
      ],
      "metadata": {
        "id": "4Z_edgxGPC8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data leadlag --config ./config/TGAT.yml --eval_neg_samples 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuJHzqr-adns",
        "outputId": "8f66c4c4-c3a4-46b7-f946-b1251f82ee52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/utils.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  node_feats = torch.load('DATA/{}/node_features.pt'.format(d))\n",
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/utils.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  edge_feats = torch.load('DATA/{}/edge_features.pt'.format(d))\n",
            "Epoch 0:\n",
            "\ttrain loss:11653.2178 train ap:0.581647  train auc:0.604629  val loss:5.001884 val ap:0.678130  val auc:0.713207\n",
            "\ttotal time:2.58s sample time:0.01s prep time:0.26s\n",
            "Epoch 1:\n",
            "\ttrain loss:11226.5374 train ap:0.622923  train auc:0.653813  val loss:4.653022 val ap:0.719660  val auc:0.758115\n",
            "\ttotal time:1.64s sample time:0.01s prep time:0.19s\n",
            "Epoch 2:\n",
            "\ttrain loss:11158.3716 train ap:0.622271  train auc:0.656126  val loss:4.606540 val ap:0.730042  val auc:0.754466\n",
            "\ttotal time:1.65s sample time:0.01s prep time:0.19s\n",
            "Epoch 3:\n",
            "\ttrain loss:11119.2251 train ap:0.622617  train auc:0.655818  val loss:4.675408 val ap:0.715226  val auc:0.746518\n",
            "\ttotal time:1.68s sample time:0.01s prep time:0.20s\n",
            "Epoch 4:\n",
            "\ttrain loss:10967.5526 train ap:0.631076  train auc:0.667040  val loss:4.929156 val ap:0.708943  val auc:0.715407\n",
            "\ttotal time:1.72s sample time:0.01s prep time:0.20s\n",
            "Epoch 5:\n",
            "\ttrain loss:10979.9524 train ap:0.632029  train auc:0.665539  val loss:4.980302 val ap:0.655929  val auc:0.695354\n",
            "\ttotal time:1.79s sample time:0.00s prep time:0.22s\n",
            "Epoch 6:\n",
            "\ttrain loss:10839.9996 train ap:0.640282  train auc:0.678296  val loss:4.868878 val ap:0.686515  val auc:0.722158\n",
            "\ttotal time:2.20s sample time:0.00s prep time:0.26s\n",
            "Epoch 7:\n",
            "\ttrain loss:10827.1888 train ap:0.640109  train auc:0.676964  val loss:4.787454 val ap:0.690835  val auc:0.727365\n",
            "\ttotal time:2.02s sample time:0.00s prep time:0.23s\n",
            "Epoch 8:\n",
            "\ttrain loss:10819.3059 train ap:0.639735  train auc:0.679421  val loss:4.711480 val ap:0.697304  val auc:0.749393\n",
            "\ttotal time:1.66s sample time:0.01s prep time:0.19s\n",
            "Epoch 9:\n",
            "\ttrain loss:10711.1100 train ap:0.646582  train auc:0.687173  val loss:4.806503 val ap:0.692548  val auc:0.738711\n",
            "\ttotal time:1.68s sample time:0.01s prep time:0.20s\n",
            "Epoch 10:\n",
            "\ttrain loss:10666.1823 train ap:0.654114  train auc:0.689885  val loss:4.936336 val ap:0.666037  val auc:0.708993\n",
            "\ttotal time:1.66s sample time:0.01s prep time:0.19s\n",
            "Epoch 11:\n",
            "\ttrain loss:10704.0283 train ap:0.647274  train auc:0.684857  val loss:4.859523 val ap:0.681346  val auc:0.719741\n",
            "\ttotal time:1.75s sample time:0.01s prep time:0.19s\n",
            "Epoch 12:\n",
            "\ttrain loss:10633.1550 train ap:0.650483  train auc:0.691533  val loss:4.908509 val ap:0.692057  val auc:0.731186\n",
            "\ttotal time:1.74s sample time:0.00s prep time:0.19s\n",
            "Epoch 13:\n",
            "\ttrain loss:10649.5282 train ap:0.651841  train auc:0.689824  val loss:4.904436 val ap:0.736222  val auc:0.739007\n",
            "\ttotal time:2.14s sample time:0.00s prep time:0.24s\n",
            "Epoch 14:\n",
            "\ttrain loss:10602.9742 train ap:0.657382  train auc:0.693649  val loss:4.959321 val ap:0.671272  val auc:0.705973\n",
            "\ttotal time:2.33s sample time:0.00s prep time:0.26s\n",
            "Epoch 15:\n",
            "\ttrain loss:10588.9811 train ap:0.656564  train auc:0.697854  val loss:4.630685 val ap:0.737030  val auc:0.771339\n",
            "\ttotal time:1.77s sample time:0.01s prep time:0.20s\n",
            "Epoch 16:\n",
            "\ttrain loss:10555.5509 train ap:0.657767  train auc:0.699730  val loss:4.724886 val ap:0.718508  val auc:0.747857\n",
            "\ttotal time:1.78s sample time:0.01s prep time:0.20s\n",
            "Epoch 17:\n",
            "\ttrain loss:10549.8482 train ap:0.662809  train auc:0.701318  val loss:4.701850 val ap:0.704869  val auc:0.751932\n",
            "\ttotal time:1.77s sample time:0.00s prep time:0.20s\n",
            "Epoch 18:\n",
            "\ttrain loss:10496.1059 train ap:0.661307  train auc:0.702561  val loss:4.723909 val ap:0.705433  val auc:0.741673\n",
            "\ttotal time:1.75s sample time:0.00s prep time:0.19s\n",
            "Epoch 19:\n",
            "\ttrain loss:10446.3319 train ap:0.666029  train auc:0.707442  val loss:4.635165 val ap:0.761763  val auc:0.771089\n",
            "\ttotal time:1.74s sample time:0.00s prep time:0.19s\n",
            "Epoch 20:\n",
            "\ttrain loss:10458.7299 train ap:0.666215  train auc:0.703109  val loss:4.614205 val ap:0.728788  val auc:0.763323\n",
            "\ttotal time:2.13s sample time:0.00s prep time:0.24s\n",
            "Epoch 21:\n",
            "\ttrain loss:10430.9982 train ap:0.668901  train auc:0.708798  val loss:4.633292 val ap:0.710963  val auc:0.755639\n",
            "\ttotal time:2.35s sample time:0.00s prep time:0.27s\n",
            "Epoch 22:\n",
            "\ttrain loss:10395.6994 train ap:0.674600  train auc:0.711519  val loss:4.804811 val ap:0.682258  val auc:0.725642\n",
            "\ttotal time:1.78s sample time:0.01s prep time:0.20s\n",
            "Epoch 23:\n",
            "\ttrain loss:10448.8424 train ap:0.663502  train auc:0.704152  val loss:4.597906 val ap:0.736582  val auc:0.775991\n",
            "\ttotal time:1.74s sample time:0.01s prep time:0.19s\n",
            "Epoch 24:\n",
            "\ttrain loss:10302.3533 train ap:0.677454  train auc:0.716452  val loss:4.744235 val ap:0.742313  val auc:0.770347\n",
            "\ttotal time:1.74s sample time:0.01s prep time:0.19s\n",
            "Epoch 25:\n",
            "\ttrain loss:10312.6406 train ap:0.679180  train auc:0.716384  val loss:4.880187 val ap:0.662081  val auc:0.721955\n",
            "\ttotal time:1.73s sample time:0.01s prep time:0.19s\n",
            "Epoch 26:\n",
            "\ttrain loss:10325.5473 train ap:0.673953  train auc:0.715857  val loss:4.656613 val ap:0.729545  val auc:0.757018\n",
            "\ttotal time:1.75s sample time:0.01s prep time:0.19s\n",
            "Epoch 27:\n",
            "\ttrain loss:10275.4210 train ap:0.677269  train auc:0.717485  val loss:4.842584 val ap:0.673798  val auc:0.719808\n",
            "\ttotal time:2.03s sample time:0.00s prep time:0.23s\n",
            "Epoch 28:\n",
            "\ttrain loss:10277.4257 train ap:0.671848  train auc:0.714882  val loss:4.546736 val ap:0.707711  val auc:0.762706\n",
            "\ttotal time:2.39s sample time:0.00s prep time:0.27s\n",
            "Epoch 29:\n",
            "\ttrain loss:10207.6971 train ap:0.683974  train auc:0.725507  val loss:4.588535 val ap:0.717039  val auc:0.755324\n",
            "\ttotal time:1.78s sample time:0.01s prep time:0.20s\n",
            "Epoch 30:\n",
            "\ttrain loss:10268.8080 train ap:0.675662  train auc:0.717728  val loss:4.865584 val ap:0.664658  val auc:0.718734\n",
            "\ttotal time:1.72s sample time:0.01s prep time:0.19s\n",
            "Epoch 31:\n",
            "\ttrain loss:10171.6791 train ap:0.679918  train auc:0.721811  val loss:4.666089 val ap:0.717386  val auc:0.764804\n",
            "\ttotal time:1.76s sample time:0.01s prep time:0.19s\n",
            "Epoch 32:\n",
            "\ttrain loss:10222.4267 train ap:0.687258  train auc:0.723219  val loss:4.872670 val ap:0.708809  val auc:0.752809\n",
            "\ttotal time:1.72s sample time:0.01s prep time:0.19s\n",
            "Epoch 33:\n",
            "\ttrain loss:10173.4631 train ap:0.680600  train auc:0.723149  val loss:4.667638 val ap:0.767785  val auc:0.778435\n",
            "\ttotal time:1.73s sample time:0.01s prep time:0.19s\n",
            "Epoch 34:\n",
            "\ttrain loss:10183.4884 train ap:0.684397  train auc:0.723564  val loss:4.763412 val ap:0.692271  val auc:0.743940\n",
            "\ttotal time:1.96s sample time:0.00s prep time:0.22s\n",
            "Epoch 35:\n",
            "\ttrain loss:10148.6908 train ap:0.681003  train auc:0.723307  val loss:4.793014 val ap:0.699836  val auc:0.727204\n",
            "\ttotal time:2.35s sample time:0.00s prep time:0.28s\n",
            "Epoch 36:\n",
            "\ttrain loss:10143.8177 train ap:0.692733  train auc:0.729266  val loss:4.793864 val ap:0.696563  val auc:0.731785\n",
            "\ttotal time:1.90s sample time:0.00s prep time:0.20s\n",
            "Epoch 37:\n",
            "\ttrain loss:10092.2840 train ap:0.690953  train auc:0.730076  val loss:4.556840 val ap:0.774833  val auc:0.795644\n",
            "\ttotal time:1.77s sample time:0.01s prep time:0.20s\n",
            "Epoch 38:\n",
            "\ttrain loss:10115.6184 train ap:0.686380  train auc:0.728384  val loss:5.123046 val ap:0.684229  val auc:0.719821\n",
            "\ttotal time:1.75s sample time:0.01s prep time:0.20s\n",
            "Epoch 39:\n",
            "\ttrain loss:10130.0983 train ap:0.688083  train auc:0.726197  val loss:4.649761 val ap:0.743417  val auc:0.774636\n",
            "\ttotal time:1.74s sample time:0.00s prep time:0.19s\n",
            "Epoch 40:\n",
            "\ttrain loss:10072.2280 train ap:0.689799  train auc:0.730568  val loss:5.260069 val ap:0.667835  val auc:0.715090\n",
            "\ttotal time:1.74s sample time:0.01s prep time:0.19s\n",
            "Epoch 41:\n",
            "\ttrain loss:10107.8236 train ap:0.690604  train auc:0.730160  val loss:5.016580 val ap:0.687077  val auc:0.711219\n",
            "\ttotal time:1.93s sample time:0.00s prep time:0.22s\n",
            "Epoch 42:\n",
            "\ttrain loss:10058.5858 train ap:0.689157  train auc:0.734753  val loss:4.975484 val ap:0.711235  val auc:0.747345\n",
            "\ttotal time:2.26s sample time:0.00s prep time:0.26s\n",
            "Epoch 43:\n",
            "\ttrain loss:10002.8714 train ap:0.690571  train auc:0.733634  val loss:4.905858 val ap:0.692101  val auc:0.738024\n",
            "\ttotal time:1.91s sample time:0.00s prep time:0.20s\n",
            "Epoch 44:\n",
            "\ttrain loss:9992.6230 train ap:0.694377  train auc:0.737115  val loss:4.999277 val ap:0.735137  val auc:0.747299\n",
            "\ttotal time:1.75s sample time:0.01s prep time:0.19s\n",
            "Epoch 45:\n",
            "\ttrain loss:9991.0803 train ap:0.697062  train auc:0.737984  val loss:4.633581 val ap:0.700788  val auc:0.757540\n",
            "\ttotal time:1.77s sample time:0.00s prep time:0.19s\n",
            "Epoch 46:\n",
            "\ttrain loss:9921.7906 train ap:0.700482  train auc:0.740637  val loss:4.889278 val ap:0.729365  val auc:0.744553\n",
            "\ttotal time:1.78s sample time:0.01s prep time:0.20s\n",
            "Epoch 47:\n",
            "\ttrain loss:9930.8657 train ap:0.698570  train auc:0.738370  val loss:5.123075 val ap:0.698828  val auc:0.710772\n",
            "\ttotal time:1.76s sample time:0.01s prep time:0.20s\n",
            "Epoch 48:\n",
            "\ttrain loss:9913.2911 train ap:0.699860  train auc:0.740595  val loss:4.751268 val ap:0.709276  val auc:0.752830\n",
            "\ttotal time:1.90s sample time:0.01s prep time:0.22s\n",
            "Epoch 49:\n",
            "\ttrain loss:9930.1901 train ap:0.699024  train auc:0.742454  val loss:4.649429 val ap:0.753764  val auc:0.775593\n",
            "\ttotal time:2.27s sample time:0.00s prep time:0.26s\n",
            "Epoch 50:\n",
            "\ttrain loss:9820.5746 train ap:0.702965  train auc:0.745851  val loss:5.262847 val ap:0.710162  val auc:0.722481\n",
            "\ttotal time:1.98s sample time:0.00s prep time:0.21s\n",
            "Epoch 51:\n",
            "\ttrain loss:10017.1841 train ap:0.694649  train auc:0.737522  val loss:4.909408 val ap:0.766396  val auc:0.770918\n",
            "\ttotal time:1.78s sample time:0.01s prep time:0.20s\n",
            "Epoch 52:\n",
            "\ttrain loss:9875.8359 train ap:0.700685  train auc:0.742727  val loss:5.100496 val ap:0.718156  val auc:0.729756\n",
            "\ttotal time:1.76s sample time:0.01s prep time:0.20s\n",
            "Epoch 53:\n",
            "\ttrain loss:9833.0903 train ap:0.703806  train auc:0.748132  val loss:4.975957 val ap:0.727279  val auc:0.765109\n",
            "\ttotal time:1.76s sample time:0.01s prep time:0.20s\n",
            "Epoch 54:\n",
            "\ttrain loss:9836.8741 train ap:0.701923  train auc:0.745954  val loss:5.013456 val ap:0.765876  val auc:0.784965\n",
            "\ttotal time:1.78s sample time:0.01s prep time:0.20s\n",
            "Epoch 55:\n",
            "\ttrain loss:9820.8011 train ap:0.706859  train auc:0.746406  val loss:4.935719 val ap:0.722869  val auc:0.757047\n",
            "\ttotal time:1.84s sample time:0.01s prep time:0.21s\n",
            "Epoch 56:\n",
            "\ttrain loss:9864.8738 train ap:0.705188  train auc:0.746830  val loss:4.848664 val ap:0.733417  val auc:0.765774\n",
            "\ttotal time:2.30s sample time:0.00s prep time:0.26s\n",
            "Epoch 57:\n",
            "\ttrain loss:9776.2846 train ap:0.708514  train auc:0.748784  val loss:5.245730 val ap:0.715811  val auc:0.737677\n",
            "\ttotal time:2.04s sample time:0.01s prep time:0.22s\n",
            "Epoch 58:\n",
            "\ttrain loss:9755.1451 train ap:0.713807  train auc:0.753486  val loss:5.502717 val ap:0.696747  val auc:0.722778\n",
            "\ttotal time:1.75s sample time:0.01s prep time:0.19s\n",
            "Epoch 59:\n",
            "\ttrain loss:9687.0286 train ap:0.717730  train auc:0.759226  val loss:5.206519 val ap:0.725163  val auc:0.752167\n",
            "\ttotal time:1.76s sample time:0.01s prep time:0.20s\n",
            "Epoch 60:\n",
            "\ttrain loss:9659.1796 train ap:0.720540  train auc:0.758746  val loss:4.948046 val ap:0.674428  val auc:0.733991\n",
            "\ttotal time:1.84s sample time:0.01s prep time:0.21s\n",
            "Epoch 61:\n",
            "\ttrain loss:9730.1900 train ap:0.709074  train auc:0.751444  val loss:5.051745 val ap:0.712938  val auc:0.750211\n",
            "\ttotal time:1.82s sample time:0.01s prep time:0.20s\n",
            "Epoch 62:\n",
            "\ttrain loss:9721.4192 train ap:0.711863  train auc:0.754290  val loss:5.434733 val ap:0.690171  val auc:0.730211\n",
            "\ttotal time:1.87s sample time:0.01s prep time:0.21s\n",
            "Epoch 63:\n",
            "\ttrain loss:9681.1599 train ap:0.717197  train auc:0.755696  val loss:4.939250 val ap:0.743718  val auc:0.771585\n",
            "\ttotal time:2.27s sample time:0.00s prep time:0.26s\n",
            "Epoch 64:\n",
            "\ttrain loss:9703.3962 train ap:0.714157  train auc:0.756277  val loss:5.241284 val ap:0.699452  val auc:0.733211\n",
            "\ttotal time:2.07s sample time:0.00s prep time:0.22s\n",
            "Epoch 65:\n",
            "\ttrain loss:9669.9727 train ap:0.721690  train auc:0.761651  val loss:5.320075 val ap:0.673818  val auc:0.721312\n",
            "\ttotal time:1.76s sample time:0.00s prep time:0.20s\n",
            "Epoch 66:\n",
            "\ttrain loss:9623.6235 train ap:0.722388  train auc:0.760285  val loss:5.455979 val ap:0.745505  val auc:0.764365\n",
            "\ttotal time:1.78s sample time:0.01s prep time:0.19s\n",
            "Epoch 67:\n",
            "\ttrain loss:9585.7959 train ap:0.721404  train auc:0.763166  val loss:5.286891 val ap:0.712640  val auc:0.742568\n",
            "\ttotal time:1.78s sample time:0.01s prep time:0.20s\n",
            "Epoch 68:\n",
            "\ttrain loss:9711.1389 train ap:0.716987  train auc:0.756134  val loss:5.355728 val ap:0.712500  val auc:0.753766\n",
            "\ttotal time:1.73s sample time:0.00s prep time:0.20s\n",
            "Epoch 69:\n",
            "\ttrain loss:9649.8055 train ap:0.716070  train auc:0.758582  val loss:4.863269 val ap:0.734809  val auc:0.776885\n",
            "\ttotal time:1.84s sample time:0.00s prep time:0.21s\n",
            "Epoch 70:\n",
            "\ttrain loss:9628.5507 train ap:0.725075  train auc:0.762298  val loss:5.230582 val ap:0.728553  val auc:0.746771\n",
            "\ttotal time:2.25s sample time:0.00s prep time:0.26s\n",
            "Epoch 71:\n",
            "\ttrain loss:9639.9755 train ap:0.720088  train auc:0.761676  val loss:5.506029 val ap:0.722438  val auc:0.740403\n",
            "\ttotal time:2.13s sample time:0.00s prep time:0.24s\n",
            "Epoch 72:\n",
            "\ttrain loss:9654.0965 train ap:0.717198  train auc:0.757236  val loss:5.876004 val ap:0.680727  val auc:0.709529\n",
            "\ttotal time:1.75s sample time:0.01s prep time:0.19s\n",
            "Epoch 73:\n",
            "\ttrain loss:9543.1655 train ap:0.727800  train auc:0.766061  val loss:5.458040 val ap:0.711091  val auc:0.727385\n",
            "\ttotal time:1.74s sample time:0.01s prep time:0.19s\n",
            "Epoch 74:\n",
            "\ttrain loss:9465.9764 train ap:0.723815  train auc:0.768798  val loss:5.717824 val ap:0.694248  val auc:0.710390\n",
            "\ttotal time:1.75s sample time:0.00s prep time:0.19s\n",
            "Epoch 75:\n",
            "\ttrain loss:9450.6159 train ap:0.731664  train auc:0.772242  val loss:5.208153 val ap:0.764721  val auc:0.797947\n",
            "\ttotal time:1.77s sample time:0.01s prep time:0.20s\n",
            "Epoch 76:\n",
            "\ttrain loss:9478.3969 train ap:0.721847  train auc:0.766650  val loss:5.488159 val ap:0.704730  val auc:0.753669\n",
            "\ttotal time:1.75s sample time:0.01s prep time:0.19s\n",
            "Epoch 77:\n",
            "\ttrain loss:9538.4642 train ap:0.728010  train auc:0.766029  val loss:5.752561 val ap:0.720910  val auc:0.746047\n",
            "\ttotal time:2.32s sample time:0.00s prep time:0.27s\n",
            "Epoch 78:\n",
            "\ttrain loss:9372.5270 train ap:0.732664  train auc:0.773425  val loss:5.114329 val ap:0.746042  val auc:0.760549\n",
            "\ttotal time:2.20s sample time:0.01s prep time:0.24s\n",
            "Epoch 79:\n",
            "\ttrain loss:9471.8731 train ap:0.728231  train auc:0.769906  val loss:5.369113 val ap:0.705892  val auc:0.745240\n",
            "\ttotal time:1.73s sample time:0.01s prep time:0.20s\n",
            "Epoch 80:\n",
            "\ttrain loss:9469.9305 train ap:0.729229  train auc:0.770488  val loss:5.416875 val ap:0.712988  val auc:0.740007\n",
            "\ttotal time:1.73s sample time:0.01s prep time:0.19s\n",
            "Epoch 81:\n",
            "\ttrain loss:9477.4283 train ap:0.728377  train auc:0.769372  val loss:5.865171 val ap:0.665081  val auc:0.701409\n",
            "\ttotal time:1.76s sample time:0.01s prep time:0.19s\n",
            "Epoch 82:\n",
            "\ttrain loss:9472.0700 train ap:0.725911  train auc:0.767687  val loss:5.679492 val ap:0.701721  val auc:0.727593\n",
            "\ttotal time:1.78s sample time:0.00s prep time:0.20s\n",
            "Epoch 83:\n",
            "\ttrain loss:9311.8998 train ap:0.739896  train auc:0.778641  val loss:5.566525 val ap:0.726627  val auc:0.754446\n",
            "\ttotal time:1.75s sample time:0.00s prep time:0.20s\n",
            "Epoch 84:\n",
            "\ttrain loss:9357.7844 train ap:0.732999  train auc:0.774196  val loss:5.794227 val ap:0.688137  val auc:0.730348\n",
            "\ttotal time:2.24s sample time:0.00s prep time:0.25s\n",
            "Epoch 85:\n",
            "\ttrain loss:9365.1434 train ap:0.733398  train auc:0.774606  val loss:5.579979 val ap:0.742059  val auc:0.762269\n",
            "\ttotal time:2.28s sample time:0.01s prep time:0.27s\n",
            "Epoch 86:\n",
            "\ttrain loss:9515.1860 train ap:0.723777  train auc:0.767698  val loss:5.340175 val ap:0.732449  val auc:0.745121\n",
            "\ttotal time:1.74s sample time:0.01s prep time:0.19s\n",
            "Epoch 87:\n",
            "\ttrain loss:9374.8152 train ap:0.737826  train auc:0.774807  val loss:5.938344 val ap:0.685711  val auc:0.715735\n",
            "\ttotal time:1.75s sample time:0.01s prep time:0.19s\n",
            "Epoch 88:\n",
            "\ttrain loss:9350.5940 train ap:0.732804  train auc:0.773464  val loss:5.545855 val ap:0.733426  val auc:0.758711\n",
            "\ttotal time:1.73s sample time:0.01s prep time:0.19s\n",
            "Epoch 89:\n",
            "\ttrain loss:9195.1185 train ap:0.742776  train auc:0.783560  val loss:5.988693 val ap:0.734749  val auc:0.754532\n",
            "\ttotal time:1.76s sample time:0.00s prep time:0.19s\n",
            "Loading model at epoch 37...\n",
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/train.py:290: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path_saver))\n",
            "{'test_R@1': 0.27559054, 'test_R@5': 0.5826772, 'test_R@10': 0.77165353, 'test_mrr': 0.3212065520849026}\n",
            "\taverage test precision:0.734749  test AUC:0.754532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positive lead/lag"
      ],
      "metadata": {
        "id": "01Q9vIzyk4OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python gen_graph.py --data leadlag"
      ],
      "metadata": {
        "id": "lr906EcnDBSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data positive --config ./config/TGAT.yml --eval_neg_samples 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ard8QxRTk5zn",
        "outputId": "5dd1534d-2b53-4d4f-8879-0dadf5876ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/utils.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  node_feats = torch.load('DATA/{}/node_features.pt'.format(d))\n",
            "Epoch 0:\n",
            "\ttrain loss:3336.5164 train ap:0.574402  train auc:0.575152  val loss:2.619398 val ap:0.661707  val auc:0.677083\n",
            "\ttotal time:1.69s sample time:0.00s prep time:0.21s\n",
            "Epoch 1:\n",
            "\ttrain loss:3204.7812 train ap:0.634938  train auc:0.663218  val loss:2.595548 val ap:0.645815  val auc:0.668949\n",
            "\ttotal time:0.67s sample time:0.00s prep time:0.06s\n",
            "Epoch 2:\n",
            "\ttrain loss:3128.5319 train ap:0.656175  train auc:0.688539  val loss:2.538484 val ap:0.659765  val auc:0.682863\n",
            "\ttotal time:0.62s sample time:0.00s prep time:0.05s\n",
            "Epoch 3:\n",
            "\ttrain loss:3059.5942 train ap:0.670172  train auc:0.701824  val loss:2.456458 val ap:0.677945  val auc:0.710308\n",
            "\ttotal time:0.45s sample time:0.00s prep time:0.04s\n",
            "Epoch 4:\n",
            "\ttrain loss:3076.9841 train ap:0.657230  train auc:0.692882  val loss:2.484517 val ap:0.655571  val auc:0.710497\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.04s\n",
            "Epoch 5:\n",
            "\ttrain loss:2991.5429 train ap:0.685236  train auc:0.722981  val loss:2.561524 val ap:0.663274  val auc:0.688201\n",
            "\ttotal time:0.42s sample time:0.00s prep time:0.04s\n",
            "Epoch 6:\n",
            "\ttrain loss:2988.3631 train ap:0.680198  train auc:0.711659  val loss:2.519921 val ap:0.688817  val auc:0.687593\n",
            "\ttotal time:0.39s sample time:0.00s prep time:0.04s\n",
            "Epoch 7:\n",
            "\ttrain loss:3032.8151 train ap:0.656418  train auc:0.698998  val loss:2.428263 val ap:0.654204  val auc:0.702355\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.04s\n",
            "Epoch 8:\n",
            "\ttrain loss:2983.3057 train ap:0.676395  train auc:0.716243  val loss:2.505795 val ap:0.662336  val auc:0.690609\n",
            "\ttotal time:0.39s sample time:0.00s prep time:0.04s\n",
            "Epoch 9:\n",
            "\ttrain loss:2936.9361 train ap:0.694013  train auc:0.733108  val loss:2.443361 val ap:0.681903  val auc:0.731722\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.04s\n",
            "Epoch 10:\n",
            "\ttrain loss:2954.5443 train ap:0.678215  train auc:0.719451  val loss:2.402339 val ap:0.687749  val auc:0.730825\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.04s\n",
            "Epoch 11:\n",
            "\ttrain loss:2963.7275 train ap:0.678294  train auc:0.722344  val loss:2.518717 val ap:0.648939  val auc:0.691879\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.04s\n",
            "Epoch 12:\n",
            "\ttrain loss:2914.0579 train ap:0.687181  train auc:0.730500  val loss:2.544918 val ap:0.635134  val auc:0.678038\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.04s\n",
            "Epoch 13:\n",
            "\ttrain loss:2875.3268 train ap:0.701059  train auc:0.733606  val loss:2.489377 val ap:0.668740  val auc:0.704566\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.04s\n",
            "Epoch 14:\n",
            "\ttrain loss:2848.0123 train ap:0.709508  train auc:0.752871  val loss:2.506437 val ap:0.672557  val auc:0.701386\n",
            "\ttotal time:0.42s sample time:0.00s prep time:0.04s\n",
            "Epoch 15:\n",
            "\ttrain loss:2852.8831 train ap:0.697448  train auc:0.742636  val loss:2.591022 val ap:0.621152  val auc:0.665725\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.04s\n",
            "Epoch 16:\n",
            "\ttrain loss:2821.4355 train ap:0.698536  train auc:0.748795  val loss:2.466620 val ap:0.636269  val auc:0.710259\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.06s\n",
            "Epoch 17:\n",
            "\ttrain loss:2840.4987 train ap:0.708404  train auc:0.743830  val loss:2.406999 val ap:0.640717  val auc:0.712956\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.05s\n",
            "Epoch 18:\n",
            "\ttrain loss:2835.1298 train ap:0.696451  train auc:0.745509  val loss:2.542109 val ap:0.618049  val auc:0.676080\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.05s\n",
            "Epoch 19:\n",
            "\ttrain loss:2837.6240 train ap:0.694768  train auc:0.739038  val loss:2.484019 val ap:0.664184  val auc:0.718885\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.05s\n",
            "Epoch 20:\n",
            "\ttrain loss:2791.9340 train ap:0.699725  train auc:0.750750  val loss:2.522326 val ap:0.636424  val auc:0.681817\n",
            "\ttotal time:0.75s sample time:0.00s prep time:0.06s\n",
            "Epoch 21:\n",
            "\ttrain loss:2759.2134 train ap:0.720692  train auc:0.764619  val loss:2.455588 val ap:0.623102  val auc:0.697379\n",
            "\ttotal time:0.88s sample time:0.00s prep time:0.07s\n",
            "Epoch 22:\n",
            "\ttrain loss:2774.0196 train ap:0.709189  train auc:0.755191  val loss:2.469666 val ap:0.646789  val auc:0.697818\n",
            "\ttotal time:0.63s sample time:0.00s prep time:0.05s\n",
            "Epoch 23:\n",
            "\ttrain loss:2748.6918 train ap:0.714463  train auc:0.761236  val loss:2.388856 val ap:0.719790  val auc:0.759505\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.05s\n",
            "Epoch 24:\n",
            "\ttrain loss:2652.5284 train ap:0.738439  train auc:0.780541  val loss:2.566006 val ap:0.679566  val auc:0.682193\n",
            "\ttotal time:0.63s sample time:0.00s prep time:0.06s\n",
            "Epoch 25:\n",
            "\ttrain loss:2731.2169 train ap:0.720830  train auc:0.769184  val loss:2.516695 val ap:0.678508  val auc:0.711295\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.05s\n",
            "Epoch 26:\n",
            "\ttrain loss:2671.5165 train ap:0.734497  train auc:0.777519  val loss:2.704476 val ap:0.619804  val auc:0.664208\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.05s\n",
            "Epoch 27:\n",
            "\ttrain loss:2684.4716 train ap:0.720475  train auc:0.774317  val loss:2.656527 val ap:0.653627  val auc:0.672190\n",
            "\ttotal time:0.42s sample time:0.00s prep time:0.04s\n",
            "Epoch 28:\n",
            "\ttrain loss:2700.0896 train ap:0.725535  train auc:0.773898  val loss:2.556752 val ap:0.677738  val auc:0.693950\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.04s\n",
            "Epoch 29:\n",
            "\ttrain loss:2727.6203 train ap:0.726843  train auc:0.773206  val loss:2.585186 val ap:0.642464  val auc:0.692384\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.04s\n",
            "Epoch 30:\n",
            "\ttrain loss:2697.9785 train ap:0.733062  train auc:0.775124  val loss:2.585200 val ap:0.665395  val auc:0.703750\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.04s\n",
            "Epoch 31:\n",
            "\ttrain loss:2654.5412 train ap:0.744552  train auc:0.783586  val loss:2.600638 val ap:0.634678  val auc:0.680001\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.04s\n",
            "Epoch 32:\n",
            "\ttrain loss:2653.2021 train ap:0.737786  train auc:0.784793  val loss:2.556899 val ap:0.660844  val auc:0.699194\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.04s\n",
            "Epoch 33:\n",
            "\ttrain loss:2671.6524 train ap:0.726066  train auc:0.777703  val loss:2.823172 val ap:0.637369  val auc:0.686451\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.04s\n",
            "Epoch 34:\n",
            "\ttrain loss:2754.2210 train ap:0.715249  train auc:0.760723  val loss:2.751603 val ap:0.601236  val auc:0.629917\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.04s\n",
            "Epoch 35:\n",
            "\ttrain loss:2709.1886 train ap:0.748115  train auc:0.784099  val loss:2.767075 val ap:0.623660  val auc:0.661808\n",
            "\ttotal time:0.42s sample time:0.00s prep time:0.04s\n",
            "Epoch 36:\n",
            "\ttrain loss:2629.8370 train ap:0.742151  train auc:0.784643  val loss:2.709373 val ap:0.630259  val auc:0.687125\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 37:\n",
            "\ttrain loss:2580.0350 train ap:0.749082  train auc:0.796970  val loss:2.617076 val ap:0.629628  val auc:0.684455\n",
            "\ttotal time:0.46s sample time:0.00s prep time:0.04s\n",
            "Epoch 38:\n",
            "\ttrain loss:2625.0434 train ap:0.744483  train auc:0.790178  val loss:2.627805 val ap:0.650027  val auc:0.709067\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 39:\n",
            "\ttrain loss:2554.3697 train ap:0.759279  train auc:0.801348  val loss:2.610195 val ap:0.671507  val auc:0.701650\n",
            "\ttotal time:0.42s sample time:0.00s prep time:0.04s\n",
            "Epoch 40:\n",
            "\ttrain loss:2584.7775 train ap:0.748446  train auc:0.793085  val loss:2.612205 val ap:0.621794  val auc:0.676187\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 41:\n",
            "\ttrain loss:2755.4298 train ap:0.724148  train auc:0.774609  val loss:2.806237 val ap:0.611086  val auc:0.668735\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.04s\n",
            "Epoch 42:\n",
            "\ttrain loss:2613.0250 train ap:0.742503  train auc:0.788257  val loss:2.567307 val ap:0.658739  val auc:0.695045\n",
            "\ttotal time:0.44s sample time:0.00s prep time:0.04s\n",
            "Epoch 43:\n",
            "\ttrain loss:2531.3568 train ap:0.764081  train auc:0.805418  val loss:2.599242 val ap:0.653289  val auc:0.692656\n",
            "\ttotal time:0.42s sample time:0.00s prep time:0.04s\n",
            "Epoch 44:\n",
            "\ttrain loss:2590.3279 train ap:0.757398  train auc:0.796625  val loss:2.763369 val ap:0.680152  val auc:0.712365\n",
            "\ttotal time:0.44s sample time:0.00s prep time:0.04s\n",
            "Epoch 45:\n",
            "\ttrain loss:2533.9145 train ap:0.763029  train auc:0.799434  val loss:2.612453 val ap:0.654720  val auc:0.684598\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 46:\n",
            "\ttrain loss:2547.0167 train ap:0.750940  train auc:0.794128  val loss:2.718019 val ap:0.608461  val auc:0.653558\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 47:\n",
            "\ttrain loss:2521.7196 train ap:0.761114  train auc:0.803533  val loss:3.077296 val ap:0.596051  val auc:0.649710\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 48:\n",
            "\ttrain loss:2460.7753 train ap:0.771743  train auc:0.810875  val loss:2.878761 val ap:0.607736  val auc:0.654782\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.05s\n",
            "Epoch 49:\n",
            "\ttrain loss:2472.4486 train ap:0.775316  train auc:0.815205  val loss:2.811521 val ap:0.666461  val auc:0.681209\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.05s\n",
            "Epoch 50:\n",
            "\ttrain loss:2488.9414 train ap:0.760738  train auc:0.806250  val loss:3.007096 val ap:0.660230  val auc:0.691748\n",
            "\ttotal time:0.61s sample time:0.00s prep time:0.05s\n",
            "Epoch 51:\n",
            "\ttrain loss:2450.0679 train ap:0.770323  train auc:0.813923  val loss:3.064979 val ap:0.619685  val auc:0.651719\n",
            "\ttotal time:0.57s sample time:0.00s prep time:0.05s\n",
            "Epoch 52:\n",
            "\ttrain loss:2499.1247 train ap:0.764965  train auc:0.808931  val loss:2.955045 val ap:0.610099  val auc:0.657605\n",
            "\ttotal time:0.64s sample time:0.00s prep time:0.05s\n",
            "Epoch 53:\n",
            "\ttrain loss:2467.2336 train ap:0.766018  train auc:0.809312  val loss:3.602846 val ap:0.621957  val auc:0.638480\n",
            "\ttotal time:0.62s sample time:0.00s prep time:0.05s\n",
            "Epoch 54:\n",
            "\ttrain loss:2460.2677 train ap:0.770576  train auc:0.814636  val loss:2.782452 val ap:0.697893  val auc:0.712340\n",
            "\ttotal time:0.52s sample time:0.00s prep time:0.04s\n",
            "Epoch 55:\n",
            "\ttrain loss:2503.3052 train ap:0.771994  train auc:0.815765  val loss:2.992706 val ap:0.580748  val auc:0.650705\n",
            "\ttotal time:0.44s sample time:0.00s prep time:0.04s\n",
            "Epoch 56:\n",
            "\ttrain loss:2488.5080 train ap:0.760375  train auc:0.806155  val loss:3.050318 val ap:0.637243  val auc:0.709585\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 57:\n",
            "\ttrain loss:2442.4747 train ap:0.777096  train auc:0.822639  val loss:2.838535 val ap:0.665015  val auc:0.679129\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 58:\n",
            "\ttrain loss:2564.0201 train ap:0.765822  train auc:0.807818  val loss:2.924446 val ap:0.637450  val auc:0.685516\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 59:\n",
            "\ttrain loss:2482.3711 train ap:0.767520  train auc:0.811586  val loss:3.105488 val ap:0.625164  val auc:0.677168\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 60:\n",
            "\ttrain loss:2450.9712 train ap:0.763489  train auc:0.811946  val loss:2.946867 val ap:0.647051  val auc:0.698404\n",
            "\ttotal time:0.45s sample time:0.00s prep time:0.04s\n",
            "Epoch 61:\n",
            "\ttrain loss:2352.2989 train ap:0.800966  train auc:0.832912  val loss:3.093850 val ap:0.644893  val auc:0.685971\n",
            "\ttotal time:0.45s sample time:0.00s prep time:0.04s\n",
            "Epoch 62:\n",
            "\ttrain loss:2391.5976 train ap:0.781815  train auc:0.822957  val loss:2.929660 val ap:0.659503  val auc:0.695042\n",
            "\ttotal time:0.44s sample time:0.00s prep time:0.04s\n",
            "Epoch 63:\n",
            "\ttrain loss:2420.9907 train ap:0.765293  train auc:0.817716  val loss:3.001400 val ap:0.620697  val auc:0.671046\n",
            "\ttotal time:0.45s sample time:0.00s prep time:0.04s\n",
            "Epoch 64:\n",
            "\ttrain loss:2426.7845 train ap:0.785557  train auc:0.822740  val loss:3.102862 val ap:0.645055  val auc:0.686619\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 65:\n",
            "\ttrain loss:2382.1103 train ap:0.786352  train auc:0.825921  val loss:3.235222 val ap:0.627376  val auc:0.653850\n",
            "\ttotal time:0.42s sample time:0.00s prep time:0.04s\n",
            "Epoch 66:\n",
            "\ttrain loss:2381.8515 train ap:0.781135  train auc:0.823529  val loss:2.850656 val ap:0.656437  val auc:0.711896\n",
            "\ttotal time:0.45s sample time:0.00s prep time:0.04s\n",
            "Epoch 67:\n",
            "\ttrain loss:2369.1086 train ap:0.785810  train auc:0.827467  val loss:2.977519 val ap:0.617381  val auc:0.680908\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 68:\n",
            "\ttrain loss:2412.3973 train ap:0.759306  train auc:0.811183  val loss:3.173720 val ap:0.647590  val auc:0.705727\n",
            "\ttotal time:0.44s sample time:0.00s prep time:0.04s\n",
            "Epoch 69:\n",
            "\ttrain loss:2318.1366 train ap:0.790295  train auc:0.836213  val loss:3.248860 val ap:0.605931  val auc:0.658610\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 70:\n",
            "\ttrain loss:2379.2816 train ap:0.783405  train auc:0.825344  val loss:3.243454 val ap:0.614391  val auc:0.671978\n",
            "\ttotal time:0.44s sample time:0.00s prep time:0.04s\n",
            "Epoch 71:\n",
            "\ttrain loss:2263.5353 train ap:0.799163  train auc:0.841595  val loss:3.425434 val ap:0.608512  val auc:0.660191\n",
            "\ttotal time:0.45s sample time:0.00s prep time:0.04s\n",
            "Epoch 72:\n",
            "\ttrain loss:2302.0464 train ap:0.791352  train auc:0.833306  val loss:3.313569 val ap:0.623427  val auc:0.650078\n",
            "\ttotal time:0.44s sample time:0.00s prep time:0.04s\n",
            "Epoch 73:\n",
            "\ttrain loss:2345.8091 train ap:0.783487  train auc:0.825535  val loss:3.118967 val ap:0.640995  val auc:0.687450\n",
            "\ttotal time:0.45s sample time:0.00s prep time:0.04s\n",
            "Epoch 74:\n",
            "\ttrain loss:2285.2315 train ap:0.798064  train auc:0.839241  val loss:3.478048 val ap:0.664398  val auc:0.682567\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Epoch 75:\n",
            "\ttrain loss:2335.6528 train ap:0.779154  train auc:0.826123  val loss:3.627867 val ap:0.606988  val auc:0.641477\n",
            "\ttotal time:0.53s sample time:0.00s prep time:0.05s\n",
            "Epoch 76:\n",
            "\ttrain loss:2350.1772 train ap:0.784135  train auc:0.830026  val loss:3.244701 val ap:0.627493  val auc:0.685806\n",
            "\ttotal time:0.54s sample time:0.00s prep time:0.05s\n",
            "Epoch 77:\n",
            "\ttrain loss:2369.7773 train ap:0.796847  train auc:0.830920  val loss:3.243532 val ap:0.598817  val auc:0.666144\n",
            "\ttotal time:0.59s sample time:0.00s prep time:0.05s\n",
            "Epoch 78:\n",
            "\ttrain loss:2309.1521 train ap:0.793867  train auc:0.833472  val loss:3.514430 val ap:0.677708  val auc:0.694799\n",
            "\ttotal time:0.58s sample time:0.00s prep time:0.05s\n",
            "Epoch 79:\n",
            "\ttrain loss:2294.3418 train ap:0.797459  train auc:0.837229  val loss:3.752750 val ap:0.592975  val auc:0.619132\n",
            "\ttotal time:0.62s sample time:0.00s prep time:0.06s\n",
            "Epoch 80:\n",
            "\ttrain loss:2306.6595 train ap:0.784924  train auc:0.833146  val loss:3.381836 val ap:0.663461  val auc:0.694790\n",
            "\ttotal time:0.63s sample time:0.00s prep time:0.05s\n",
            "Epoch 81:\n",
            "\ttrain loss:2305.0007 train ap:0.791915  train auc:0.835763  val loss:3.722028 val ap:0.613173  val auc:0.651520\n",
            "\ttotal time:0.50s sample time:0.00s prep time:0.04s\n",
            "Epoch 82:\n",
            "\ttrain loss:2305.1113 train ap:0.796583  train auc:0.837588  val loss:3.453307 val ap:0.580243  val auc:0.626556\n",
            "\ttotal time:0.46s sample time:0.00s prep time:0.04s\n",
            "Epoch 83:\n",
            "\ttrain loss:2320.6191 train ap:0.796577  train auc:0.839978  val loss:3.327403 val ap:0.647634  val auc:0.692004\n",
            "\ttotal time:0.44s sample time:0.00s prep time:0.04s\n",
            "Epoch 84:\n",
            "\ttrain loss:2272.0813 train ap:0.790723  train auc:0.836056  val loss:3.442841 val ap:0.663960  val auc:0.685721\n",
            "\ttotal time:0.47s sample time:0.00s prep time:0.04s\n",
            "Epoch 85:\n",
            "\ttrain loss:2192.0543 train ap:0.805641  train auc:0.847657  val loss:3.052820 val ap:0.645147  val auc:0.660971\n",
            "\ttotal time:0.45s sample time:0.00s prep time:0.04s\n",
            "Epoch 86:\n",
            "\ttrain loss:2160.0417 train ap:0.823401  train auc:0.860048  val loss:3.919715 val ap:0.612371  val auc:0.680984\n",
            "\ttotal time:0.44s sample time:0.00s prep time:0.04s\n",
            "Epoch 87:\n",
            "\ttrain loss:2258.9026 train ap:0.804812  train auc:0.841780  val loss:3.336707 val ap:0.651884  val auc:0.686491\n",
            "\ttotal time:0.44s sample time:0.00s prep time:0.04s\n",
            "Epoch 88:\n",
            "\ttrain loss:2313.0444 train ap:0.797213  train auc:0.834554  val loss:3.292199 val ap:0.679416  val auc:0.717740\n",
            "\ttotal time:0.46s sample time:0.00s prep time:0.04s\n",
            "Epoch 89:\n",
            "\ttrain loss:2262.0474 train ap:0.804131  train auc:0.843174  val loss:3.272488 val ap:0.709213  val auc:0.723258\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.04s\n",
            "Loading model at epoch 23...\n",
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/train.py:290: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path_saver))\n",
            "{'test_R@1': 0.13333334, 'test_R@5': 0.3809524, 'test_R@10': 0.5714286, 'test_mrr': 0.1983056132384321}\n",
            "\taverage test precision:0.709213  test AUC:0.723258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APAN"
      ],
      "metadata": {
        "id": "0UVmSeWTTo_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data leadlag --config ./config/APAN.yml --eval_neg_samples 30"
      ],
      "metadata": {
        "id": "QccKWJOZY8QS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f41a81-6e14-4822-f910-d74380981ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/utils.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  node_feats = torch.load('DATA/{}/node_features.pt'.format(d))\n",
            "Epoch 0:\n",
            "\ttrain loss:11841.5461 train ap:0.557908  train auc:0.574973  val loss:5.433209 val ap:0.608786  val auc:0.653262\n",
            "\ttotal time:1.35s sample time:0.00s prep time:0.48s\n",
            "Epoch 1:\n",
            "\ttrain loss:11758.5678 train ap:0.571014  train auc:0.596263  val loss:5.371663 val ap:0.637596  val auc:0.652863\n",
            "\ttotal time:1.39s sample time:0.00s prep time:0.58s\n",
            "Epoch 2:\n",
            "\ttrain loss:11641.4152 train ap:0.568577  train auc:0.596001  val loss:5.302316 val ap:0.642596  val auc:0.657570\n",
            "\ttotal time:1.46s sample time:0.00s prep time:0.61s\n",
            "Epoch 3:\n",
            "\ttrain loss:11501.9002 train ap:0.573989  train auc:0.604627  val loss:5.332990 val ap:0.630092  val auc:0.646567\n",
            "\ttotal time:1.16s sample time:0.00s prep time:0.47s\n",
            "Epoch 4:\n",
            "\ttrain loss:11383.9624 train ap:0.578303  train auc:0.612103  val loss:5.197940 val ap:0.671164  val auc:0.702486\n",
            "\ttotal time:0.97s sample time:0.00s prep time:0.40s\n",
            "Epoch 5:\n",
            "\ttrain loss:11314.5096 train ap:0.586010  train auc:0.619035  val loss:5.253998 val ap:0.607656  val auc:0.632605\n",
            "\ttotal time:0.97s sample time:0.00s prep time:0.39s\n",
            "Epoch 6:\n",
            "\ttrain loss:11173.8664 train ap:0.597571  train auc:0.629509  val loss:5.512519 val ap:0.520599  val auc:0.542096\n",
            "\ttotal time:0.98s sample time:0.00s prep time:0.40s\n",
            "Epoch 7:\n",
            "\ttrain loss:11247.8187 train ap:0.582961  train auc:0.617700  val loss:5.172104 val ap:0.692543  val auc:0.692600\n",
            "\ttotal time:1.00s sample time:0.00s prep time:0.40s\n",
            "Epoch 8:\n",
            "\ttrain loss:11244.0633 train ap:0.593542  train auc:0.624752  val loss:5.012531 val ap:0.604586  val auc:0.676991\n",
            "\ttotal time:0.97s sample time:0.00s prep time:0.39s\n",
            "Epoch 9:\n",
            "\ttrain loss:11154.6147 train ap:0.588975  train auc:0.625309  val loss:4.965231 val ap:0.717235  val auc:0.724528\n",
            "\ttotal time:0.98s sample time:0.00s prep time:0.40s\n",
            "Epoch 10:\n",
            "\ttrain loss:11159.2117 train ap:0.587849  train auc:0.621816  val loss:5.143387 val ap:0.588245  val auc:0.631128\n",
            "\ttotal time:1.02s sample time:0.00s prep time:0.42s\n",
            "Epoch 11:\n",
            "\ttrain loss:11166.2423 train ap:0.590445  train auc:0.624453  val loss:4.993057 val ap:0.668061  val auc:0.711980\n",
            "\ttotal time:1.01s sample time:0.00s prep time:0.41s\n",
            "Epoch 12:\n",
            "\ttrain loss:11179.4890 train ap:0.589386  train auc:0.623484  val loss:5.065001 val ap:0.659970  val auc:0.674955\n",
            "\ttotal time:1.03s sample time:0.00s prep time:0.42s\n",
            "Epoch 13:\n",
            "\ttrain loss:11115.5184 train ap:0.598208  train auc:0.633099  val loss:4.893975 val ap:0.648691  val auc:0.699551\n",
            "\ttotal time:1.41s sample time:0.00s prep time:0.60s\n",
            "Epoch 14:\n",
            "\ttrain loss:11144.2871 train ap:0.591203  train auc:0.629207  val loss:5.011888 val ap:0.654144  val auc:0.677131\n",
            "\ttotal time:1.43s sample time:0.00s prep time:0.60s\n",
            "Epoch 15:\n",
            "\ttrain loss:11132.2520 train ap:0.596384  train auc:0.633257  val loss:4.849434 val ap:0.630877  val auc:0.684971\n",
            "\ttotal time:1.13s sample time:0.00s prep time:0.46s\n",
            "Epoch 16:\n",
            "\ttrain loss:11071.7182 train ap:0.601934  train auc:0.639331  val loss:4.962140 val ap:0.687297  val auc:0.727654\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 17:\n",
            "\ttrain loss:11147.9801 train ap:0.591415  train auc:0.623524  val loss:5.170772 val ap:0.624522  val auc:0.661230\n",
            "\ttotal time:1.00s sample time:0.00s prep time:0.40s\n",
            "Epoch 18:\n",
            "\ttrain loss:11097.5516 train ap:0.595512  train auc:0.629087  val loss:5.118590 val ap:0.604864  val auc:0.647493\n",
            "\ttotal time:1.00s sample time:0.00s prep time:0.40s\n",
            "Epoch 19:\n",
            "\ttrain loss:11040.7321 train ap:0.599087  train auc:0.637333  val loss:5.077017 val ap:0.668959  val auc:0.707129\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 20:\n",
            "\ttrain loss:11005.5223 train ap:0.598145  train auc:0.637702  val loss:5.296257 val ap:0.582894  val auc:0.623378\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 21:\n",
            "\ttrain loss:11064.7992 train ap:0.594674  train auc:0.632554  val loss:5.095859 val ap:0.615777  val auc:0.666045\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 22:\n",
            "\ttrain loss:11017.8204 train ap:0.594043  train auc:0.629636  val loss:5.417699 val ap:0.607504  val auc:0.653295\n",
            "\ttotal time:0.97s sample time:0.00s prep time:0.39s\n",
            "Epoch 23:\n",
            "\ttrain loss:10994.1674 train ap:0.593635  train auc:0.632642  val loss:5.231084 val ap:0.645606  val auc:0.694420\n",
            "\ttotal time:0.97s sample time:0.00s prep time:0.40s\n",
            "Epoch 24:\n",
            "\ttrain loss:11016.1220 train ap:0.596625  train auc:0.636111  val loss:5.070290 val ap:0.652412  val auc:0.681316\n",
            "\ttotal time:1.04s sample time:0.00s prep time:0.43s\n",
            "Epoch 25:\n",
            "\ttrain loss:10957.2027 train ap:0.605293  train auc:0.642536  val loss:4.920533 val ap:0.670435  val auc:0.692619\n",
            "\ttotal time:1.39s sample time:0.00s prep time:0.59s\n",
            "Epoch 26:\n",
            "\ttrain loss:10896.4869 train ap:0.612960  train auc:0.647966  val loss:5.234612 val ap:0.637435  val auc:0.664276\n",
            "\ttotal time:1.44s sample time:0.00s prep time:0.59s\n",
            "Epoch 27:\n",
            "\ttrain loss:10933.6910 train ap:0.601600  train auc:0.640476  val loss:5.236583 val ap:0.663440  val auc:0.701950\n",
            "\ttotal time:1.18s sample time:0.00s prep time:0.48s\n",
            "Epoch 28:\n",
            "\ttrain loss:10907.2495 train ap:0.606785  train auc:0.647621  val loss:5.274002 val ap:0.590966  val auc:0.628594\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 29:\n",
            "\ttrain loss:11022.4992 train ap:0.603084  train auc:0.642112  val loss:4.848277 val ap:0.691198  val auc:0.736995\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 30:\n",
            "\ttrain loss:10984.1427 train ap:0.603187  train auc:0.640424  val loss:5.316131 val ap:0.642482  val auc:0.671318\n",
            "\ttotal time:1.00s sample time:0.00s prep time:0.41s\n",
            "Epoch 31:\n",
            "\ttrain loss:10959.8947 train ap:0.598022  train auc:0.638237  val loss:5.070630 val ap:0.688822  val auc:0.726093\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 32:\n",
            "\ttrain loss:10895.1181 train ap:0.608197  train auc:0.649720  val loss:5.266611 val ap:0.612397  val auc:0.670458\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 33:\n",
            "\ttrain loss:11015.0643 train ap:0.603686  train auc:0.644293  val loss:4.997348 val ap:0.638718  val auc:0.668429\n",
            "\ttotal time:0.97s sample time:0.00s prep time:0.39s\n",
            "Epoch 34:\n",
            "\ttrain loss:10980.5108 train ap:0.609346  train auc:0.647690  val loss:5.131795 val ap:0.601911  val auc:0.640487\n",
            "\ttotal time:1.00s sample time:0.00s prep time:0.40s\n",
            "Epoch 35:\n",
            "\ttrain loss:10990.7991 train ap:0.600442  train auc:0.640601  val loss:4.917406 val ap:0.690738  val auc:0.718265\n",
            "\ttotal time:0.97s sample time:0.00s prep time:0.39s\n",
            "Epoch 36:\n",
            "\ttrain loss:10931.4163 train ap:0.608298  train auc:0.645990  val loss:5.074759 val ap:0.660804  val auc:0.700366\n",
            "\ttotal time:0.97s sample time:0.00s prep time:0.40s\n",
            "Epoch 37:\n",
            "\ttrain loss:10889.0198 train ap:0.614331  train auc:0.654105  val loss:5.009260 val ap:0.643287  val auc:0.690508\n",
            "\ttotal time:1.44s sample time:0.00s prep time:0.61s\n",
            "Epoch 38:\n",
            "\ttrain loss:10900.3979 train ap:0.619613  train auc:0.654220  val loss:5.111045 val ap:0.702533  val auc:0.718190\n",
            "\ttotal time:1.44s sample time:0.00s prep time:0.60s\n",
            "Epoch 39:\n",
            "\ttrain loss:10940.2204 train ap:0.603621  train auc:0.644952  val loss:5.169344 val ap:0.581612  val auc:0.631390\n",
            "\ttotal time:1.21s sample time:0.00s prep time:0.50s\n",
            "Epoch 40:\n",
            "\ttrain loss:10939.9538 train ap:0.608111  train auc:0.647982  val loss:5.022767 val ap:0.582726  val auc:0.646926\n",
            "\ttotal time:1.02s sample time:0.00s prep time:0.41s\n",
            "Epoch 41:\n",
            "\ttrain loss:10810.5296 train ap:0.614481  train auc:0.656204  val loss:5.132990 val ap:0.601414  val auc:0.645086\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 42:\n",
            "\ttrain loss:10920.4116 train ap:0.613630  train auc:0.653250  val loss:5.478621 val ap:0.595595  val auc:0.623393\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 43:\n",
            "\ttrain loss:10895.3281 train ap:0.615843  train auc:0.653172  val loss:5.006715 val ap:0.655888  val auc:0.687524\n",
            "\ttotal time:0.96s sample time:0.00s prep time:0.39s\n",
            "Epoch 44:\n",
            "\ttrain loss:10904.2399 train ap:0.612724  train auc:0.649765  val loss:5.175809 val ap:0.695587  val auc:0.722855\n",
            "\ttotal time:0.97s sample time:0.00s prep time:0.39s\n",
            "Epoch 45:\n",
            "\ttrain loss:10894.4481 train ap:0.610315  train auc:0.652178  val loss:5.197444 val ap:0.621890  val auc:0.677972\n",
            "\ttotal time:1.01s sample time:0.00s prep time:0.41s\n",
            "Epoch 46:\n",
            "\ttrain loss:10847.3597 train ap:0.618424  train auc:0.655087  val loss:5.019480 val ap:0.632651  val auc:0.682030\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 47:\n",
            "\ttrain loss:10862.1032 train ap:0.609875  train auc:0.652392  val loss:5.671265 val ap:0.619740  val auc:0.646668\n",
            "\ttotal time:1.00s sample time:0.00s prep time:0.40s\n",
            "Epoch 48:\n",
            "\ttrain loss:10799.1743 train ap:0.621282  train auc:0.661814  val loss:5.286826 val ap:0.614439  val auc:0.641267\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 49:\n",
            "\ttrain loss:10831.5640 train ap:0.617145  train auc:0.658015  val loss:5.060555 val ap:0.683202  val auc:0.715180\n",
            "\ttotal time:1.48s sample time:0.00s prep time:0.62s\n",
            "Epoch 50:\n",
            "\ttrain loss:10784.6618 train ap:0.625391  train auc:0.667363  val loss:5.107460 val ap:0.604972  val auc:0.675057\n",
            "\ttotal time:1.47s sample time:0.00s prep time:0.61s\n",
            "Epoch 51:\n",
            "\ttrain loss:10798.0020 train ap:0.614249  train auc:0.657467  val loss:5.378348 val ap:0.568410  val auc:0.626372\n",
            "\ttotal time:1.31s sample time:0.00s prep time:0.54s\n",
            "Epoch 52:\n",
            "\ttrain loss:10792.3382 train ap:0.622457  train auc:0.659243  val loss:5.430470 val ap:0.589680  val auc:0.626773\n",
            "\ttotal time:1.02s sample time:0.00s prep time:0.42s\n",
            "Epoch 53:\n",
            "\ttrain loss:10882.1867 train ap:0.620382  train auc:0.659747  val loss:5.310337 val ap:0.594316  val auc:0.628708\n",
            "\ttotal time:1.04s sample time:0.00s prep time:0.42s\n",
            "Epoch 54:\n",
            "\ttrain loss:10815.4995 train ap:0.611789  train auc:0.655840  val loss:4.933697 val ap:0.657908  val auc:0.703339\n",
            "\ttotal time:1.03s sample time:0.00s prep time:0.42s\n",
            "Epoch 55:\n",
            "\ttrain loss:10793.7074 train ap:0.617332  train auc:0.658499  val loss:5.659605 val ap:0.569556  val auc:0.594642\n",
            "\ttotal time:1.02s sample time:0.00s prep time:0.41s\n",
            "Epoch 56:\n",
            "\ttrain loss:10748.1607 train ap:0.626287  train auc:0.663943  val loss:5.107193 val ap:0.609864  val auc:0.674097\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 57:\n",
            "\ttrain loss:10847.6930 train ap:0.617556  train auc:0.656044  val loss:5.292440 val ap:0.612883  val auc:0.647770\n",
            "\ttotal time:1.05s sample time:0.00s prep time:0.42s\n",
            "Epoch 58:\n",
            "\ttrain loss:10807.5286 train ap:0.621644  train auc:0.659826  val loss:5.651361 val ap:0.634243  val auc:0.666068\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 59:\n",
            "\ttrain loss:10746.1297 train ap:0.621601  train auc:0.660902  val loss:5.535463 val ap:0.657351  val auc:0.683449\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 60:\n",
            "\ttrain loss:10804.8385 train ap:0.623947  train auc:0.661791  val loss:5.376414 val ap:0.627385  val auc:0.655911\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 61:\n",
            "\ttrain loss:10703.2427 train ap:0.628056  train auc:0.666203  val loss:5.033261 val ap:0.683191  val auc:0.733568\n",
            "\ttotal time:1.38s sample time:0.00s prep time:0.58s\n",
            "Epoch 62:\n",
            "\ttrain loss:10820.9048 train ap:0.625926  train auc:0.665329  val loss:4.927164 val ap:0.700895  val auc:0.731125\n",
            "\ttotal time:1.48s sample time:0.00s prep time:0.61s\n",
            "Epoch 63:\n",
            "\ttrain loss:10805.2583 train ap:0.616373  train auc:0.658137  val loss:5.574806 val ap:0.589250  val auc:0.608246\n",
            "\ttotal time:1.22s sample time:0.00s prep time:0.50s\n",
            "Epoch 64:\n",
            "\ttrain loss:10756.2830 train ap:0.625625  train auc:0.664800  val loss:4.965234 val ap:0.645637  val auc:0.701246\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 65:\n",
            "\ttrain loss:10771.3741 train ap:0.618839  train auc:0.657885  val loss:5.291018 val ap:0.669657  val auc:0.691824\n",
            "\ttotal time:1.00s sample time:0.00s prep time:0.41s\n",
            "Epoch 66:\n",
            "\ttrain loss:10715.2931 train ap:0.620222  train auc:0.662405  val loss:5.574073 val ap:0.641587  val auc:0.675906\n",
            "\ttotal time:1.00s sample time:0.00s prep time:0.41s\n",
            "Epoch 67:\n",
            "\ttrain loss:10762.0172 train ap:0.624934  train auc:0.667024  val loss:5.587523 val ap:0.695602  val auc:0.709621\n",
            "\ttotal time:1.00s sample time:0.00s prep time:0.40s\n",
            "Epoch 68:\n",
            "\ttrain loss:10842.6579 train ap:0.615569  train auc:0.656341  val loss:5.034950 val ap:0.723208  val auc:0.738523\n",
            "\ttotal time:1.00s sample time:0.00s prep time:0.41s\n",
            "Epoch 69:\n",
            "\ttrain loss:10775.0040 train ap:0.609494  train auc:0.652511  val loss:5.203460 val ap:0.626923  val auc:0.682961\n",
            "\ttotal time:1.01s sample time:0.00s prep time:0.41s\n",
            "Epoch 70:\n",
            "\ttrain loss:10713.2111 train ap:0.623494  train auc:0.663525  val loss:5.229399 val ap:0.659468  val auc:0.682068\n",
            "\ttotal time:0.98s sample time:0.00s prep time:0.40s\n",
            "Epoch 71:\n",
            "\ttrain loss:10666.9348 train ap:0.622399  train auc:0.664826  val loss:4.935238 val ap:0.674418  val auc:0.736930\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 72:\n",
            "\ttrain loss:10850.2637 train ap:0.614318  train auc:0.656724  val loss:5.116891 val ap:0.642968  val auc:0.687019\n",
            "\ttotal time:0.98s sample time:0.00s prep time:0.40s\n",
            "Epoch 73:\n",
            "\ttrain loss:10786.1266 train ap:0.623139  train auc:0.662374  val loss:5.187999 val ap:0.650233  val auc:0.692738\n",
            "\ttotal time:1.46s sample time:0.00s prep time:0.60s\n",
            "Epoch 74:\n",
            "\ttrain loss:10754.3805 train ap:0.620044  train auc:0.659535  val loss:5.275414 val ap:0.650237  val auc:0.666972\n",
            "\ttotal time:1.51s sample time:0.00s prep time:0.63s\n",
            "Epoch 75:\n",
            "\ttrain loss:10765.4843 train ap:0.621372  train auc:0.661993  val loss:5.275931 val ap:0.665044  val auc:0.684162\n",
            "\ttotal time:1.23s sample time:0.00s prep time:0.50s\n",
            "Epoch 76:\n",
            "\ttrain loss:10744.8444 train ap:0.624030  train auc:0.660091  val loss:5.722379 val ap:0.599344  val auc:0.649802\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Epoch 77:\n",
            "\ttrain loss:10690.2182 train ap:0.626115  train auc:0.667491  val loss:5.533295 val ap:0.643482  val auc:0.678410\n",
            "\ttotal time:0.97s sample time:0.00s prep time:0.39s\n",
            "Epoch 78:\n",
            "\ttrain loss:10652.6105 train ap:0.628213  train auc:0.673345  val loss:5.022858 val ap:0.665832  val auc:0.709685\n",
            "\ttotal time:0.96s sample time:0.00s prep time:0.39s\n",
            "Epoch 79:\n",
            "\ttrain loss:10742.8748 train ap:0.623691  train auc:0.660739  val loss:4.781575 val ap:0.637114  val auc:0.698314\n",
            "\ttotal time:1.02s sample time:0.00s prep time:0.41s\n",
            "Epoch 80:\n",
            "\ttrain loss:10607.7293 train ap:0.628823  train auc:0.671712  val loss:5.059424 val ap:0.654127  val auc:0.702923\n",
            "\ttotal time:1.04s sample time:0.00s prep time:0.42s\n",
            "Epoch 81:\n",
            "\ttrain loss:10650.0199 train ap:0.626929  train auc:0.667046  val loss:5.067588 val ap:0.608103  val auc:0.684825\n",
            "\ttotal time:0.98s sample time:0.00s prep time:0.40s\n",
            "Epoch 82:\n",
            "\ttrain loss:10631.2737 train ap:0.633181  train auc:0.672526  val loss:5.540724 val ap:0.568695  val auc:0.627464\n",
            "\ttotal time:1.00s sample time:0.00s prep time:0.41s\n",
            "Epoch 83:\n",
            "\ttrain loss:10742.9350 train ap:0.619705  train auc:0.659375  val loss:5.200820 val ap:0.617092  val auc:0.646872\n",
            "\ttotal time:0.98s sample time:0.00s prep time:0.40s\n",
            "Epoch 84:\n",
            "\ttrain loss:10616.6607 train ap:0.632758  train auc:0.673555  val loss:5.175525 val ap:0.620660  val auc:0.668195\n",
            "\ttotal time:1.04s sample time:0.00s prep time:0.42s\n",
            "Epoch 85:\n",
            "\ttrain loss:10639.9583 train ap:0.620583  train auc:0.664061  val loss:4.968251 val ap:0.676718  val auc:0.711116\n",
            "\ttotal time:1.46s sample time:0.00s prep time:0.61s\n",
            "Epoch 86:\n",
            "\ttrain loss:10600.0756 train ap:0.630682  train auc:0.672850  val loss:4.884269 val ap:0.606885  val auc:0.672465\n",
            "\ttotal time:1.45s sample time:0.00s prep time:0.62s\n",
            "Epoch 87:\n",
            "\ttrain loss:10645.2982 train ap:0.632100  train auc:0.672689  val loss:5.016119 val ap:0.657365  val auc:0.684952\n",
            "\ttotal time:1.21s sample time:0.00s prep time:0.49s\n",
            "Epoch 88:\n",
            "\ttrain loss:10580.7450 train ap:0.628689  train auc:0.672207  val loss:4.901015 val ap:0.670845  val auc:0.726624\n",
            "\ttotal time:1.01s sample time:0.00s prep time:0.41s\n",
            "Epoch 89:\n",
            "\ttrain loss:10568.5217 train ap:0.635181  train auc:0.678255  val loss:5.163567 val ap:0.676903  val auc:0.715762\n",
            "\ttotal time:0.99s sample time:0.00s prep time:0.40s\n",
            "Loading model at epoch 68...\n",
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/train.py:290: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path_saver))\n",
            "{'test_R@1': 0.115485564, 'test_R@5': 0.39895013, 'test_R@10': 0.68503934, 'test_mrr': 0.22418712358122722}\n",
            "\taverage test precision:0.676903  test AUC:0.715762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positive lead/lag"
      ],
      "metadata": {
        "id": "ZJaROdzJk7VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python gen_graph.py --data leadlag"
      ],
      "metadata": {
        "id": "jjw6L2E8DCPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data positive --config ./config/APAN.yml --eval_neg_samples 30"
      ],
      "metadata": {
        "id": "KqStB8Puafup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944a0982-5336-4624-b79a-8cdd6a49582a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/utils.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  node_feats = torch.load('DATA/{}/node_features.pt'.format(d))\n",
            "Epoch 0:\n",
            "\ttrain loss:3417.5974 train ap:0.500817  train auc:0.496761  val loss:2.737650 val ap:0.604304  val auc:0.612605\n",
            "\ttotal time:0.74s sample time:0.00s prep time:0.22s\n",
            "Epoch 1:\n",
            "\ttrain loss:3355.8672 train ap:0.547385  train auc:0.550236  val loss:2.657583 val ap:0.651115  val auc:0.676719\n",
            "\ttotal time:0.39s sample time:0.00s prep time:0.17s\n",
            "Epoch 2:\n",
            "\ttrain loss:3333.6253 train ap:0.552459  train auc:0.561144  val loss:2.689378 val ap:0.591389  val auc:0.621844\n",
            "\ttotal time:0.39s sample time:0.00s prep time:0.17s\n",
            "Epoch 3:\n",
            "\ttrain loss:3324.5583 train ap:0.549975  train auc:0.569682  val loss:2.667315 val ap:0.573375  val auc:0.644169\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.17s\n",
            "Epoch 4:\n",
            "\ttrain loss:3310.8157 train ap:0.558923  train auc:0.578949  val loss:2.667834 val ap:0.574304  val auc:0.634659\n",
            "\ttotal time:0.46s sample time:0.00s prep time:0.18s\n",
            "Epoch 5:\n",
            "\ttrain loss:3284.8893 train ap:0.578875  train auc:0.602908  val loss:2.719632 val ap:0.550171  val auc:0.592320\n",
            "\ttotal time:0.42s sample time:0.00s prep time:0.17s\n",
            "Epoch 6:\n",
            "\ttrain loss:3272.7658 train ap:0.565352  train auc:0.592078  val loss:2.700260 val ap:0.574944  val auc:0.627310\n",
            "\ttotal time:0.30s sample time:0.00s prep time:0.12s\n",
            "Epoch 7:\n",
            "\ttrain loss:3243.6974 train ap:0.583254  train auc:0.600139  val loss:2.669806 val ap:0.561987  val auc:0.612636\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 8:\n",
            "\ttrain loss:3225.7994 train ap:0.587977  train auc:0.616331  val loss:2.731633 val ap:0.516239  val auc:0.549191\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 9:\n",
            "\ttrain loss:3239.0347 train ap:0.564416  train auc:0.595732  val loss:2.600893 val ap:0.657664  val auc:0.679266\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 10:\n",
            "\ttrain loss:3245.7243 train ap:0.563078  train auc:0.591516  val loss:2.630000 val ap:0.587112  val auc:0.628515\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.12s\n",
            "Epoch 11:\n",
            "\ttrain loss:3224.3983 train ap:0.570697  train auc:0.599798  val loss:2.607726 val ap:0.526635  val auc:0.588086\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 12:\n",
            "\ttrain loss:3203.5444 train ap:0.589167  train auc:0.612961  val loss:2.606370 val ap:0.631646  val auc:0.662988\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 13:\n",
            "\ttrain loss:3207.3167 train ap:0.589672  train auc:0.622506  val loss:2.579361 val ap:0.655235  val auc:0.689816\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 14:\n",
            "\ttrain loss:3170.6453 train ap:0.607277  train auc:0.637716  val loss:2.600346 val ap:0.611381  val auc:0.661456\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 15:\n",
            "\ttrain loss:3211.4769 train ap:0.588524  train auc:0.613916  val loss:2.514958 val ap:0.658524  val auc:0.678266\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 16:\n",
            "\ttrain loss:3150.8441 train ap:0.593822  train auc:0.631168  val loss:2.575907 val ap:0.542492  val auc:0.607081\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 17:\n",
            "\ttrain loss:3185.6316 train ap:0.589074  train auc:0.618401  val loss:2.592348 val ap:0.658669  val auc:0.656174\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 18:\n",
            "\ttrain loss:3187.1079 train ap:0.589312  train auc:0.619279  val loss:2.543607 val ap:0.593908  val auc:0.643370\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 19:\n",
            "\ttrain loss:3159.5952 train ap:0.605599  train auc:0.630256  val loss:2.473785 val ap:0.651264  val auc:0.684587\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 20:\n",
            "\ttrain loss:3121.0007 train ap:0.602480  train auc:0.638236  val loss:2.512404 val ap:0.634295  val auc:0.701237\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.12s\n",
            "Epoch 21:\n",
            "\ttrain loss:3140.8130 train ap:0.614004  train auc:0.644023  val loss:2.580672 val ap:0.640278  val auc:0.685782\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.21s\n",
            "Epoch 22:\n",
            "\ttrain loss:3138.7343 train ap:0.604905  train auc:0.637102  val loss:2.599302 val ap:0.596896  val auc:0.648998\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 23:\n",
            "\ttrain loss:3109.6343 train ap:0.613370  train auc:0.646843  val loss:2.607400 val ap:0.556930  val auc:0.611711\n",
            "\ttotal time:0.51s sample time:0.00s prep time:0.22s\n",
            "Epoch 24:\n",
            "\ttrain loss:3173.2856 train ap:0.593017  train auc:0.618916  val loss:2.614517 val ap:0.544843  val auc:0.587614\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 25:\n",
            "\ttrain loss:3096.9719 train ap:0.618477  train auc:0.653174  val loss:2.630503 val ap:0.603360  val auc:0.644109\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.12s\n",
            "Epoch 26:\n",
            "\ttrain loss:3143.3320 train ap:0.622984  train auc:0.648875  val loss:2.369188 val ap:0.671360  val auc:0.724334\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 27:\n",
            "\ttrain loss:3175.0484 train ap:0.584087  train auc:0.613645  val loss:2.540527 val ap:0.537949  val auc:0.576091\n",
            "\ttotal time:0.26s sample time:0.00s prep time:0.11s\n",
            "Epoch 28:\n",
            "\ttrain loss:3139.4111 train ap:0.605976  train auc:0.633946  val loss:2.565935 val ap:0.610183  val auc:0.645767\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 29:\n",
            "\ttrain loss:3130.5499 train ap:0.607055  train auc:0.640068  val loss:2.544671 val ap:0.600533  val auc:0.668412\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 30:\n",
            "\ttrain loss:3124.3005 train ap:0.606623  train auc:0.641810  val loss:2.426966 val ap:0.646387  val auc:0.682488\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 31:\n",
            "\ttrain loss:3092.5852 train ap:0.617617  train auc:0.651782  val loss:2.712251 val ap:0.571314  val auc:0.604833\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 32:\n",
            "\ttrain loss:3078.1619 train ap:0.612107  train auc:0.641719  val loss:2.574516 val ap:0.589909  val auc:0.644252\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.12s\n",
            "Epoch 33:\n",
            "\ttrain loss:3068.9394 train ap:0.614845  train auc:0.645994  val loss:2.577247 val ap:0.573687  val auc:0.627043\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.12s\n",
            "Epoch 34:\n",
            "\ttrain loss:3097.1873 train ap:0.615053  train auc:0.644416  val loss:2.499184 val ap:0.612164  val auc:0.671840\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 35:\n",
            "\ttrain loss:3082.7133 train ap:0.605933  train auc:0.636393  val loss:2.525498 val ap:0.620239  val auc:0.653650\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.12s\n",
            "Epoch 36:\n",
            "\ttrain loss:3078.8281 train ap:0.613479  train auc:0.641867  val loss:2.699594 val ap:0.554536  val auc:0.594046\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.12s\n",
            "Epoch 37:\n",
            "\ttrain loss:3074.3154 train ap:0.615168  train auc:0.647437  val loss:2.526963 val ap:0.580859  val auc:0.616937\n",
            "\ttotal time:0.35s sample time:0.00s prep time:0.15s\n",
            "Epoch 38:\n",
            "\ttrain loss:3078.7886 train ap:0.621157  train auc:0.657833  val loss:2.580795 val ap:0.557885  val auc:0.625487\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.17s\n",
            "Epoch 39:\n",
            "\ttrain loss:3086.5350 train ap:0.620926  train auc:0.650566  val loss:2.537347 val ap:0.587471  val auc:0.652317\n",
            "\ttotal time:0.38s sample time:0.00s prep time:0.16s\n",
            "Epoch 40:\n",
            "\ttrain loss:3131.2089 train ap:0.596204  train auc:0.629585  val loss:2.601990 val ap:0.609296  val auc:0.663715\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.17s\n",
            "Epoch 41:\n",
            "\ttrain loss:3089.4542 train ap:0.607923  train auc:0.639518  val loss:2.533720 val ap:0.571000  val auc:0.621363\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.17s\n",
            "Epoch 42:\n",
            "\ttrain loss:3053.6047 train ap:0.604436  train auc:0.640635  val loss:2.536682 val ap:0.645192  val auc:0.706697\n",
            "\ttotal time:0.39s sample time:0.00s prep time:0.16s\n",
            "Epoch 43:\n",
            "\ttrain loss:3063.0704 train ap:0.607160  train auc:0.645643  val loss:2.610155 val ap:0.572150  val auc:0.628151\n",
            "\ttotal time:0.45s sample time:0.00s prep time:0.19s\n",
            "Epoch 44:\n",
            "\ttrain loss:3033.1185 train ap:0.617109  train auc:0.654500  val loss:2.549169 val ap:0.654273  val auc:0.718457\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.17s\n",
            "Epoch 45:\n",
            "\ttrain loss:3042.4753 train ap:0.616951  train auc:0.658731  val loss:2.787658 val ap:0.579864  val auc:0.611679\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.16s\n",
            "Epoch 46:\n",
            "\ttrain loss:3086.5739 train ap:0.607472  train auc:0.634236  val loss:2.756834 val ap:0.548176  val auc:0.580615\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.12s\n",
            "Epoch 47:\n",
            "\ttrain loss:2996.7035 train ap:0.633551  train auc:0.673092  val loss:2.711368 val ap:0.598717  val auc:0.643913\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 48:\n",
            "\ttrain loss:2999.7860 train ap:0.610545  train auc:0.659376  val loss:2.740123 val ap:0.562148  val auc:0.593238\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.12s\n",
            "Epoch 49:\n",
            "\ttrain loss:3014.5392 train ap:0.618453  train auc:0.656732  val loss:2.602328 val ap:0.564880  val auc:0.627064\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.12s\n",
            "Epoch 50:\n",
            "\ttrain loss:3019.5550 train ap:0.624876  train auc:0.662013  val loss:2.841795 val ap:0.594959  val auc:0.649841\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.12s\n",
            "Epoch 51:\n",
            "\ttrain loss:3022.4400 train ap:0.620463  train auc:0.657495  val loss:2.819865 val ap:0.594624  val auc:0.632950\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 52:\n",
            "\ttrain loss:3030.7787 train ap:0.607553  train auc:0.653161  val loss:2.639853 val ap:0.613438  val auc:0.645330\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.11s\n",
            "Epoch 53:\n",
            "\ttrain loss:3012.3689 train ap:0.621477  train auc:0.652575  val loss:2.775718 val ap:0.549273  val auc:0.600482\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.12s\n",
            "Epoch 54:\n",
            "\ttrain loss:3033.1069 train ap:0.619159  train auc:0.654942  val loss:2.760955 val ap:0.536107  val auc:0.581276\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 55:\n",
            "\ttrain loss:2958.8255 train ap:0.639887  train auc:0.678467  val loss:2.784068 val ap:0.602127  val auc:0.621264\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.12s\n",
            "Epoch 56:\n",
            "\ttrain loss:2988.5163 train ap:0.628576  train auc:0.664627  val loss:2.907300 val ap:0.574456  val auc:0.609053\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 57:\n",
            "\ttrain loss:3008.6750 train ap:0.623765  train auc:0.662960  val loss:2.852658 val ap:0.534312  val auc:0.572041\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 58:\n",
            "\ttrain loss:3026.4927 train ap:0.621602  train auc:0.659016  val loss:2.858918 val ap:0.566669  val auc:0.622540\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 59:\n",
            "\ttrain loss:3005.7598 train ap:0.636303  train auc:0.670176  val loss:2.875533 val ap:0.601333  val auc:0.636186\n",
            "\ttotal time:0.33s sample time:0.00s prep time:0.13s\n",
            "Epoch 60:\n",
            "\ttrain loss:3008.6538 train ap:0.644186  train auc:0.673530  val loss:2.925292 val ap:0.528563  val auc:0.587229\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 61:\n",
            "\ttrain loss:2975.8059 train ap:0.635263  train auc:0.672067  val loss:2.748815 val ap:0.624878  val auc:0.651702\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 62:\n",
            "\ttrain loss:3000.1638 train ap:0.638466  train auc:0.672582  val loss:2.804857 val ap:0.649084  val auc:0.661885\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.12s\n",
            "Epoch 63:\n",
            "\ttrain loss:3016.0612 train ap:0.620725  train auc:0.653594  val loss:2.809702 val ap:0.590460  val auc:0.624976\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 64:\n",
            "\ttrain loss:2996.4280 train ap:0.639168  train auc:0.668043  val loss:2.790067 val ap:0.604051  val auc:0.654805\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.11s\n",
            "Epoch 65:\n",
            "\ttrain loss:2970.3615 train ap:0.643343  train auc:0.680059  val loss:2.863432 val ap:0.593724  val auc:0.643719\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 66:\n",
            "\ttrain loss:3027.3917 train ap:0.620167  train auc:0.656890  val loss:2.857956 val ap:0.631745  val auc:0.657130\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 67:\n",
            "\ttrain loss:3025.0616 train ap:0.623316  train auc:0.661314  val loss:2.836439 val ap:0.597483  val auc:0.650648\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 68:\n",
            "\ttrain loss:3068.6199 train ap:0.616933  train auc:0.649281  val loss:2.783037 val ap:0.624455  val auc:0.666942\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 69:\n",
            "\ttrain loss:3033.9718 train ap:0.627142  train auc:0.659945  val loss:2.590318 val ap:0.573835  val auc:0.629870\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.12s\n",
            "Epoch 70:\n",
            "\ttrain loss:3056.5233 train ap:0.618498  train auc:0.650548  val loss:2.735906 val ap:0.642071  val auc:0.656100\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 71:\n",
            "\ttrain loss:2964.6290 train ap:0.645046  train auc:0.679405  val loss:2.884687 val ap:0.609349  val auc:0.650253\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 72:\n",
            "\ttrain loss:2961.1428 train ap:0.636693  train auc:0.668487  val loss:2.984163 val ap:0.555948  val auc:0.587454\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.12s\n",
            "Epoch 73:\n",
            "\ttrain loss:2974.5879 train ap:0.630728  train auc:0.669785  val loss:2.890068 val ap:0.575755  val auc:0.611211\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.12s\n",
            "Epoch 74:\n",
            "\ttrain loss:2947.6138 train ap:0.640367  train auc:0.678472  val loss:3.024299 val ap:0.545874  val auc:0.593850\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.11s\n",
            "Epoch 75:\n",
            "\ttrain loss:2947.3781 train ap:0.649271  train auc:0.678063  val loss:3.019986 val ap:0.596897  val auc:0.630219\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.12s\n",
            "Epoch 76:\n",
            "\ttrain loss:2985.9771 train ap:0.625403  train auc:0.659926  val loss:2.851742 val ap:0.601984  val auc:0.627760\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 77:\n",
            "\ttrain loss:2998.2572 train ap:0.633568  train auc:0.664289  val loss:2.947065 val ap:0.509244  val auc:0.559818\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 78:\n",
            "\ttrain loss:2970.6804 train ap:0.631005  train auc:0.669346  val loss:2.929615 val ap:0.562896  val auc:0.599575\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.17s\n",
            "Epoch 79:\n",
            "\ttrain loss:3004.2601 train ap:0.635517  train auc:0.680193  val loss:2.936409 val ap:0.591896  val auc:0.644937\n",
            "\ttotal time:0.39s sample time:0.00s prep time:0.17s\n",
            "Epoch 80:\n",
            "\ttrain loss:3030.2813 train ap:0.621451  train auc:0.652614  val loss:2.802944 val ap:0.621828  val auc:0.632282\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.17s\n",
            "Epoch 81:\n",
            "\ttrain loss:2946.8667 train ap:0.656782  train auc:0.690335  val loss:2.919017 val ap:0.583557  val auc:0.613116\n",
            "\ttotal time:0.42s sample time:0.00s prep time:0.19s\n",
            "Epoch 82:\n",
            "\ttrain loss:2996.5310 train ap:0.629766  train auc:0.665051  val loss:2.770204 val ap:0.670390  val auc:0.700545\n",
            "\ttotal time:0.40s sample time:0.00s prep time:0.18s\n",
            "Epoch 83:\n",
            "\ttrain loss:2990.9779 train ap:0.632597  train auc:0.670267  val loss:3.049114 val ap:0.516035  val auc:0.563110\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.17s\n",
            "Epoch 84:\n",
            "\ttrain loss:2919.7935 train ap:0.661839  train auc:0.695601  val loss:2.869327 val ap:0.684358  val auc:0.719094\n",
            "\ttotal time:0.43s sample time:0.00s prep time:0.18s\n",
            "Epoch 85:\n",
            "\ttrain loss:2963.3318 train ap:0.637891  train auc:0.672169  val loss:2.662925 val ap:0.658606  val auc:0.700315\n",
            "\ttotal time:0.41s sample time:0.00s prep time:0.17s\n",
            "Epoch 86:\n",
            "\ttrain loss:2946.7735 train ap:0.637445  train auc:0.675647  val loss:3.047015 val ap:0.589124  val auc:0.614873\n",
            "\ttotal time:0.33s sample time:0.00s prep time:0.13s\n",
            "Epoch 87:\n",
            "\ttrain loss:2932.6547 train ap:0.639355  train auc:0.680132  val loss:2.902563 val ap:0.628527  val auc:0.647501\n",
            "\ttotal time:0.28s sample time:0.00s prep time:0.12s\n",
            "Epoch 88:\n",
            "\ttrain loss:2915.8592 train ap:0.656593  train auc:0.692890  val loss:2.803760 val ap:0.632768  val auc:0.678655\n",
            "\ttotal time:0.27s sample time:0.00s prep time:0.11s\n",
            "Epoch 89:\n",
            "\ttrain loss:2989.6270 train ap:0.642155  train auc:0.681766  val loss:2.877127 val ap:0.598332  val auc:0.643680\n",
            "\ttotal time:0.29s sample time:0.00s prep time:0.12s\n",
            "Loading model at epoch 84...\n",
            "/content/drive/MyDrive/Data Science Padova/Thesis/tgl/train.py:290: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path_saver))\n",
            "{'test_R@1': 0.00952381, 'test_R@5': 0.0952381, 'test_R@10': 0.35238096, 'test_mrr': 0.10058987092793296}\n",
            "\taverage test precision:0.598332  test AUC:0.643680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1dwJlDk_LjOZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}